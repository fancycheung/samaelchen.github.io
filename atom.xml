<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>碎碎念</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://samaelchen.github.io/"/>
  <updated>2018-10-11T14:32:34.682Z</updated>
  <id>https://samaelchen.github.io/</id>
  
  <author>
    <name>Samael Chen</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>tiny XGBoost</title>
    <link href="https://samaelchen.github.io/tinyxgb/"/>
    <id>https://samaelchen.github.io/tinyxgb/</id>
    <published>2018-10-07T16:00:00.000Z</published>
    <updated>2018-10-11T14:32:34.682Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>XGBoost是GBDT的一个超级加强版，用了很久，一直没细看原理。围观了一波人家的实现，自己也来弄一遍。以后面试上来一句，要不要我现场写个XGBoost，听上去是不是很霸气。</p><p>大概撸了一遍原理，代码待续。一想到还得自己写个树心情就不美丽了。</p><a id="more"></a><h1 id="回顾一下ensemble算法">回顾一下ensemble算法</h1><p>之前也写了一篇笔记，<a href="https://samaelchen.github.io/machine_learning_step17/">台大李宏毅机器学习——集成算法</a>，最近把一些错误做了修改。</p><h2 id="bagging">bagging</h2><p>bagging的经典算法就是随机森林了，bagging的思路其实非常非常的简单，就是同时随机抽样本和feature，然后建立n个分类器，接着投票就好了。</p><p>bagging的做法本质上不会解决bias太大的问题，所以该过拟合还会过拟合。但是bagging会解决variance太大的问题。这个其实是非常直观的一件事情。</p><p>然后我突然开了一个脑洞，RF选择feature这种事情吧，如果用一个weight来表示，把树换成逻辑回归，最后voting的事情也用一个weight来表示，感觉，似乎就是单层神经网络的既视感。anyway，无脑揣测，有待探究。</p><h2 id="boosting">boosting</h2><p>boosting跟bagging的套路就不一样，bagging是同时并行很多分类器，但是boosting是串行多个分类器。bagging的分类器之间没有依赖关系，boosting的分类器是有依赖关系的。</p><p>boosting算法比较知名的就是GBDT和Adaboost两个。不过其实Adaboost就是一个特殊的GBDT。</p><p>GB的一般流程是这样的：</p><blockquote><p>初始化一个函数<span class="math inline">\(g_0(x) = 0\)</span><br>然后按照迭代次数从<span class="math inline">\(t=1\)</span>到<span class="math inline">\(T\)</span>循环，我们的目标是找到一个函数<span class="math inline">\(f_t(x)\)</span>和权重<span class="math inline">\(\alpha_t\)</span>使得我们的函数<span class="math inline">\(g_{t-1}(x)\)</span>的效果更好，也就是说：<br><span class="math display">\[g_{t-1}(x) = \sum_{i=1}^{t-1} \alpha_i f_i(x)\]</span> 换个角度来看就是<span class="math inline">\(g_t(x) = g_{t-1}(x) + \alpha_t f_t(x)\)</span><br>而最后我们优化的损失函数<span class="math inline">\(L(g) = \sum_n l(y_n, g(x_n))\)</span></p></blockquote><p>那Adaboost就是GB的损失函数<span class="math inline">\(l\)</span>用exponential表示，也就是<span class="math inline">\(\exp(-y_n g(x_n))\)</span>。很美妙的一家子。</p><p>那实际上我们看<span class="math inline">\(g_t(x) = g_{t-1}(x) + \alpha_t f_t(x)\)</span>这里，非常像梯度下降，那么如果要做梯度下降的话，其实我们就是对<span class="math inline">\(g(x)\)</span>做偏导，所以我们得到的是<span class="math inline">\(g_t(x) = g_{t-1}(x) - \eta \frac{\partial L(g)}{\partial g(x)} \bigg|_{g(x)=g_{t-1}(x)}\)</span>。那其实只要我们想办法让尾巴后面的那一部分是同一个方向的，我们不就达到了梯度下降的目的了吗？！步子的大小是可以用<span class="math inline">\(\eta\)</span>调的，同样这边也可以调整<span class="math inline">\(\alpha\)</span>。总之，保证他们的方向一致，就可以做梯度下降了。</p><h3 id="adaboost">Adaboost</h3><p>现在将Adaboost的损失函数放进来推算一下： <span class="math display">\[\begin{align}L(g) &amp;= \sum_n \exp(-y_n g_t(x_n)) \\&amp;= \sum_n \exp(-y_n (g_{t-1}(x_n) + \alpha_t f_t(x))) \\&amp;= \sum_n \exp(-y_n g_{t-1}(x_n)) \exp(-y_n \alpha_t f_t(x_n)) \\&amp;= \sum_{f_t(x) \ne y} \exp(-y_n g_{t-1}(x_n)) \exp(\alpha_t) + \sum_{f_t(x) = y} \exp(-y_n g_{t-1}(x_n)) \exp(-\alpha_t)\end{align}\]</span></p><p>这样一来，我们需要同时寻找一个<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(f(x)\)</span>使得我们的损失函数最小。找一个最优的<span class="math inline">\(f(x)\)</span>这个事情只要让决策树去学习就可以了，那么<span class="math inline">\(\alpha\)</span>怎么办呢？如果我们对<span class="math inline">\(\alpha\)</span>求偏导，让偏导数为0，那么理论上，我们在<span class="math inline">\(\alpha\)</span>这个方向上就是最优的。</p><p>具体求偏导这个之前的博客写了，这里就不多搞了。简单说就是刚好会等于<span class="math inline">\(\ln \sqrt{(1-\varepsilon_t) / \varepsilon_t}\)</span>，而这个值刚好是每一轮调整样本权重时候的系数取对数。</p><h3 id="gbdt">GBDT</h3><p>至于GBDT，实际上跟Adaboost没啥区别，也是一样的搞法，无非是损失函数不一样，优化策略不太一样而已。</p><p>GBDT的玩法是用每一棵树去学习上一棵树的残差，通俗的说就是下一棵树学习如何矫正上一棵树的错误。</p><p>但是残差这东西也是很妙的一个事情，如果我们损失函数是RMSE，其实我们的残差就是一阶导数。但是呢，如果损失函数是别的函数的时候，其实“残差”这个东西就很难去计算，比如分类的时候，残差究竟是个什么意思？！所以Freidman的梯度提升算法，也就是GBDT前面的GB就是用损失函数对<span class="math inline">\(f(x)\)</span>的一阶导数来替代残差的。</p><p>总体来说，我们想优化的损失函数是： <span class="math display">\[\Theta_t = \arg \min_{\Theta_t} \sum_{t=1}^{T} l(y_n, g_{t-1}(x_n) + \alpha_t f_t(x))\]</span></p><p>如果我们做回归问题，用RMSE为loss function，那么<span class="math inline">\(l(y_n, g_t(x_n))=(y_n - g_t(x_n))^2\)</span>，而<span class="math inline">\(y_n - g_t(x_n)\)</span>就是传说中的残差。所以用一阶导数来替代残差真的是神来之笔。这样每次的树直接去学习梯度，而在回归的时候残差跟一阶导数还是一样的，美滋滋。</p><p>然后突然有个很不成熟的想法，GBDT是不是很怕one hot的feature呢？！举个例子，如果所有的feature都是one hot的，同时我们每个DT都只有一层，是不是理论上来说，最后有多少feature就有多少树。</p><h1 id="xgboost">XGBoost</h1><p>嗯，就是一个增强版GBDT。</p><p>强在哪呢，boosting是一个加法训练，原来我们用的是一阶导数，而陈天奇则是用了二阶导并加了个正则项。</p><p>但是陈天奇做二阶导的目的怎么说呢，我不是非常理解。是inspired by GBDT的RMSE？！这个手算一下，我们的损失函数用RMSE的时候会有一个<span class="math inline">\((\alpha_t f_t(x))^2\)</span>，而这个东西类比泰勒展开的时候就是那个<span class="math inline">\(\Delta x\)</span>，所以怎么说呢，可能大牛是因为看到这个时候灵感乍现，然后尝试用了二阶导。</p><p>然后在XGBoost里面的损失函数就可以用二阶泰勒展开来表示： <span class="math display">\[\Theta = \sum_{i=1}^n [g_i f_t(x_i) + \frac{1}{2} h_i f^2_t(x_i)] + \Omega(f_t)\]</span> 其中的<span class="math inline">\(g_i\)</span>和<span class="math inline">\(h_i\)</span>分别是一阶导数和二阶导数。这样定义的好处是，以后不论用什么损失函数，总体的优化方向仅仅跟一阶导数和二阶导数有关。那么在这个大框架底下，任意可以二阶导的损失函数都可以放进来了。那么我看了XGB的<a href="https://arxiv.org/pdf/1603.02754.pdf" target="_blank" rel="noopener">论文</a>关于为什么取二阶导的描述： <img src="https://i.loli.net/2018/10/11/5bbec3573482d.png"> 直观感受上来说吧，就有点像拟牛顿法是梯度下降的一个功能性提升的样子。</p><p>另外就是正则项，这里<span class="math inline">\(\Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_j^2\)</span>。这里的<span class="math inline">\(T\)</span>是我们有多少叶节点，<span class="math inline">\(w\)</span>是叶节点的权重。这两个都在XGB的参数里面可以调整的，我们都知道正则项会改变模型的保守程度，或者说就是variance。</p><p>其他XGB的优化很多是工程上的优化，这个不是CS科班出身就看不懂了。如何并行化什么的，一脸懵逼。具体可以看陈天奇的论文，数学的部分写的非常美妙。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;XGBoost是GBDT的一个超级加强版，用了很久，一直没细看原理。围观了一波人家的实现，自己也来弄一遍。以后面试上来一句，要不要我现场写个XGBoost，听上去是不是很霸气。&lt;/p&gt;
&lt;p&gt;大概撸了一遍原理，代码待续。一想到还得自己写个树心情就不美丽了。&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>一个挂逼</title>
    <link href="https://samaelchen.github.io/guabi/"/>
    <id>https://samaelchen.github.io/guabi/</id>
    <published>2018-10-02T16:00:00.000Z</published>
    <updated>2018-10-09T13:26:34.255Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>挂逼是形容深圳三和人才市场里面一些生活极其困难的打工者，有时候也代表这个人死了。</p><a id="more"></a><h1 id="三和大神">三和大神</h1><p>这是我实在无所事事的时候偶然发现的一个神奇存在，在看到三和大神之前，你永远不会相信，有人能够这样活着。当然，在看三和大神之前，我也不会想到，自己也就是个三和大神。</p>关于三和大神的一段描述：<p align="center"><img src="https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/blog_0001.png"></p><p>反观自己的标准，惊讶地发现，似乎也没什么差别。996的不要，大饼画得太大的不要，考勤严格的不要，没有奖金的不要，活干不完的不要……</p>三和大神的人物画像：<p align="center"><img src="https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/blog_0002.png"></p><p>再反观自己，似乎也并没有什么两样。喜欢算的工资是时薪；下班就是看片游戏；吃饭叫个外卖；能找到跟三四个人挤在一起的自如就谢天谢地，还在乎甲醛？……</p><p>除了物质层面的差别，在精神上，可能早就是一个三和大神。对，自己就是个每天洗澡的挂逼。</p><h1 id="小聚">小聚</h1><p>回乡同学婚礼小聚，谁能想到，时间是这个世界上最神奇的东西。丧几乎快成为我的个人代名词，但是我不能懂的是，为什么有人能不丧。在他们身上，我看到一种非常奇怪的感觉。可能是他们身上不同于日常见到的乐观积极，这种只是为了过日子的积极态度，让我充满了怀疑。当然，也可能是挂逼对所有积极生活的人都充满了怀疑。</p><p>那些在一线城市的互联网民工们，可能你们在为了改进APP图标的几个像素点能够带来多大的ROI而兴奋不已，但是，这些事情对自己的意义又在哪里。为了升职加薪？还是所谓的为了改变世界？</p><p>不知道是在魔都呆久了的原因，还是真的被洗脑了。我曾经觉得我还年轻，但是当一些只比自己大两三岁的同学，以一种在魔都被嘲讽的中年人身姿出现在我面前的时候，突然一阵恍惚。</p><p>我很难描述这次小聚的感受。我看着他们在KTV里面嘶吼，我总是会想起来三和大神200舞。如果不论生活质量，活着，到底意味着什么？</p><h1 id="想上岸的挂逼">想上岸的挂逼</h1><p>现实是，你的热血是可以被利用的，你的未来也可以被拿来成为他们的垫脚石。你以为，他们款款深情让你将所有托付给他们的以后能够让你为荣耀而活。但是你马上发现，自己的体力、脑力、技能、资源、能力都与这一切都毫无关系，你在他们的棋局里面已经被淘汰了。</p><p>非常感谢这段经历，我终于也成为众多三和大神中的一员。它教会了我，其实他们所说的未来一直都与你无关，那是他们的未来。当然，正因为他们的未来与自己无关，所以自己也不必有什么负罪感。</p><p>在三和，不是每个人都甘心成为挂逼，开始在快手直播的200舞，也许某一天他也就火了，然后也就上岸了。</p><p>时间有限，一生可以用来上岸的的时间其实没有多久，所以，虽然现实很丧，但是不要因为一时被人利用就做挂逼。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;挂逼是形容深圳三和人才市场里面一些生活极其困难的打工者，有时候也代表这个人死了。&lt;/p&gt;
    
    </summary>
    
      <category term="碎碎念" scheme="https://samaelchen.github.io/categories/self-questioning/"/>
    
    
  </entry>
  
  <entry>
    <title>Char-RNN生成古诗</title>
    <link href="https://samaelchen.github.io/pytorch-char-rnn/"/>
    <id>https://samaelchen.github.io/pytorch-char-rnn/</id>
    <published>2018-09-27T16:00:00.000Z</published>
    <updated>2018-10-09T13:26:34.259Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>尝试用char-RNN生成古诗，本来是想要尝试用来生成广告文案的，测试一波生成古诗的效果。嘛，虽然我对业务兴趣不大，不过这个模型居然把我硬盘跑挂了，也是醉。</p><a id="more"></a><p>其实Char-RNN来生成文本的逻辑非常简单，就是一个字一个字放进去，让RNN开始学，按照前面的字预测下面的字。所以就要想办法把文本揉成我们需要的格式。</p><p>比如说，我们现在有一句诗“床前明月光，疑是地上霜”。那么我们的输入就是“床前明月光”，那么我们的预测就是“前明月光，”，其实就是错位一位。</p><p>然后我们要考虑的是如何批量的把数据喂进去，这里参考了<a href="http://zh.gluon.ai/chapter_recurrent-neural-networks/lang-model-dataset.html" target="_blank" rel="noopener">gluon的教程</a>上面的一个操作，因为诗歌是有上下文联系的，如果我们用随机选取的话，很可能就会丢掉很多有用的信息，所以我们还要想办法将诗歌的这种连续性保留下来。</p><p>mxnet教程的方法是先将所有的文本串成一行。所有的换行符替换为空格，所以空格在这里起到了分段的作用，空格也就有了意义。然后我们因为我们要批量训练，所以先按照我们每批打算训练多少行文本，将这一个超长的文本截断成这样，然后按照我们一次想看多少个字的窗口扫描过去。代码实现上如下： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_consecutive</span><span class="params">(corpus_indices, batch_size, num_steps)</span>:</span></span><br><span class="line">    corpus_indices = torch.tensor(corpus_indices)</span><br><span class="line">    data_len = len(corpus_indices)</span><br><span class="line">    batch_len = data_len // batch_size</span><br><span class="line">    indices = corpus_indices[<span class="number">0</span>: batch_size*batch_len].reshape((</span><br><span class="line">        batch_size, batch_len))</span><br><span class="line">    epoch_size = (batch_len - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        i = i * num_steps</span><br><span class="line">        X = indices[:, i: i + num_steps]</span><br><span class="line">        Y = indices[:, i + <span class="number">1</span>: i + num_steps + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br></pre></td></tr></table></figure></p><p>这样有一个好处就是可以保持诗句的连续性，效果上大概是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 所有诗句拼成一行</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>， <span class="number">10</span>， <span class="number">11</span>， <span class="number">12</span>]</span><br><span class="line"><span class="comment"># batch_size = 2, num_steps = 3</span></span><br><span class="line"><span class="comment"># batch 1</span></span><br><span class="line">[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</span><br><span class="line"><span class="comment"># batch 2</span></span><br><span class="line">[[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]</span><br></pre></td></tr></table></figure><p>这样一来，一句诗[1, 2, 3, 4, 5, 6]就能在不同batch里面保持连贯性了。</p><p>然后就是很简单设计网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">lyricNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_dim, embed_dim, num_layers, weight,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_labels, bidirectional, dropout=<span class="number">0.5</span>, **kwargs)</span>:</span></span><br><span class="line">        super(lyricNet, self).__init__(**kwargs)</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.num_labels = num_labels</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        <span class="keyword">if</span> num_layers &lt;= <span class="number">1</span>:</span><br><span class="line">            self.dropout = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.dropout = dropout</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(weight)</span><br><span class="line">        self.embedding.weight.requires_grad = <span class="keyword">False</span></span><br><span class="line"><span class="comment">#         self.embedding = nn.Embedding(num_labels, self.embed_dim)</span></span><br><span class="line">        self.rnn = nn.GRU(input_size=self.embed_dim, hidden_size=self.hidden_dim,</span><br><span class="line">                          num_layers=self.num_layers, bidirectional=self.bidirectional,</span><br><span class="line">                          dropout=self.dropout)</span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            self.decoder = nn.Linear(hidden_dim * <span class="number">2</span>, self.num_labels)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.decoder = nn.Linear(hidden_dim, self.num_labels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, hidden=None)</span>:</span></span><br><span class="line">        embeddings = self.embedding(inputs)</span><br><span class="line">        states, hidden = self.rnn(embeddings.permute([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]), hidden)</span><br><span class="line">        outputs = self.decoder(states.reshape((<span class="number">-1</span>, states.shape[<span class="number">-1</span>])))</span><br><span class="line">        <span class="keyword">return</span>(outputs, hidden)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, num_layers, batch_size, hidden_dim, **kwargs)</span>:</span></span><br><span class="line">        hidden = torch.zeros(num_layers, batch_size, hidden_dim)</span><br><span class="line">        <span class="keyword">return</span> hidden</span><br></pre></td></tr></table></figure><p>这里我用的是很简单的one-hot做词向量，当然数据量大一点可以考虑pretrained的字向量。不过直观感受上用白话文训练的字向量应该效果不会太好吧。</p><p>接着就可以开始训练了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epoch):</span><br><span class="line">    start = time.time()</span><br><span class="line">    num, total_loss = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    data = data_iter_consecutive(corpus_indice, batch_size, <span class="number">35</span>)</span><br><span class="line">    hidden = model.init_hidden(num_layers, batch_size, hidden_dim)</span><br><span class="line">    <span class="keyword">for</span> X, Y <span class="keyword">in</span> data:</span><br><span class="line">        num += <span class="number">1</span></span><br><span class="line">        hidden.detach_()</span><br><span class="line">        <span class="keyword">if</span> use_gpu:</span><br><span class="line">            X = X.to(device)</span><br><span class="line">            Y = Y.to(device)</span><br><span class="line">            hidden = hidden.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output, hidden = model(X, hidden)</span><br><span class="line">        l = loss_function(output, Y.t().reshape((<span class="number">-1</span>,)))</span><br><span class="line">        l.backward()</span><br><span class="line">        norm = nn.utils.clip_grad_norm_(model.parameters(), <span class="number">1e-2</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += l.item()</span><br><span class="line">    end = time.time()</span><br><span class="line">    s = end - since</span><br><span class="line">    h = math.floor(s / <span class="number">3600</span>)</span><br><span class="line">    m = s - h * <span class="number">3600</span></span><br><span class="line">    m = math.floor(m / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">if</span> (epoch % <span class="number">10</span> == <span class="number">0</span>) <span class="keyword">or</span> (epoch == (num_epoch - <span class="number">1</span>)):</span><br><span class="line">        print(<span class="string">'epoch %d/%d, loss %.4f, norm %.4f, time %.3fs, since %dh %dm %ds'</span></span><br><span class="line">              %(epoch+<span class="number">1</span>, num_epoch, total_loss / num, norm, end-start, h, m, s))</span><br></pre></td></tr></table></figure><p>这里的训练过程需要注意两个点，一个是hidden的initial，因为我们想要保持句子的连续性，所以我们hidden的initial只要每个epoch的第一次initial一下就可以了，后面训练的过程中需要从计算图中拿掉。另外就是因为有梯度爆炸的问题，所以我们需要对梯度进行修剪。</p><p>最后一个是我自己最容易犯错的地方，死活记不住的就是RNN的输入输出每个dimension都代表了什么含义。原始的RNN接受的输入是(seq_len, batch_size, embedding_dimension)，输出的是(seq_len, batch_size, num_direction * hidden_dim)。所以我们习惯的batch在先的数据需要在这里做一个permute，将batch和seq做一下调换。然后就是我们做分类的时候，直接flatten成为一个长向量的时候，其实已经变成了[seq_len, seq_len, …]这样的样子。简单理解就是本来我们都是横着看诗歌的，现在模型的输出是竖着输出的。所以我们后面算loss的时候，y也需要做一个转置再flatten。</p><p>具体的可以看我的这个<a href="&#39;https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/text%20generater/generate%20poem.ipynb&#39;">notebook</a>。</p><p>接下来可能想试一下的是如果不用这种方法的话，是不是可以用padding的方法把句子长度统一再训练。</p><p>另外强势推荐<a href="https://github.com/chinese-poetry/chinese-poetry" target="_blank" rel="noopener">最全中华古诗词数据库</a>。数据非常非常全了。</p><p>后面如果要做到很好的效果可以做的方向一个是做韵脚的信息，还有就是平仄的信息也带进去。</p><p>anyway，想了一下，这样训练完的hidden是不是就包含了一个作者的文风信息？！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;尝试用char-RNN生成古诗，本来是想要尝试用来生成广告文案的，测试一波生成古诗的效果。嘛，虽然我对业务兴趣不大，不过这个模型居然把我硬盘跑挂了，也是醉。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://samaelchen.github.io/categories/DeepLearning/"/>
    
      <category term="PyTorch" scheme="https://samaelchen.github.io/categories/PyTorch/"/>
    
    
  </entry>
  
  <entry>
    <title>日常的丧</title>
    <link href="https://samaelchen.github.io/depressed/"/>
    <id>https://samaelchen.github.io/depressed/</id>
    <published>2018-09-20T16:00:00.000Z</published>
    <updated>2018-10-09T13:26:34.255Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>日常很丧的各种不开心，小确丧。</p><a id="more"></a><h1 id="section">2018-09-21</h1><p>一直在试char-rnn生成，可能notebook硬盘io频繁了一点，终于把工作站的硬盘搞到写保护了。现在整个硬盘全是坏道。情绪稳定。</p><p>顺便，Linux检查硬盘坏道的方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo badblocks -s -v /dev/sdXX &gt; badblocks.txt</span><br></pre></td></tr></table></figure><p>-s可以显示检查进度，不过一般显示进度的话实际检查速度貌似会变慢。</p><p>然后可以用recovery模式去修复一下，fsck -a /dev/sdXX。运气好是逻辑坏道的话能修复好，如果跟我一样修复好一会儿会儿就又写保护了，估计十有八九是物理坏道。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;日常很丧的各种不开心，小确丧。&lt;/p&gt;
    
    </summary>
    
      <category term="今天份的不开心" scheme="https://samaelchen.github.io/categories/wtf/"/>
    
    
  </entry>
  
  <entry>
    <title>台大李宏毅深度学习——seq2seq</title>
    <link href="https://samaelchen.github.io/deep_learning_step6/"/>
    <id>https://samaelchen.github.io/deep_learning_step6/</id>
    <published>2018-09-05T16:00:00.000Z</published>
    <updated>2018-10-09T13:26:34.255Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>这节课的内容讲的有点浅，所以我看到是李沐的gluon教程，配合这节课的内容。</p><a id="more"></a><h1 id="seq2seq">Seq2Seq</h1><p>这个是encode-decode的过程。之前写的LSTM做文档分类是限定了输入的长度。超出规定长度的句子我们是截断，没达到长度的我们是padding。但是用seq2seq可以接受不定长的输入和不定长的输出。</p>实际上seq2seq是有两个循环神经网络，一个处理输入序列，另一个处理输出序列。处理输入序列的叫编码器，处理输出序列的叫解码器。流程上如下图：<p align="center"><img src="https://i.loli.net/2018/09/06/5b90c7ddec62a.png"></p><h2 id="encoder">encoder</h2><p>编码器是将一个不定长的输入序列变换成一个定长的背景向量<span class="math inline">\(c\)</span>。根据不一样的任务，编码器可以是不一样的网络。例如在对话系统或者机器翻译的场景下，我们用的编码器可以是LSTM，如果在caption的场景下，CNN就是编码器。</p><p>现在假设我们做一个机器翻译的任务，那么有一句话可以拆成<span class="math inline">\(x_1, \dots, x_T\)</span>个词的序列。下一个时刻的隐藏状态可以表示为<span class="math inline">\(h_t = f(x_t, h_{t-1})\)</span>。<span class="math inline">\(f\)</span>是循环网络隐藏层的变换函数。</p><p>然后我们定义一个函数<span class="math inline">\(q\)</span>将每个时间步的隐藏状态变成背景向量：<span class="math inline">\(c=q(h_1, \dots, h_T)\)</span>。</p><h2 id="decoder">decoder</h2><p>之前的编码器将整个输入序列的信息编码成了背景向量<span class="math inline">\(c\)</span>。而解码器就是根据背景信息输出序列<span class="math inline">\(y_1, y_2, \dots, y_{T&#39;}\)</span>。解码器每一步的输出要基于上一步的输出和背景向量，所以表示为<span class="math inline">\(P(y_{t&#39;}|y_1, \dots, y_{t&#39;-1}, c)\)</span>。</p><p>像机器翻译的时候，我们的解码器也会是一个循环网络。我们用<span class="math inline">\(g\)</span>表示这个循环网络的函数，那么当前步的隐藏状态<span class="math inline">\(s_{t&#39;}=g(y_{t&#39;-1}, c, s_{t&#39;-1})\)</span>。然后我就可以自定义一个输出层来计算输出序列的概率分布。</p><h2 id="损失函数">损失函数</h2><p>一般而言，会用最大似然法来最大化输出序列基于输入序列的条件概率： <span class="math display">\[\begin{split}\begin{aligned}\mathbb{P}(y_1, \ldots, y_{T&#39;} \mid x_1, \ldots, x_T)&amp;= \prod_{t&#39;=1}^{T&#39;} \mathbb{P}(y_{t&#39;} \mid y_1, \ldots, y_{t&#39;-1}, x_1, \ldots, x_T)\\&amp;= \prod_{t&#39;=1}^{T&#39;} \mathbb{P}(y_{t&#39;} \mid y_1, \ldots, y_{t&#39;-1}, \boldsymbol{c}),\end{aligned}\end{split}\]</span></p><p>因此损失函数可以表示为： <span class="math display">\[- \log\mathbb{P}(y_1, \ldots, y_{T&#39;} \mid x_1, \ldots, x_T) = -\sum_{t&#39;=1}^{T&#39;} \log \mathbb{P}(y_{t&#39;} \mid y_1, \ldots, y_{t&#39;-1}, \boldsymbol{c})\]</span></p><h1 id="beam-search">beam search</h1><p>通常情况下，我们会在输入和输出序列前后分别加一个特殊符号’&lt;bos&gt;‘和’&lt;eos&gt;‘，分别表示句子的开始和结束。不过很多时候好像’&lt;bos&gt;’不是必须加的，虽然我觉得不加很奇怪。</p><p>假设我们输出一段文本序列，那么输出辞典<span class="math inline">\(\mathcal{Y}\)</span>，大小为<span class="math inline">\(|\mathcal{Y}|\)</span>，输出的序列长度为<span class="math inline">\(T&#39;\)</span>，那么我们一共有<span class="math inline">\(|\mathcal{Y}|^{T&#39;}\)</span>种可能。</p><p>那么如果按照穷举检索，我们要评估的序列数量就是全部的可能性。假设我们有10000个词，输出长度为10的序列，那么我们的可能性就是<span class="math inline">\(10000^{10}\)</span>这么多种可能性。这几乎是不可能评估完的。</p><p>那么换个思路，如果每一次我们都只拿概率最高的那一个词，也就是说每一次拿的是<span class="math inline">\(y_{t&#39;} = \arg\max_{y_{t&#39;} \in \mathcal{Y}} P(y_{t&#39;}|y_1, \dots, y_{t&#39;-1}, c)\)</span>。只要遇到’&lt;eos&gt;’就停止检索。这就是一个非常典型的贪婪算法。这样的话我们的计算开销会显著下降。</p>但是贪婪算法会有典型的问题，就是检索空间太小，无法保证最优解。比如下图：<p align="center"><img src="https://i.loli.net/2018/09/06/5b90ebf798140.png"></p><p>这里的数字表示每一个state，ABC表示每一个词。中间的数字是条件概率，比如B2这里的0.4表示在<span class="math inline">\(P(B|A)\)</span>，而A2就是表示<span class="math inline">\(P(A|A)\)</span>。如果我们按照贪婪算法的话，我们会得到的结果是ABC，那么概率是<span class="math inline">\(0.5 \times 0.4 \times 0.2 \times 0.6\)</span>，而如果不是贪婪算法的话，我们得到ACB，概率是<span class="math inline">\(0.5 \times 0.3 \times 0.6 \times 0.6\)</span>明显概率更大。</p><p>所以我们为了保证有更大的概率可以检索到较多的可能性，我们可以采用束搜索的方法，也就是说，我们每一次不再只看概率最高的那一个词，而是看概率最高的数个词。我们用束宽（beam size）<span class="math inline">\(k\)</span>来表示。之后根据<span class="math inline">\(k\)</span>个候选词输出下一个阶段的序列，接着再选出概率最高的<span class="math inline">\(k\)</span>个序列，不断重复这件事情。最后我们会在各个状态的候选序列中筛选出包含特殊符号’&lt;eos&gt;’的序列，并将这个符号后的子序列舍弃，得到最后的输出序列。然后再在这些序列中选择分数最高的作为最后的输出序列： <span class="math display">\[\frac{1}{L^\alpha} \log \mathbb{P}(y_1, \ldots, y_{L}) = \frac{1}{L^\alpha} \sum_{t&#39;=1}^L \log \mathbb{P}(y_{t&#39;} \mid y_1, \ldots, y_{t&#39;-1}, \boldsymbol{c}),\]</span> 其中<span class="math inline">\(L\)</span>是最终序列的长度，<span class="math inline">\(\alpha\)</span>一般选0.75。这<span class="math inline">\(L\)</span>的系数起到的作用是惩罚太长的序列得分过高的情况。</p>事实上，贪婪搜索可以看做是beam size为1的束搜索。过程上就像下图：<p align="center"><img src="https://i.loli.net/2018/09/06/5b90f03697a3c.png"></p><p>那么不同于贪婪搜索，束搜索其实并不知道什么时候停下来，所以一般来说要定义一个最长的输出序列长度。</p><h1 id="attention">Attention</h1><p>前面说的解码器是将编码器的整个序列都作为背景来学习。那比如说机器翻译里面，我们翻译的时候其实可能没必要全部都看一遍，只要看一部分，然后就可以将这部分翻译出来。比如说“机器学习”翻译为“machine learning”，“机器”对应的是“machine”，而“学习”是“learning”，所以翻译machine的时候只要关注机器就可以了。</p>其实所谓的关注点，如果用数据来表示也就是权重大小，关注度越高权重越高。如下图：<p align="center"><img src="https://i.loli.net/2018/09/06/5b90fb86eac3c.png" width="70%"></p><p>我们在输出背景向量的时候做一个softmax，然后每一个state给一个权重，作为<span class="math inline">\(t&#39;\)</span>时刻的输入，这样jointly训练就可以学出一个attention的形式。</p>那么这里的<span class="math inline">\(\alpha\)</span>是这样计算出来的：<p align="center"><img src="https://i.loli.net/2018/09/06/5b90fd0f9519e.png" width="70%"></p><p>其实就是每一个state的decoder的input拿来和encoder的hidden做一个match。至于match的函数可以自己随意定义。</p><p>这样一来，我们就可以让解码器在不同的state的时候关注输入序列的不同部分。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这节课的内容讲的有点浅，所以我看到是李沐的gluon教程，配合这节课的内容。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://samaelchen.github.io/categories/DeepLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>台大李宏毅深度学习——Batch normalization &amp; SELU</title>
    <link href="https://samaelchen.github.io/deep_learning_step5/"/>
    <id>https://samaelchen.github.io/deep_learning_step5/</id>
    <published>2018-09-02T16:00:00.000Z</published>
    <updated>2018-10-09T13:26:34.255Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>学习一下batch normalization和SELU，顺便看点深度学习的八卦。</p><a id="more"></a><h1 id="bn">BN</h1><p>Batch normalization是一个比较新的深度学习技巧，但是在深度学习的实作中有非常迅速成为中流砥柱。</p><p>normalization是以前统计学习比较常用的一种方法，因为对于损失函数而言，<span class="math inline">\(L(y, \hat{y})\)</span>会受到输入数据的影响。这个其实是非常直观的，比如说一个数据有两个维度，一个维度都是1-10的范围内波动的，另一个维度是1000-10000之间波动的，那么如果<span class="math inline">\(y=x_1 + x_2\)</span>很明显后一个维度的数据对<span class="math inline">\(y\)</span>的影响非常大。</p><p>那么在这种情况下，我们做梯度下降，在scale大的维度上梯度就比较大，但是在scale小的地方梯度就比较小。这个在我之前学<a href="‘https://samaelchen.github.io/machine_learning_step3/’">梯度下降的博客</a>里面也有。大概图形上看就是下面这样：</p><p><img src="https://i.imgur.com/mb0vi91.png"></p><p>那这样我们在不同维度上的梯度下降步长是不一样的。所以在统计学习或者传统的机器学习里面，为了加快收敛的速度，虽然用二阶导可以解决，但是一般用feature scaling就可以了。</p><p>而batch normalization其实也是使用了这样的理念。一般而言，我们做normalization就是<span class="math inline">\(\frac{x-\mu}{\sigma}\)</span>，那batch normalization其实就是在每一个layer的input前做这么一下操作。</p><p>那batch normalization和normalization的差别其实就在于batch这个地方。我们知道平时我们训练深度学习网络的时候避免炸内存，会将数据分批导进去训练，在这种情况下，我们其实是没有办法得到全局的<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\sigma\)</span>的。所以事实上，batch normalization每一次算的都是一个batch的<span class="math inline">\(\mu \ \&amp; \ \sigma\)</span>。</p>那整个流程看上去就是下图这样的：<p align="center"><img src="https://i.loli.net/2018/09/04/5b8e3c359e425.png" width="70%"></p><p>那实际上可以将这个过程看作是一个hidden layer来处理。</p>如果说觉得这样全部normalization到0，1这样的形式可能有些activation function效果不好，所以我们可以考虑一下再加一层linear layer来转换一下，那流程上就是：<p align="center"><img src="https://i.loli.net/2018/09/04/5b8e3e03c9485.png" width="70%"></p><p>当然，如果好巧不巧，机器学着学着，刚好<span class="math inline">\(\beta\)</span>和<span class="math inline">\(\gamma\)</span>跟前面的一样，那么这轮的batch normalization就白做了。不过一般来说不会这么巧。</p><p>那么在训练过程中，我们一般都是一个batch一个batch喂进去，但是test的时候，我们一般是一口气全部过模型一遍，那么我们并没有办法得到一个合适的<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\sigma\)</span>。那么一种解决方法是计算一下全部training set的均值和标准差，另一种方法是，每次训练后，我们都保留最后一个batch的均值和标准差。</p><p>BN的好处非常显而易见，一个是可以减少covariate shift。也就是说，以前为了避免每个layer的方差太大，我们会减小步长，但是用了BN以后就可以用大的步长加速训练。此外，对于sigmoid或者tanh这样的激活函数来说，可以有效减少深层网络的梯度爆炸或者消失的问题。另外BN的一个副产物是可以减少过拟合。</p><h1 id="selu">SELU</h1><p>ReLu是一种比较特殊的激活函数，本身是为了解决sigmoid在叠加多层后会出现梯度消失的问题。ReLu的函数其实非常简单，就是： <span class="math display">\[a =\begin{cases}0, &amp;\mbox{if }z&lt;0 \\z, &amp;\mbox{if }z&gt;0\end{cases}\]</span> 不过现在回过头看ReLu，其实某种程度上效果很像是dropout？！</p><p>但是ReLu相对来说还是比较激进的，所以后来有各种各样的变种，比如说Leaky ReLu，就是： <span class="math display">\[a =\begin{cases}0.01z, &amp;\mbox{if }z&lt;0 \\z, &amp;\mbox{if }z&gt;0\end{cases}\]</span> 还有parametric ReLu： <span class="math display">\[a =\begin{cases}\alpha z, &amp;\mbox{if }z&lt;0 \\z, &amp;\mbox{if }z&gt;0\end{cases}\]</span></p><p>再后来在竞赛中还有人提出了randomized relu，其实就是上面的parametric relu的<span class="math inline">\(\alpha\)</span>每次训练的时候都随机生成一个，而不是让机器去学习，然后test的时候再固定一个就可以了。据说效果还不错。</p><p>但是这种形式的ReLu都是负无穷到正无穷的值域，于是又有人修正为ELU（exponential linear unit），函数是： <span class="math display">\[a =\begin{cases}\alpha(e^z - 1), &amp;\mbox{if }z&lt;0 \\z, &amp;\mbox{if }z&gt;0\end{cases}\]</span> 这样一来，ELU的值域就是<span class="math inline">\(\alpha\)</span>到正无穷。</p><p>之后横空出世了一个SELU，其实就是ELU前面乘了一个参数<span class="math inline">\(\lambda\)</span>，函数表示为： <span class="math display">\[a =\lambda \begin{cases}\alpha(e^z - 1), &amp; \mbox{if }z&lt;0 \\z, &amp; \mbox{if }z&gt;0\end{cases}\]</span> 不过，这里的两个参数是有确定值的，而不是随便学习出来的。这里<span class="math inline">\(\alpha=1.6732632423543772848170429916717\)</span>，<span class="math inline">\(\lambda=1.0507009873554804934193349852946\)</span>。</p><p>这两个非常神奇的数据说是可以推导出来的，有兴趣的同学可以去看一下原文93页的证明。看不下去的可以看一下作者放出来的<a href="https://github.com/bioinf-jku/SNNs" target="_blank" rel="noopener">源码</a>。</p><p>那么为什么要定这样两个实数，其实目的是保证每次的layer吐出来的都是一个标志正态分布的数据。</p><h1 id="花式调参">花式调参</h1><p>最后是现在有的一些花式调参的方法。毕竟实作的时候基本上也就是调参了，菜如我这种也不可能提出什么突破性的方法。</p><p>深度学习说白了也就是机器学习的一种，所以传统机器学习中的grid search这种非常暴力的方法当然也适用。不过为了加速搜索，一般会用random search的方法，通常也不会太差。</p>另外现在有一些非常非常骚气的方法，一种就是learn to learn。其实就是用一个RNN去学习另一个网络的所有参数。看上去就是下图的样子：<p align="center"><img src="https://i.loli.net/2018/09/05/5b8f7f31490b4.png" width="70%"></p><p>还有一个很重要的调参方向其实就是learning rate，因为深度学习很多时候是一个非凸优化的问题，所以我们以为loss下不去了可能待在了saddle point，实际上也可能是在一个local minimum的山谷里来回震荡。这种时候只要降低lr就可以继续收敛了。所以很多时候我们在训练的过程中，每50个epoch或者100个epoch就缩小一下lr，很多时候loss会出现一次很明显的降低。</p><p>最后是Google brain提出了一些非常神奇的激活函数，具体可以看看这篇<a href="https://arxiv.org/pdf/1710.05941.pdf" target="_blank" rel="noopener">论文</a>。</p><h1 id="深度学习究竟有没有学到东西">深度学习究竟有没有学到东西</h1>这个其实是非常有意思的一个争论点。很多人质疑深度学习其实只是强行记忆了数据的特征，并没有学到潜在的规律。于是有人做了相关的研究，<a href="https://arxiv.org/pdf/1706.05394.pdf" target="_blank" rel="noopener">A Closer Look at Memorization in Deep Networks</a>这篇论文就是相关的研究，里面有一个很有意思的地方就是对label加noise。不论加了多少noise，模型都可以train到一个百分百正确的地方。但是test上的表现很自然会变得很差。过程如下图：<p align="center"><img src="https://i.loli.net/2018/09/05/5b8f861ddf832.png" width="70%"></p><p>这个其实是非常风骚的一个操作，就是说故意给一些错误的信息让机器去学习。这个图里面的实线是train，虚线是test，我们可以看到其实一开始test是上升的，然后才下降。所以实际上一开始模型还是正常学到了一些正确的规律的。但是后面就被噪声带跑偏了。</p><p>不过从某种程度上来说，传统的决策树不是更像是强行记住一些东西么。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;学习一下batch normalization和SELU，顺便看点深度学习的八卦。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://samaelchen.github.io/categories/DeepLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>李宏毅深度学习作业——language model</title>
    <link href="https://samaelchen.github.io/pytorch_cloze/"/>
    <id>https://samaelchen.github.io/pytorch_cloze/</id>
    <published>2018-08-27T16:00:00.000Z</published>
    <updated>2018-10-09T13:26:34.259Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>之前用LSTM做过情感分析，李宏毅老师17年的课程第一个大作业是做一个完形填空的language model，试着做了一个简单的demo。 <a id="more"></a></p><p>做完型填空其实很直观，就是跟CBOW很像，我们按照上下文猜被挖掉的那个词是什么。</p><p>这次用的还是之前训词向量的语料库，因为那个都是小说原文，所以我们要把数据揉成我们想要的形式，也就是context包含上下文，中间空掉的词是我们的target。</p><p>然后因为要训练LSTM，所以我们会再做一个padding的工作，最后看起来大概会是这样的： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ <span class="number">9405</span>,  <span class="number">1236</span>,  <span class="number">6282</span>,   <span class="number">371</span>,  <span class="number">1968</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">6085</span>, <span class="number">10586</span>,   <span class="number">900</span>,  <span class="number">7561</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>]]])</span><br></pre></td></tr></table></figure></p><p>形式上是<span class="math inline">\(2 \times \text{batch_size} \times \text{seq_len}\)</span>。</p><p>网络的设置非常简单，前半部分过一个LSTM，后半部分过一个LSTM，然后将这两个网络的output拼到一起最后过一个fc。</p><p>这里因为有可能完形填空的时候空的是第一个词或者是最后一个词，所以我们会在句子开头和结尾加上<bos>和<eos>的标志。</eos></bos></p><p>一个示例可以看这个<a href="‘https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/langage%20model/LSTM-Full-text-Copy1.ipynb’">notebook</a>。</p><p>这个notebook的脚本没啥通用性，一个是其实没有解决unknown的词的问题，另外是没有解决训练效率的问题。PyTorch没有nce_loss或者是negative sampling这样的loss function，所以后面用softmax做cross entropy的时候复杂度是O(vocab_size)。之前写的negative sampling是针对word2vec写的，所以没什么通用性，看了其他人写的通用性的nce或者negative sampling，总感觉哪里怪怪的。后面还是要考虑自己实现一个。有点烦(╯﹏╰)。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前用LSTM做过情感分析，李宏毅老师17年的课程第一个大作业是做一个完形填空的language model，试着做了一个简单的demo。
    
    </summary>
    
      <category term="深度学习" scheme="https://samaelchen.github.io/categories/DeepLearning/"/>
    
      <category term="PyTorch" scheme="https://samaelchen.github.io/categories/PyTorch/"/>
    
    
  </entry>
  
  <entry>
    <title>PyTorch实现LSTM情感分析</title>
    <link href="https://samaelchen.github.io/pytorch_lstm_sentiment/"/>
    <id>https://samaelchen.github.io/pytorch_lstm_sentiment/</id>
    <published>2018-08-14T16:00:00.000Z</published>
    <updated>2018-10-09T13:26:34.259Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>2018.08.16更新一个textCNN。</p><p>尝试使用LSTM做情感分析，这个gluon有非常详细的例子，可以直接参考gluon的<a href="http://zh.gluon.ai/chapter_natural-language-processing/sentiment-analysis.html" target="_blank" rel="noopener">官方教程</a>。这里尝试使用PyTorch复现一个。数据用的是IMDB的数据<a href="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz" class="uri" target="_blank" rel="noopener">http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz</a></p><a id="more"></a><p>首先我们导入相关的package：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.utils.data.dataloader <span class="keyword">as</span> dataloader</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torchtext.vocab <span class="keyword">as</span> torchvocab</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> snowballstemmer</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> chain</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br></pre></td></tr></table></figure><p>然后我们定义读数的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readIMDB</span><span class="params">(path, seg=<span class="string">'train'</span>)</span>:</span></span><br><span class="line">    pos_or_neg = [<span class="string">'pos'</span>, <span class="string">'neg'</span>]</span><br><span class="line">    data = []</span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> pos_or_neg:</span><br><span class="line">        files = os.listdir(os.path.join(path, seg, label))</span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">            <span class="keyword">with</span> open(os.path.join(path, seg, label, file), <span class="string">'r'</span>, encoding=<span class="string">'utf8'</span>) <span class="keyword">as</span> rf:</span><br><span class="line">                review = rf.read().replace(<span class="string">'\n'</span>, <span class="string">''</span>)</span><br><span class="line">                <span class="keyword">if</span> label == <span class="string">'pos'</span>:</span><br><span class="line">                    data.append([review, <span class="number">1</span>])</span><br><span class="line">                <span class="keyword">elif</span> label == <span class="string">'neg'</span>:</span><br><span class="line">                    data.append([review, <span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">train_data = readIMDB(<span class="string">'aclImdb'</span>)</span><br><span class="line">test_data = readIMDB(<span class="string">'aclImdb'</span>, <span class="string">'test'</span>)</span><br></pre></td></tr></table></figure><p>接着是分词，这里只做非常简单的分词，也就是按照空格分词。当然按照一些传统的清洗方式效果会更好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenizer</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [tok.lower() <span class="keyword">for</span> tok <span class="keyword">in</span> text.split(<span class="string">' '</span>)]</span><br><span class="line"></span><br><span class="line">train_tokenized = []</span><br><span class="line">test_tokenized = []</span><br><span class="line"><span class="keyword">for</span> review, score <span class="keyword">in</span> train_data:</span><br><span class="line">    train_tokenized.append(tokenizer(review))</span><br><span class="line"><span class="keyword">for</span> review, score <span class="keyword">in</span> test_data:</span><br><span class="line">    test_tokenized.append(tokenizer(review))</span><br><span class="line"></span><br><span class="line">vocab = set(chain(*train_tokenized))</span><br><span class="line">vocab_size = len(vocab)</span><br></pre></td></tr></table></figure><p>因为这个数据集非常小，所以如果我们用这个数据集做word embedding有可能过拟合，而且模型没有通用性，所以我们传入一个已经学好的word embedding。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">wvmodelwvmodel = gensim.models.KeyedVectors.load_word2vec_format(<span class="string">'test_word.txt'</span>,</span><br><span class="line">                                                          binary=<span class="keyword">False</span>, encoding=<span class="string">'utf-8'</span>)</span><br></pre></td></tr></table></figure><p>这里的“test_word.txt”是我将glove的词向量转换后的结果，当时测试gensim的这个功能瞎起的名字，用的是glove的6B，100维的预训练数据。</p><p>然后一样要定义一个word to index的词典：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">word_to_idxword_to  = &#123;word: i+<span class="number">1</span> <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(vocab)&#125;</span><br><span class="line">word_to_idx[<span class="string">'&lt;unk&gt;'</span>] = <span class="number">0</span></span><br><span class="line">idx_to_word = &#123;i+<span class="number">1</span>: word <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(vocab)&#125;</span><br><span class="line">idx_to_word[<span class="number">0</span>] = <span class="string">'&lt;unk&gt;'</span></span><br></pre></td></tr></table></figure><p>定义的目的是为了将预训练的weight跟我们的词库拼上。另外我们定义了一个unknown的词，也就是说没有出现在训练集里的词，我们都叫做unknown，词向量就定义为0。</p><p>然后就是编码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode_samples</span><span class="params">(tokenized_samples, vocab)</span>:</span></span><br><span class="line">    features = []</span><br><span class="line">    <span class="keyword">for</span> sample <span class="keyword">in</span> tokenized_samples:</span><br><span class="line">        feature = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> sample:</span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">in</span> word_to_idx:</span><br><span class="line">                feature.append(word_to_idx[token])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                feature.append(<span class="number">0</span>)</span><br><span class="line">        features.append(feature)</span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_samples</span><span class="params">(features, maxlen=<span class="number">500</span>, PAD=<span class="number">0</span>)</span>:</span></span><br><span class="line">    padded_features = []</span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> features:</span><br><span class="line">        <span class="keyword">if</span> len(feature) &gt;= maxlen:</span><br><span class="line">            padded_feature = feature[:maxlen]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            padded_feature = feature</span><br><span class="line">            <span class="keyword">while</span>(len(padded_feature) &lt; maxlen):</span><br><span class="line">                padded_feature.append(PAD)</span><br><span class="line">        padded_features.append(padded_feature)</span><br><span class="line">    <span class="keyword">return</span> padded_features</span><br></pre></td></tr></table></figure><p>我们这里为了解决评论长度不一致的问题，将所有的评论都取500个词，超过的就取前500个，不足的补0。</p><p>整理一下训练数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_features = torch.tensor(pad_samples(encode_samples(train_tokenized, vocab)))</span><br><span class="line">train_labels = torch.tensor([score <span class="keyword">for</span> _, score <span class="keyword">in</span> train_data])</span><br><span class="line">test_features = torch.tensor(pad_samples(encode_samples(test_tokenized, vocab)))</span><br><span class="line">test_labels = torch.tensor([score <span class="keyword">for</span> _, score <span class="keyword">in</span> test_data])</span><br></pre></td></tr></table></figure><p>然后就是定义网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentimentNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 bidirectional, weight, labels, use_gpu, **kwargs)</span>:</span></span><br><span class="line">        super(SentimentNet, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.use_gpu = use_gpu</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(weight)</span><br><span class="line">        self.embedding.weight.requires_grad = <span class="keyword">False</span></span><br><span class="line">        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=self.num_hiddens,</span><br><span class="line">                               num_layers=num_layers, bidirectional=self.bidirectional,</span><br><span class="line">                               dropout=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            self.decoder = nn.Linear(num_hiddens * <span class="number">4</span>, labels)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.decoder = nn.Linear(num_hiddens * <span class="number">2</span>, labels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        embeddings = self.embedding(inputs)</span><br><span class="line">        states, hidden = self.encoder(embeddings.permute([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]))</span><br><span class="line">        encoding = torch.cat([states[<span class="number">0</span>], states[<span class="number">-1</span>]], dim=<span class="number">1</span>)</span><br><span class="line">        outputs = self.decoder(encoding)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><p>那这里需要注意几个点，第一，LSTM可以不initialize hidden，如果不initialize的话，那么PyTorch会默认初始为0。</p><p>另外就是LSTM这里传进去的数据格式是[seq_len, batch_size, embedded_size]。而我们传进去的数据是[batch_size, seq_len]的样子，那经过embedding之后的结果是[batch_size, seq_len, embedded_size]。所以我们这里要将第二个维度和第一个维度做个调换。而LSTM这边output的dimension和inputs是一致的，如果这里我们不做维度的调换，可以将LSTM的batch_first参数设置为True。然后我们要拿到每个batch的初始状态和最后状态还是一样要去做一个第一第二维度的调换。这里非常的绕，我在这里卡了好久(=<span class="citation" data-cites="__">@__</span>@=)</p><p>第三就是我这里用了最初始的状态和最后的状态拼起来作为分类的输入。</p><p>另外有一点吐槽的就是，MXNet的dense层比较强大啊，不用定义输入的维度，只要定义输出的维度就可以了，操作比较骚啊。</p><p>然后我们把weight导进来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">weight = torch.zeros(vocab_size+<span class="number">1</span>, embed_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(wvmodel.index2word)):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        index = word_to_idx[wvmodel.index2word[i]]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    weight[index, :] = torch.from_numpy(wvmodel.get_vector(</span><br><span class="line">        idx_to_word[word_to_idx[wvmodel.index2word[i]]]))</span><br></pre></td></tr></table></figure><p>这里我们将不在glove里面的词全部填为0，后面想了一下，其实也可以试试这些全部随机试试。</p><p>接着定义参数就可以训练了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">embed_size = <span class="number">100</span></span><br><span class="line">num_hiddens = <span class="number">100</span></span><br><span class="line">num_layers = <span class="number">2</span></span><br><span class="line">bidirectional = <span class="keyword">True</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">labels = <span class="number">2</span></span><br><span class="line">lr = <span class="number">0.8</span></span><br><span class="line">device = torch.device(<span class="string">'cuda:0'</span>)</span><br><span class="line">use_gpu = <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">net = SentimentNet(vocab_size=(vocab_size+<span class="number">1</span>), embed_size=embed_size,</span><br><span class="line">                   num_hiddens=num_hiddens, num_layers=num_layers,</span><br><span class="line">                   bidirectional=bidirectional, weight=weight,</span><br><span class="line">                   labels=labels, use_gpu=use_gpu)</span><br><span class="line">net.to(device)</span><br><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=lr)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train_set = torch.utils.data.TensorDataset(train_features, train_labels)</span><br><span class="line">test_set = torch.utils.data.TensorDataset(test_features, test_labels)</span><br><span class="line"></span><br><span class="line">train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size,</span><br><span class="line">                                         shuffle=<span class="keyword">True</span>)</span><br><span class="line">test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size,</span><br><span class="line">                                        shuffle=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p>这个位置需要注意的是，我们在train加了一个shuffle，如果不加shuffle的话，模型会学到奇奇怪怪的地方去。</p><p>最后训练一下就好了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    start = time.time()</span><br><span class="line">    train_loss, test_losses = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    train_acc, test_acc = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    n, m = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> feature, label <span class="keyword">in</span> train_iter:</span><br><span class="line">        n += <span class="number">1</span></span><br><span class="line">        net.zero_grad()</span><br><span class="line">        feature = Variable(feature.cuda())</span><br><span class="line">        label = Variable(label.cuda())</span><br><span class="line">        score = net(feature)</span><br><span class="line">        loss = loss_function(score, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        train_acc += accuracy_score(torch.argmax(score.cpu().data,</span><br><span class="line">                                                 dim=<span class="number">1</span>), label.cpu())</span><br><span class="line">        train_loss += loss</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> test_feature, test_label <span class="keyword">in</span> test_iter:</span><br><span class="line">            m += <span class="number">1</span></span><br><span class="line">            test_feature = test_feature.cuda()</span><br><span class="line">            test_label = test_label.cuda()</span><br><span class="line">            test_score = net(test_feature)</span><br><span class="line">            test_loss = loss_function(test_score, test_label)</span><br><span class="line">            test_acc += accuracy_score(torch.argmax(test_score.cpu().data,</span><br><span class="line">                                                    dim=<span class="number">1</span>), test_label.cpu())</span><br><span class="line">            test_losses += test_loss</span><br><span class="line">    end = time.time()</span><br><span class="line">    runtime = end - start</span><br><span class="line">    print(<span class="string">'epoch: %d, train loss: %.4f, train acc: %.2f, test loss: %.4f, test acc: %.2f, time: %.2f'</span> %</span><br><span class="line">          (epoch, train_loss.data / n, train_acc / n, test_losses.data / m, test_acc / m, runtime))</span><br></pre></td></tr></table></figure><p>也可以直接看我的<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/langage%20model/lstm-sentiment.ipynb" target="_blank" rel="noopener">notebook</a></p><p>后面试试textCNN，感觉也挺骚气的。</p><hr><p>2018.08.16 更新一个textCNN的玩法。</p><p>CNN太熟了，很容易搞，其实只要把网络改一下，其他的动都不用动：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">textCNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, seq_len, labels, weight, **kwargs)</span>:</span></span><br><span class="line">        super(textCNN, self).__init__(**kwargs)</span><br><span class="line">        self.labels = labels</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(weight)</span><br><span class="line">        self.embedding.weight.requires_grad = <span class="keyword">False</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, (<span class="number">3</span>, embed_size))</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, (<span class="number">4</span>, embed_size))</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, (<span class="number">5</span>, embed_size))</span><br><span class="line">        self.pool1 = nn.MaxPool2d((seq_len - <span class="number">3</span> + <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.pool2 = nn.MaxPool2d((seq_len - <span class="number">4</span> + <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.pool3 = nn.MaxPool2d((seq_len - <span class="number">5</span> + <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.linear = nn.Linear(<span class="number">3</span>, labels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        inputs = self.embedding(inputs).view(inputs.shape[<span class="number">0</span>], <span class="number">1</span>, inputs.shape[<span class="number">1</span>], <span class="number">-1</span>)</span><br><span class="line">        x1 = F.relu(self.conv1(inputs))</span><br><span class="line">        x2 = F.relu(self.conv2(inputs))</span><br><span class="line">        x3 = F.relu(self.conv3(inputs))</span><br><span class="line"></span><br><span class="line">        x1 = self.pool1(x1)</span><br><span class="line">        x2 = self.pool2(x2)</span><br><span class="line">        x3 = self.pool3(x3)</span><br><span class="line"></span><br><span class="line">        x = torch.cat((x1, x2, x3), <span class="number">-1</span>)</span><br><span class="line">        x = x.view(inputs.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, self.labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span>(x)</span><br></pre></td></tr></table></figure><p>这里的网络设计很简单，就是用三个filter去扫一遍文章，filter的尺寸其实就是我们一次看多少个词。这样扫完以后是三个向量，然后pooling一下得到三个实数。把这三个实数拼成一个向量，然后用fc分类一下就结束了。</p><p>然后初始化网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = textCNN(vocab_size=(vocab_size+<span class="number">1</span>), embed_size=embed_size,</span><br><span class="line">              seq_len=<span class="number">500</span>, labels=labels, weight=weight)</span><br></pre></td></tr></table></figure><p>其他的都没改，就可以直接跑了。速度上CNN比LSTM的参数少，速度快很多，不过只跑几轮的话效果差一点。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2018.08.16更新一个textCNN。&lt;/p&gt;
&lt;p&gt;尝试使用LSTM做情感分析，这个gluon有非常详细的例子，可以直接参考gluon的&lt;a href=&quot;http://zh.gluon.ai/chapter_natural-language-processing/sentiment-analysis.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官方教程&lt;/a&gt;。这里尝试使用PyTorch复现一个。数据用的是IMDB的数据&lt;a href=&quot;http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="PyTorch" scheme="https://samaelchen.github.io/categories/PyTorch/"/>
    
    
  </entry>
  
  <entry>
    <title>word2vec的PyTorch实现</title>
    <link href="https://samaelchen.github.io/word2vec_pytorch/"/>
    <id>https://samaelchen.github.io/word2vec_pytorch/</id>
    <published>2018-08-01T16:00:00.000Z</published>
    <updated>2018-10-09T13:26:34.259Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Word Embedding现在是现在NLP的入门必备，这里简单实现一个CBOW的W2V。</p><p>2018-07-06更新一发用一篇小说来训练模型的脚本。</p><p>2018-08-02更新一发negative sampling版本。</p><a id="more"></a><h1 id="negtive-sampling版本">negtive sampling版本</h1><p><strong>2018-08-02更新基于negative sampling方法的W2V</strong></p><p>翻了之前项亮实现的MXNet版本的NCE，看的不甚理解，感觉他写的那个是NEG的样子，然后还是自己写一个简单的negative sampling来做这个事情。关于NCE和NEG的区别，其实NEG就像是NCE的一个特殊情况，这个可以看<a href="https://arxiv.org/pdf/1410.8251.pdf" target="_blank" rel="noopener">Notes on Noise Contrastive Estimation and Negative Sampling</a>，或者是谷歌的一篇<a href="https://www.tensorflow.org/extras/candidate_sampling.pdf" target="_blank" rel="noopener">总结</a>。</p><p>关于negative sampling这里简单介绍一下，其实负采样的思路非常的简单，就是原来我们有多少个词，那么softmax就要算多少个词的概率，用负采样的方法就是将原来这样的巨量分类问题变成一个简单的二分类问题。也就是说，原来正确的label依然保留，接着只要sample出一小部分的负样本出来，然后做一个二分类问题就可以了。至于需要sample多少负样本，谷歌的C版本中是用了5个，好像哪里见过说不超过25个就可以了，但是现在忘了是哪篇文章了，可能不准确O__O &quot;…</p><p>具体的公式推导其实很简单，可以看一下gluon关于<a href="http://zh.gluon.ai/chapter_natural-language-processing/word2vec.html" target="_blank" rel="noopener">负采样的介绍</a>。</p><p>所以实际上要实现这个负采样非常的容易，只要设计一个抽样分布，然后开始抽样就可以了。在很多词向量的资料里面都说到了，采样分布选用的是： <span class="math display">\[P(w_i) = \frac{f(w_i)^{0.75}}{\sum(f(w_j)^{0.75})}\]</span> 这个其实非常像softmax，就是说用单个词的词频除以全部词频的和，原来的代码中加入了0.75的这个幂指数，完全是炼丹经验。</p><p>然后网上参考了一个开源的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NEGLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ix_to_word, word_freqs, num_negative_samples=<span class="number">5</span>,)</span>:</span></span><br><span class="line">        super(NEGLoss, self).__init__()</span><br><span class="line">        self.num_negative_samples = num_negative_samples</span><br><span class="line">        self.vocab_size = len(word_freqs)</span><br><span class="line">        self.dist = F.normalize(torch.Tensor(</span><br><span class="line">            [word_freqs[ix_to_word[i]] <span class="keyword">for</span> i <span class="keyword">in</span> range(self.vocab_size)]).pow(<span class="number">0.75</span>), dim=<span class="number">0</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, num_samples, positives=[])</span>:</span></span><br><span class="line">        weights = torch.zeros((self.vocab_size, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> positives:</span><br><span class="line">            weights[w] += <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_samples):</span><br><span class="line">            w = torch.multinomial(self.dist, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">while</span>(w <span class="keyword">in</span> positives):</span><br><span class="line">                w = torch.multinomial(self.dist, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">            weights[w] += <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, target)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.nll_loss(input, target,</span><br><span class="line">                          self.sample(self.num_negative_samples,</span><br><span class="line">                                      positives=target.data.numpy()))</span><br></pre></td></tr></table></figure><p>但是有个小问题就是，这里采用的其实是很取巧的一个方法，就是说，我每次会生成一个矩阵告诉pytorch究竟有哪6个sample被我拿到了，然后算negative log likelihood的时候就只算这6个。结果上来说，是实现了负采样，但是从算法效率上来说，其实并没有起到减少计算量的效果。</p><p>所以这里我们实现一个非常简单，类似nagative sampling，但是不是非常严格的采样函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neg_sample</span><span class="params">(num_samples, positives=[])</span>:</span></span><br><span class="line">    freqs_pow = torch.Tensor([freqs[ix_to_word[i]] <span class="keyword">for</span> i <span class="keyword">in</span> range(vocab_size)]).pow(<span class="number">0.75</span>)</span><br><span class="line">    dist = freqs_pow / freqs_pow.sum()</span><br><span class="line">    w = np.random.choice(len(dist), (len(positives), num_samples), p=dist.numpy())</span><br><span class="line">    <span class="keyword">if</span> positives.is_cuda:</span><br><span class="line">        <span class="keyword">return</span> torch.tensor(w).to(device)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.tensor(w)</span><br></pre></td></tr></table></figure><p>然后相应的，我们需要将我们的CBOW也变一下，按照 <span class="math display">\[-\text{log} \frac{1}{1+\text{exp}\left(-u_c^\top (v_{o_1} + \ldots + v_{o_{2m}}) /(2m)\right)}  - \sum_{k=1, w_k \sim \mathbb{P}(w)}^K \text{log} \frac{1}{1+\text{exp}\left((u_{i_k}^\top (v_{o_1} + \ldots + v_{o_{2m}}) /(2m)\right)}.\]</span> 这个公式计算最后的loss。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOW</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, context_size)</span>:</span></span><br><span class="line">        super(CBOW, self).__init__()</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.embeddings.weight.data.uniform_(<span class="number">-0.5</span> / vocab_size, <span class="number">0.5</span> / vocab_size)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, label)</span>:</span></span><br><span class="line">        negs = neg_sample(<span class="number">5</span>, label)</span><br><span class="line">        u_embeds = self.embeddings(label).view(len(label), <span class="number">-1</span>)</span><br><span class="line">        v_embeds_pos = self.embeddings(inputs).mean(dim=<span class="number">1</span>)</span><br><span class="line">        v_embeds_neg = self.embeddings(negs).mean(dim=<span class="number">1</span>)</span><br><span class="line">        loss1 = torch.diag(torch.matmul(u_embeds, v_embeds_pos.transpose(<span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">        loss2 = torch.diag(torch.matmul(u_embeds, v_embeds_neg.transpose(<span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">        loss1 = -torch.log(<span class="number">1</span> / (<span class="number">1</span> + torch.exp(-loss1)))</span><br><span class="line">        loss2 = -torch.log(<span class="number">1</span> / (<span class="number">1</span> + torch.exp(loss2)))</span><br><span class="line">        loss = (loss1.mean() + loss2.mean())</span><br><span class="line">        <span class="keyword">return</span>(loss)</span><br></pre></td></tr></table></figure><p>这里我将embedding层的权重进行了标准化，通过这样的标准化可以避免后面计算loss的时候出现无穷大的情况。然后其他参数不用做什么变化，开始训练看看效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    total_loss = torch.Tensor([<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> context, target <span class="keyword">in</span> tqdm(data_iter):</span><br><span class="line">        context_ids = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(context[<span class="number">0</span>])):</span><br><span class="line">            context_ids.append(make_context_vector([context[j][i] <span class="keyword">for</span> j <span class="keyword">in</span> range(len(context))], word_to_ix))</span><br><span class="line">        context_ids = torch.stack(context_ids)</span><br><span class="line">        context_ids = context_ids.to(device)</span><br><span class="line">        model.zero_grad()</span><br><span class="line">        label = make_context_vector(target, word_to_ix)</span><br><span class="line">        label = label.to(device)</span><br><span class="line">        loss = model(context_ids, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    losses.append(total_loss)</span><br><span class="line">    print(<span class="string">'epoch %d loss %.4f'</span> %(epoch, total_loss))</span><br><span class="line">print(losses)</span><br></pre></td></tr></table></figure><p>完整的notebook可以看<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/langage%20model/w2v_ngs.ipynb" target="_blank" rel="noopener">这个</a>，效率上有质的提升。batchsize还是1024的时候大概压缩到15分钟左右，放到8192的时候大概一个epoch是10分钟。一本满足。</p><hr><h1 id="toy-版本">toy 版本</h1><p>首先import必要的模块： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br></pre></td></tr></table></figure></p><p>CBOW的全称是continuous bag of words。和传统的N-gram相比，CBOW会同时左右各看一部分词。也就是说，根据左右两边的词，猜测中间的词是什么。而传统的N-gram是根据前面的词，猜后面的词是什么。在PyTorch的官网上给出了N-gram的实现。因此我们只需要在这个基础上进行简单的修改就可以得到基于CBOW的W2V模型。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">CONTEXT_SIZE = <span class="number">2</span>  <span class="comment"># 2 words to the left, 2 to the right</span></span><br><span class="line">raw_text = <span class="string">"""We are about to study the idea of a computational process.</span></span><br><span class="line"><span class="string">Computational processes are abstract beings that inhabit computers.</span></span><br><span class="line"><span class="string">As they evolve, processes manipulate other abstract things called data.</span></span><br><span class="line"><span class="string">The evolution of a process is directed by a pattern of rules</span></span><br><span class="line"><span class="string">called a program. People create programs to direct processes. In effect,</span></span><br><span class="line"><span class="string">we conjure the spirits of the computer with our spells."""</span>.split()</span><br><span class="line"></span><br><span class="line"><span class="comment"># By deriving a set from `raw_text`, we deduplicate the array</span></span><br><span class="line">vocab = set(raw_text)</span><br><span class="line">vocab_size = len(vocab)</span><br><span class="line"></span><br><span class="line">word_to_ix = &#123;word: i <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(vocab)&#125;</span><br><span class="line">data = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, len(raw_text) - <span class="number">2</span>):</span><br><span class="line">    context = [raw_text[i - <span class="number">2</span>], raw_text[i - <span class="number">1</span>],</span><br><span class="line">               raw_text[i + <span class="number">1</span>], raw_text[i + <span class="number">2</span>]]</span><br><span class="line">    target = raw_text[i]</span><br><span class="line">    data.append((context, target))</span><br><span class="line">print(data[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure><p>首先定义我们需要的数据。这里的CBOW的Windows是2。因为单词没法直接拿来训练，因此这里我们用id来唯一标识每一个单词。然后我们需要做的一个事情就是将这些id编码成向量。14年谷歌放出来的C那一版我印象中是用的霍夫曼树再降维，现在的PyTorch和gluon都有embedding的类，可以将分类的数据直接编码成向量。所以我们现在用框架实现这个事情就非常简单了。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOW</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, context_size)</span>:</span></span><br><span class="line">        super(CBOW, self).__init__()</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.linear1 = nn.Linear(context_size * embedding_dim, <span class="number">128</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">128</span>, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        embeds = self.embeddings(inputs).view((<span class="number">1</span>, <span class="number">-1</span>))</span><br><span class="line">        out = F.relu(self.linear1(embeds))</span><br><span class="line">        out = self.linear2(out)</span><br><span class="line">        log_probs = F.log_softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span>(log_probs)</span><br></pre></td></tr></table></figure><p>这个CBOW的类很简单，继承了PyTorch的Module类，然后第一步我们就做了一个embedding，然后做了一个隐藏层和一个输出层。最后我们做了一个softmax的动作来得到probability。这就是我们需要训练的神经网络。所以一直说W2V是一个单层的神经网络就是这个原因。</p><p>然后我们定义一个简单的函数，将单词转变成id <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_context_vector</span><span class="params">(context, word_to_ix)</span>:</span></span><br><span class="line">    idxs = [word_to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> context]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(idxs, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">make_context_vector(data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix)</span><br></pre></td></tr></table></figure></p><p>接着定义一些需要的参数： <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">'cuda:0'</span>)</span><br><span class="line">losses = []</span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">model = CBOW(len(vocab), embedding_dim=<span class="number">10</span>, context_size=CONTEXT_SIZE*<span class="number">2</span>)</span><br><span class="line">model.to(device)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure></p><p>这里需要注意一点，context_size需要windows的大小乘2，因为CBOW同时左右都看了这些词，所以我们放进来的词实际上是windows乘2的数量。</p><p>这里我用了GPU来加速计算。如果没有GPU的可以注释掉所有跟device相关的代码，这个数据量不大，体会不到GPU的优势。</p><p>然后就是正式训练 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    total_loss = torch.Tensor([<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> context, target <span class="keyword">in</span> data:</span><br><span class="line">        context_ids = make_context_vector(context, word_to_ix)</span><br><span class="line">        context_ids = context_ids.to(device)</span><br><span class="line">        model.zero_grad()</span><br><span class="line">        log_probs = model(context_ids)</span><br><span class="line">        label = torch.tensor([word_to_ix[target]], dtype=torch.long)</span><br><span class="line">        label = label.to(device)</span><br><span class="line">        loss = loss_function(log_probs, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    losses.append(total_loss)</span><br><span class="line">print(losses)</span><br></pre></td></tr></table></figure></p><p>这样就是一个词向量的训练过程。如果我们需要得到embedding之后的结果，只需要将数据过一遍embedding这一层就可以了。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.embeddings(make_context_vector(data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix))</span><br></pre></td></tr></table></figure><p>可以对比一下训练前和训练后向量的差异。</p><hr><h1 id="softmax低效率版本">softmax低效率版本</h1><p>2018-07-06更新内容：</p><p>之前写的那个是一个非常toy的网络，本质上就是了解一下word2vec是怎么一回事。不过完全不具备实操的能力。下面找了一些开源的语料，稍微修改了一下之前的脚本，还是基于CBOW的模型，这样就可以正常跑日常的数据。语料地址<a href="https://github.com/lxrogers/CS221SAT/tree/master/data/Holmes_Training_Data" class="uri" target="_blank" rel="noopener">https://github.com/lxrogers/CS221SAT/tree/master/data/Holmes_Training_Data</a>。</p><p>先import一些必要的包，这里的tqdm是显示进度的。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data.dataloader <span class="keyword">as</span> dataloader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure></p><p>然后读入语料数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">text = []</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(<span class="string">'Holmes_Training_Data/'</span>):</span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(<span class="string">'Holmes_Training_Data'</span>, file), <span class="string">'r'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        text.extend(f.read().splitlines())</span><br><span class="line"></span><br><span class="line">text = [x.replace(<span class="string">'*'</span>, <span class="string">''</span>) <span class="keyword">for</span> x <span class="keyword">in</span> text]</span><br><span class="line">text = [re.sub(<span class="string">'[^ \fA-Za-z0-9_]'</span>, <span class="string">''</span>, x) <span class="keyword">for</span> x <span class="keyword">in</span> text]</span><br><span class="line">text = [x <span class="keyword">for</span> x <span class="keyword">in</span> text <span class="keyword">if</span> x != <span class="string">''</span>]</span><br><span class="line">print(text[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>这里我ignore了一些文本读入的错误，然后过滤掉了符号。</p><p>因为语料是英文的，所以这里按照空格分割单词，比中文方便太多。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">raw_text = []</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> text:</span><br><span class="line">    raw_text.extend(x.split(<span class="string">' '</span>))</span><br><span class="line">raw_text = [x <span class="keyword">for</span> x <span class="keyword">in</span> raw_text <span class="keyword">if</span> x != <span class="string">''</span>]</span><br></pre></td></tr></table></figure><p>分好词以后就可以开始构建词库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vocab = set(raw_text)</span><br><span class="line">vocab_size = len(vocab)</span><br></pre></td></tr></table></figure><p>接着跟之前一样，构建一个提供训练数据的函数，并准备好训练数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_context_vector</span><span class="params">(context, word_to_ix)</span>:</span></span><br><span class="line">    idxs = [word_to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> context]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(idxs, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">word_to_ix = &#123;word: i <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(vocab)&#125;</span><br><span class="line">data = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, len(raw_text) - <span class="number">2</span>):</span><br><span class="line">    context = [raw_text[i - <span class="number">2</span>], raw_text[i - <span class="number">1</span>],</span><br><span class="line">               raw_text[i + <span class="number">1</span>], raw_text[i + <span class="number">2</span>]]</span><br><span class="line">    target = raw_text[i]</span><br><span class="line">    data.append((context, target))</span><br><span class="line">print(data[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure><p>定义网络，这里要注意的是，因为数据比较大，我们是分batch喂进来的，因此之前forward的时候，我们把embedding的数据摊开的时候是摊成一行的，这里需要摊成每个batch_size的大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOW</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, context_size)</span>:</span></span><br><span class="line">        super(CBOW, self).__init__()</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.linear1 = nn.Linear(context_size * embedding_dim, <span class="number">128</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">128</span>, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        embeds = self.embeddings(inputs).view(len(inputs), <span class="number">-1</span>)</span><br><span class="line">        out = F.relu(self.linear1(embeds))</span><br><span class="line">        out = self.linear2(out)</span><br><span class="line">        log_probs = F.log_softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span>(log_probs)</span><br></pre></td></tr></table></figure><p>定义各种参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">CONTEXT_SIZE = <span class="number">2</span></span><br><span class="line">batch_size = <span class="number">1024</span></span><br><span class="line">device = torch.device(<span class="string">'cuda:0'</span>)</span><br><span class="line">losses = []</span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">model = CBOW(vocab_size, embedding_dim=<span class="number">100</span>,</span><br><span class="line">             context_size=CONTEXT_SIZE*<span class="number">2</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"><span class="comment"># model = torch.nn.DataParallel(model, device_ids=[0, 1]).cuda()</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>我这里本来写了多卡的跑法，但是不知道是不是我写法有问题还是为什么，每次我跑第二块卡的时候，PyTorch都会去第一块卡开一块空间出来，就算我只是在第二块卡跑也会在第一块卡开一些空间。比较神奇，后面再研究一下。</p><p>然后定义一下data iterator。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_iter = torch.utils.data.DataLoader(data, batch_size=batch_size,</span><br><span class="line">                                        shuffle=<span class="keyword">False</span>, num_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>然后这里要注意的是，shuffle参数会影响每次iter的速度，shuffle会慢很多。另外num_workers越多速度越快，但是很可能会内存爆炸，需要自己调一个合适的。</p><p>然后就可以开始训练了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    total_loss = torch.Tensor([<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> context, target <span class="keyword">in</span> tqdm(data_iter):</span><br><span class="line">        context_ids = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(context[<span class="number">0</span>])):</span><br><span class="line">            context_ids.append(make_context_vector([context[j][i] <span class="keyword">for</span> j <span class="keyword">in</span> range(len(context))], word_to_ix))</span><br><span class="line">        context_ids = torch.stack(context_ids)</span><br><span class="line">        context_ids = context_ids.to(device)</span><br><span class="line"><span class="comment">#         context_ids = torch.autograd.Variable(context_ids.cuda())</span></span><br><span class="line">        model.zero_grad()</span><br><span class="line">        log_probs = model(context_ids)</span><br><span class="line">        label = make_context_vector(target, word_to_ix)</span><br><span class="line">        label = label.to(device)</span><br><span class="line"><span class="comment">#         label = torch.autograd.Variable(label.cuda())</span></span><br><span class="line">        loss = loss_function(log_probs, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    losses.append(total_loss)</span><br><span class="line">    print(<span class="string">'epoch %d loss %.4f'</span> %(epoch, total_loss))</span><br><span class="line">print(losses)</span><br></pre></td></tr></table></figure><p>如果要多卡可以把to(device)的代码改成注释的代码就可以了。</p><p>然后就是需要<strong>注意</strong>的点了。</p><p><strong>这个网络的确是work的，训练完可以试一下发现queen-woman+man和king的cosine similarity的确比monkey或者其他的单词要高。但是这个网络的效率很低！很低！很低！（你觉得我会告诉你一个epoch需要跑一个半小时么）。</strong></p><p>原因在哪呢？其实很简单因为我这里使用的是softmax，也就是说，这个网络每一次训练都需要预测所有的词，比如我这个训练集里面有接近37万个词，那么每次就需要预测37万个类，效率之低可想而知。那么有什么解决方案呢？最早的时候，也就是谷歌C版本的解决方案是基于霍夫曼树的hierarchical softmax。后来DeepMind有一篇介绍把NCE（Noise-contrastive estimation）用来加速的论文<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" data-aria-label="http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf">[1]</span></a></sup>。再后来又出现了negative sampling的论文<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top-right hint--error hint--large" data-aria-label="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">[2]</span></a></sup>。不过直观感受上，NCE和negative sampling是很像的，算是殊途同归吧。</p><p>后面过段时间更新对这两种方法的理解和代码。</p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf<a href="#fnref:2" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Word Embedding现在是现在NLP的入门必备，这里简单实现一个CBOW的W2V。&lt;/p&gt;
&lt;p&gt;2018-07-06更新一发用一篇小说来训练模型的脚本。&lt;/p&gt;
&lt;p&gt;2018-08-02更新一发negative sampling版本。&lt;/p&gt;
    
    </summary>
    
      <category term="PyTorch" scheme="https://samaelchen.github.io/categories/PyTorch/"/>
    
    
  </entry>
  
  <entry>
    <title>台大李宏毅深度学习——神经网络一些骚操作</title>
    <link href="https://samaelchen.github.io/deep_learning_step4/"/>
    <id>https://samaelchen.github.io/deep_learning_step4/</id>
    <published>2018-07-22T16:00:00.000Z</published>
    <updated>2018-10-09T13:26:34.255Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>神经网络自2012年躲过世界末日后开始寒武纪大爆发的既视感，从最原始的FC开始出现了各种很风骚的操作。这里介绍三种特殊的结构：spatial transformer layer，highway network &amp; grid LSTM，还有recursive network。</p><a id="more"></a><h1 id="spatial-transformer">Spatial Transformer</h1><p>CNN是这一次深度学习大爆发的导火线，但是CNN有非常明显的缺陷。如果一个图像里的元素发生了旋转、位移、缩放等等变换，那么CNN的filter就不认识同样的元素了。也就是说，对于CNN而言，如果现在训练数字识别，喂进去的数据全是规整的，那么倾斜的数字可能就不认识了。</p><p>其实从某种意义上来说，这个就是过拟合了，每个filter能做的事情是非常固定的。不过换个角度来看，是不是也能理解为数据过分干净了？</p><p>那么为了解决这样的问题，其实有很多解决方案，比如说增加样本量是最简单粗暴的方法，通过image augment就可以得到海量的训练数据。另外一般CNN里面的pooling层也是解决这个问题的，不过受限于pooling filter的size大小，一般来说很难做到全图级别的transform。另外一种做法就是spatial transformer。实际上，spatial transformer layer我感觉上就是嵌入网络的image augment，或者说是有导向性的image augment。这样的做法可以减少无脑augment带来太多的噪音。个人理解不一定对。这种方法是DeepMind提出的，论文就是<a href="https://arxiv.org/pdf/1506.02025.pdf" target="_blank" rel="noopener">Spatial Transformer Networks</a>。</p><p>首先看一下如果要对图像进行transform的操作，我们应该怎么做？对于一个图像里的像素而言有两个下标<span class="math inline">\((x,y)\)</span>来表示位置，那么我们就可以将这个看作是一个向量。这样以来，我们只需要通过一个二阶方阵就可以操作图像的缩放和旋转，然后加上一个二维向量就可以控制图片的平移。也就是说 <span class="math display">\[\begin{bmatrix}x&#39; \\ y&#39; \end{bmatrix} = \begin{bmatrix}a &amp; b \\ c &amp; d \end{bmatrix} \begin{bmatrix}x \\ y \end{bmatrix} + \begin{bmatrix}e \\ f \end{bmatrix}\]</span></p><p>当然，简洁一点可以写成： <span class="math display">\[\begin{bmatrix}x&#39; \\ y&#39; \end{bmatrix} = \begin{bmatrix}a &amp; b &amp; c \\ d &amp; e &amp; f \end{bmatrix} \begin{bmatrix}x \\ y \\ 1 \end{bmatrix}\]</span> 两个公式的元素没有严格对应，不过意思一样。</p><p>但是这里需要注意的事情是，比如我们原来输入的图片是<span class="math inline">\(3 \times 3\)</span>的，我们输出的还是一个<span class="math inline">\(3 \times 3\)</span>的图片，而位置变换后超出下标的部分我们就直接丢弃掉，而原来有值，现在没有值的部分就填0。示意图如下：</p><p><img src="https://i.imgur.com/CSnxAU6.png"></p><p>然后有一点我一直没理解的点就是，在论文里面上一个等式的左边是source layer，右边的是target layer。直观上从forward的方向上看，数据从上一层到下一层，那么变化就应该是第一层经过变化后变到第二层。</p><p>论文里面没有太解释为什么会是这样的操作，看了一些别人的博客，大部分人也说得不清不楚的。个人的感觉吧，为了这么做是为了保证输出的feature map的维度能够保持不变，论文里面有一个示例图：</p><p align="center"><img src="https://i.imgur.com/XIBMatZ.png" width="70%"></p><p>从图上面看的话，target和source都是保持不变的，唯一变换的是source上的sampling grid（感觉这么说也不太对，sampling grid的像素点数量其实也没变，就是位置或者说形状变了）。而这个sampling grid就是将target的网格坐标通过上面的公式做仿射变换得到的。那如果反过来，也就是说我们直接用source做放射变换的话，很可能得到的target是不规整的。所以应该说spatial transformer layer做的事情是学习我们正常理解的仿射变换的逆矩阵。比较神奇的是这个用bp居然可以自己学出来。</p><p>那么这里就会有个问题，因为sampling grid的像素点其实是没有变过的，所以这就意味着说仿射变换的结果很可能得到是小数的index。比如说<span class="math inline">\(\begin{bmatrix}1.6 \\ 2.4 \end{bmatrix} = \begin{bmatrix}0 &amp; 0.5 \\ 1 &amp; 0 \end{bmatrix} \begin{bmatrix}2 \\ 2 \end{bmatrix} + \begin{bmatrix}0.6 \\ 0.4 \end{bmatrix}\)</span>，那么这个时候要怎么办呢？如果我们按照就近原则的话，那么这个位置又会被定位到原图的<span class="math inline">\([2, 2]\)</span>这个位置，那么梯度就会变成0。所以这样是不行的，那么为了可以进行bp，论文里面采用了双线性插值的方法。也就是说，用离这个位置最近的四个顶点的像素，按照距离的比例作为权重，然后加权平均来填补这个位置的像素。</p><p>这个算法大概原理如下：</p><p align="center"><img src="https://i.imgur.com/b7IprgN.png" width="50%"></p><p>我们现在想要求中间绿色点的像素，那么我们先算出<span class="math inline">\(R_1\)</span>和<span class="math inline">\(R_2\)</span>的像素： <span class="math display">\[R_1 = \frac{x_2 - x}{x_2 - x_1}Q_{11} + \frac{x - x_1}{x_2 - x_1}Q_{21} \\R_2 = \frac{x_2 - x}{x_2 - x_1}Q_{12} + \frac{x - x_1}{x_2 - x_1}Q_{22}\]</span> 然后计算<span class="math inline">\(P\)</span>的像素： <span class="math display">\[\boxed{P = \frac{y_2 - y}{y_2 - y_1}R_1 + \frac{y - y_1}{y_2 - y_1}R_2}\]</span></p><p>那么在DeepMind的试验里面，在卷基层里面加入了ST层之后，收敛以后target得到的输出大体上都是不变的。就像下图：</p><p align="center"><img src="https://i.imgur.com/x0Za3Tx.gif"></p><p>另外就是这个变换矩阵，如果我们强行让这个矩阵长成<span class="math inline">\(\begin{bmatrix}1 &amp; 0 &amp; a \\ 0 &amp; 1 &amp; b \end{bmatrix}\)</span>，那么就会变成attention模式，网络自己会去原图上面扫描，这样就会知道模型在训练的时候关注图片的哪个位置。看起来就像下图：</p><p align="center"><img src="https://i.imgur.com/IDeic8W.png" width="70%"></p><p>上面那一排的网络有两个ST layer，大体上可以看出来，红色的框都是在鸟头的位置，绿色的框都是在鸟身的位置。</p><h1 id="highway-network-grid-lstm">Highway network &amp; Grid LSTM</h1><p>Highway network最早是<a href="https://arxiv.org/pdf/1505.00387.pdf" target="_blank" rel="noopener">Highway Networks</a>和<a href="https://arxiv.org/pdf/1507.06228.pdf" target="_blank" rel="noopener">Training Very Deep Networks</a>这两篇论文提出的。Highway network实际上受到了LSTM的启发，从结构上来看，深层的前馈网络其实和LSTM非常的像，如下图：</p><p align="center"><img src="https://i.imgur.com/L4cqtdk.png" width="80%"></p><p>所以二者的差别就在于，在前馈中只有一个input，而LSTM中每一层都要把这一个时刻的x也作为输入。所以很自然的一个想法，在LSTM中有一个forget gate决定要记住以前多久的信息，那么在前馈网络中也可以引入一个gate来决定有哪些之前的信息干脆就不要了，又或者有哪些以前的信息直接在后面拿来用。那最简单LSTM变种是GRU，所以highway network借鉴了GRU的方法，把reset gate拿掉，再把每个阶段的x拿掉。</p><p>所以将GRU简化一下再竖起来，我们就可以得到highway network：</p><p align="center"><img src="https://i.imgur.com/SsDSDuy.png" width="70%"></p><p>那么模仿GRU的计算方法，我们计算<span class="math inline">\(h&#39; = \sigma(Wa^{t-1})\)</span>，<span class="math inline">\(z = \sigma(W^z a^{t-1})\)</span>，所以<span class="math inline">\(a^t = z \odot a^{t-1} + (1-z) \odot h&#39;\)</span>。</p><p>而后面微软的<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">ResNet</a>其实就是一个highway network的特别版本：</p><p align="center"><img src="https://i.imgur.com/hDTBRrE.png" width="60%"></p><p>当然感觉也可以将ResNet看做是竖起来的LSTM。那ResNet里面的变换可以是很多层的，所以在现在的实现中，很常见的一个情况是将这个东西叫做一个residual block。</p><p>所以利用highway network有一个非常明显的好处就是可以避免前馈网络太深的时候会导致梯度消失的问题。另外有一个好处就是通过highway network可以让网络自己去学习到底哪个layer是有用的。</p><p>那既然可以将深度的记忆传递下去，那么这样的操作也可以用到LSTM里面，也就是grid LSTM。一般的LSTM是通过forget gate将时间方向上的信息传递下去的，但是并没有将layer之间的信息传递下去。因此grid LSTM就是加一个参数纵向传递，从而将layer的信息传递下去，直观上来说，就是在<span class="math inline">\(y\)</span>后面再拼一个vector，然后这个vector的作用跟<span class="math inline">\(c\)</span>一样。具体的可以看一下DeepMind的这篇论文，<a href="https://arxiv.org/pdf/1507.01526v1.pdf" target="_blank" rel="noopener">Grid LSTM</a>。粗略来说，结构上像这样：</p><p align="center"><img src="https://i.imgur.com/BUWr2kn.png" width="60%"></p><p>那有2D的grid LSTM很自然就会有3D的grid LSTM，套路都是差不多的。不过我还没想到的是，3D的grid LSTM要用在什么场景当中，多个output？！</p><h1 id="recursive-structure">Recursive Structure</h1><p>遥想当年刚接触RNN的时候根本分不清recursive network和recurrent network，一个是递归神经网络，一个是循环神经网络，傻傻分不清。但是实际上，recurrent network可以看作是recursive network的特殊结构。Recursive network本身是需要事先定义好结构的，比如：</p><p align="center"><img src="https://i.imgur.com/SgEEBbw.png" width="60%"></p><p>那常见的recurrent network其实也可以看做是这样的一个树结构的recursive network。Recursive network感觉上好像也没什么特别有意思的东西，比较有趣的就是这边<span class="math inline">\(f\)</span>的设计。比如说现在想要让机器学会做句子的情感分析，那么很简单的一个想法就是把每一个词embedding，然后放到网络里面训练，那么我们可以用这样的一个结构：</p><p align="center"><img src="https://i.imgur.com/YFhqVos.png" width="60%"></p><p>因为在自然语言里面会有一些类似否定之否定的语法，所以我们希望说very出现的时候是加强语气，但是not出现的时候就是否定之前的。如果用数学的语言来表达，这不就是乘以一个系数嘛。所以在这样的情况下，如果我们只是简单的加减操作，那么就没有办法实现这种“乘法”操作。所以这个时候，我们的<span class="math inline">\(f\)</span>设计就会有点技巧：</p><p align="center"><img src="https://i.imgur.com/E4Gt3UC.png" width="60%"></p><p>那么看一下这个设计。如果我们直接采用最传统的做法，就是将<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>直接concat起来，然后乘以一个矩阵<span class="math inline">\(W\)</span>，再经过一个激活函数变换，这样的操作其实只能做到线性的关系，个人感觉，实际上这样的设计会将<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>的一些交互特性变成隐藏特征保存在<span class="math inline">\(W\)</span>当中，但是一旦输入变化了，这些隐藏的特征却不能被传递出来，所以效果不好。</p><p>因此下面的一种设计就比较骚气，后面还是传统的做法，但是前面加上了一个vector。这个vector的元素就是来学习这些词之间的interaction，然后将这些interaction变成bias，因为recursive network的function都是不变的，因此这些bias就这样被传递下去了。那么这里有一个需要注意的就是，我们这里有几个词，那我们就需要多少个bias，而且每个bias中间的这个矩阵<span class="math inline">\(W\)</span>都是不一样的。</p><p>这是三个比较骚气的网络结构变换，感觉看了这么多，好多网络之间都是殊途同归啊，会不会最后有一个非常general的网络结构出现，使得现在的每一种网络都是其一种特殊情况呢？</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;神经网络自2012年躲过世界末日后开始寒武纪大爆发的既视感，从最原始的FC开始出现了各种很风骚的操作。这里介绍三种特殊的结构：spatial transformer layer，highway network &amp;amp; grid LSTM，还有recursive network。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://samaelchen.github.io/categories/DeepLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>经济思想史读书笔记——亚当·斯密</title>
    <link href="https://samaelchen.github.io/economics_thoughts_notes4/"/>
    <id>https://samaelchen.github.io/economics_thoughts_notes4/</id>
    <published>2018-07-17T16:00:00.000Z</published>
    <updated>2018-10-09T13:26:34.255Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>“亚当·斯密在政治经济学的历史中占据如此中心的位置，以至于这个谨慎的水手不愿在这样广阔的海洋上开始着手航行。”——亚历山大·格雷</p><a id="more"></a><p>亚当·斯密是经济思想史上一个非常重要的人物，他的著作是经济思想发展中的一道分水岭。</p><h1 id="博学多才的经济学家">博学多才的经济学家</h1><p>斯密的思想涉及到很多领域，包括经济、政治还有哲学。在斯密所处的时代，从事研究的知识精英被要求掌握最宽广的人类知识，而不是仅仅专攻某一个领域的知识。这种多学科方法的一个后果是，像亚当·斯密探求如今称为社会科学的人认为，牛顿在物理学中建立的科学严密性，他们也可以做到。</p><p>斯密常被称为经济学之父，尽管很多先贤看到了经济学很多方面，但是没有人能够将决定国民财富的力量、培育经济增长与发展的适宜的政策以及通过市场力量有效协调大量经济决策的方式，整合成一个全面的观点。</p><p>斯密对经济学范围的认识继承了英国重商主义的观点。他对解释国民财富的性质与原因感兴趣。但是斯密考察的范围比现代经济学家研究的要宽泛，他用政治的、社会的、历史的材料来填充其经济模型。</p><p>虽然斯密在经济学史上的地位非常重要，但是斯密的理论模型缺少优雅与严密。</p><h1 id="斯密的市场分析与政策结论">斯密的市场分析与政策结论</h1><p>斯密在经济思想史的重要性基于（1）他对经济体相互关联性的广泛了解和（2）他对经济政策的影响。斯密作为一位经济学家的强大实力在于，他洞察了：（1）经济体组成部分的相互依赖；（2）用来推动已过财富的政策。他不仅是一个经济学家，而且是一位指出了经济发展与富足方法的哲学家。</p><h2 id="前后关联的经济政策">前后关联的经济政策</h2><p>斯密的方法论塑造了他对经济体的分析，以及他关注政府政策的决心。斯密对自由放任的主张部分依赖于市场如何产生某些结果的理论模型，还来自于他对现有历史与制度环境的观察。所以，斯密的经济政策是前后关联的。斯密对自由市场的拥护并不是因为他认为市场是完美的，而是因为他所处的时代，英国的历史与制度结构导致，市场通常比政府干预产生更好的结果。</p><p>在之前讨论过，经济学科学处理经济变量之间实证的、事务性的关系，即探讨“是什么”。规范经济学涉及的是“应当怎样”的问题。经济学艺术是以政策为导向的。亚当·斯密是一个超群的经济学艺术大师。因而，前后关联的经济政策，就是另一种表达经济学艺术观点的方式。</p><h2 id="自然秩序和谐和自由放任">自然秩序、和谐和自由放任</h2><p>斯密的经济学与重商主义有很多相同的基本因素。他们认为通过科学的调查能够揭示事务性的因果关系。斯密也像重商主义一样，提出人类本性的基本假设：人类是理性的，是有私心的，很大程度上受经济利己主义的驱使。也就是古典经济学里面的一个基本假设——理性人假设。</p><p>但是斯密体系和重商主义有一个很大的不同点在于，他假设竞争性市场在极大程度上是存在的，在这些市场内部，生产要素自由流动，从而提升了它们的经济优势。第二个区别是，经济体的自然运作，能够比人类做出的任何安排更有效地解决冲突。这也是斯密体系里面一个非常重要的假设基础，就是存在完全竞争市场。实际上，斯密的自由放任政策只有在完全竞争市场下才能成立。</p><p>斯密的推论过程非常简单。人类是理性的，是有私心的，受利己主义驱使。如果放任不管，每个个体都会追求自己的私利，在促进私利的同时也促进了社会利益。政府不应当干预这一过程，而应当遵循自由放任的政策。在斯密体系内，私利与公共利益是和谐的。也就是说，在斯密看来，在没有政府干预的竞争性市场上会出现资源的最佳配置。</p><h2 id="竞争性市场的运作">竞争性市场的运作</h2><p>斯密对经济理论最重要的贡献是他对竞争性市场运作的分析。他能比以前的经济学家更详细说明，源于竞争的价格在长期中为什么等于生产成本这一道理。他对价格形成与资源配置的分析中，他将短期价格称为“市场价格”， 将长期价格称为“自然价格”。他认为，竞争从根本上要求有大量的销售者；数值经济体中利润、工资、租金的一群资源拥有者；资源在行业间自由流动。给定这些条件，资源拥有者的私利将会形成长期自然价格，该价格将经济体不同部门之间的利润率、工资、租金均等化。</p><p>确立了竞争性市场的优越性后，斯密毫无困难地构建起他反对垄断与政府干预的论据。但是斯密的自由放任是基于竞争市场存在的假设的。</p><p>斯密认为，尽管重商主义者关于政府干预的很多主张都声称是促进了社会利益，但是其实是增进了个人私利，这种管制不是有利于国家，而是利于商人。但是斯密也并不主张完全的自由放任，他认为在他所处时代历史的、政治的、制度的背景下，他认为，保护幼稚产业的关税是有必要的；国防的贸易管制也是有必要的；政府还应该提供具有极大社会收益但是私人市场因为没有充足利润而不去提供的产品，以此来限定他对自由放任的主张。</p><h2 id="资本与资本家">资本与资本家</h2><p>斯密提出了关于资本在财富生产过程中和在经济发展中的作用的一些重要概念。首先，他指出，一国的现有财富取决于资本积累，原因在于资本积累决定了劳动分工和参加生产性劳动的人口比例。其次，斯密断定资本积累也会导致经济发展。再次，与资本积累相结合的个人私利导致资本在各产业之间的最佳配置。</p><p>在斯密体系中，资本家在经济体运行中扮演主要的角色。资本家对财富与利润的追逐，引导经济体实现资源的有效配置和经济增长。在私有财产经济体中，资本的来源是个人储蓄。斯密认为，劳动并不能够积累资本，原因在于工资水平仅仅能够满足直接的消费欲望。</p><p>斯密断定，恰恰是一部分正在星期的产业接济是对社会有益的人，他们为了利润而奋斗，努力积累资本，通过储蓄和投资来增加他们的财富。因此有利于资本家的收入不平等分配具有巨大的社会重要性。没有收入的不平等分配，就不可能有经济增长，因为全部的年产出都会被消费掉。</p><h2 id="斯密对政策的影响">斯密对政策的影响</h2><p>斯密的重要贡献在于他对市场经济在多种用途之间配置稀缺资源方式的广泛看法。他的主要政策结论是政府应当接受自由放任的政策。</p><h1 id="国民财富的性质与原因">国民财富的性质与原因</h1><p>斯密不认同重商主义的一国的贵金属就等同于一国财富的观点。他认为，财富是产品与服务的年流量，而不是贵金属的累计储备量。他也解释了出口与进口之间的关系，认识到出口的基本作用是支付进口。此外他暗示，经济活动的最终目的是消费。这将斯密的经济学与重商主义的经济学加以区分，后者将生产视为目的。而在国家财富源泉的认识上，斯密与重农主义也不一样，后者强调的是消费。</p><p>斯密继而建议，国家的财富应当按照人均指标来衡量，这就是现在经常被提及的人均GDP。</p><h2 id="国民财富的原因">国民财富的原因</h2><p>斯密主张，一个国家的财富，也就是国家的收入取决于：（1）劳动生产力；（2）有效使用或者生产性使用的劳动者的比例。因为在他的完全竞争市场下，经济体将自动实现资源的充分利用，因此他只需要考察那些决定一国产品与服务生产能力的因素。</p><h3 id="劳动生产力">劳动生产力</h3><p>斯密认为，劳动生产力取决于劳动分工。专业化与劳动分工提高了劳动生产力。尽管斯密认为专业化与劳动分工的经济利益，但是他也察觉到一些严重的社会成本。劳动分工的一个社会缺点是公认被赋予重复性的任务，这些任务很快变得单调乏味。这一点其实与管理学里面的科学管理法非常相似，另外可以看一下霍桑试验。但是我们不得不承认，劳动分工增加了人类福利。</p><p>劳动分工一次取决于斯密所谓的市场的范围与资本的积累。市场越大，可销售的数量越多，劳动分工的机会就越多。另一方面，有限的市场仅允许有限的劳动分工。劳动分工收到资本积累的限制，原因在于生产过程是耗时的：在生产开始与成品的最终销售之间存在时间间隔。</p><p>在一个简单的经济体中，劳动分工是微小的，只需要很少的资本来维持劳动力。但是随着劳动分工的增加，劳动者不再为其自身的消费生产产品，在耗时的生产过程期间，必须保持一定的消费品储备来维持劳动者。这一定数量的产品来自储蓄，也就是斯密所说的资本。资本家的一个主要功能是缩短生产开始与最终产品销售之间的时间间隔而提供手段。因此可以使用劳动分工的生产过程的范围，收到可以利用的资本积累数量的限制。换而言之，斯密认为，劳动分工随着越来越多的储蓄而越来越细分。如果将这一个观点放到小一点的经济体上，比如公司，我们就会很直观理解，创业公司，一个人当好几个人用，当公司庞大到商业帝国的程度，几个人当一个人用。</p><h3 id="生产性与非生产性劳动">生产性与非生产性劳动</h3><p>按照斯密的观点，资本积累也决定了生产性：劳动者与非生产性劳动者的比例。他主张，生产可销售商品所使用的劳动是生产性劳动，而生产服务所使用的劳动则是非生产性劳动。斯密的观点其实非常的朴素，资本应当用于再生产。他的观点，如果一个做法对个体是正确的，那么对国家也是正确的，因此，国家的资本越多，就更应该用于支持生产性劳动。在斯密的时代，他并没有意识到第三产业的作用。</p><p>斯密强调，将大量收入分配给进行储蓄和投资的资本家，将少量收入分配给地主，可以获得最高的经济增长率。某种程度上，是不是也可以看作是需要大力扶持实业。此外，因为经济增长收到政府非生产性劳动支出的约束，例如军队，所以拥有较小的政府，就可以对资本家征收较少的税，以便他们可以积累更多的资本。这里有一点点小国寡民的意味。</p><h2 id="对国民财富的总结">对国民财富的总结</h2><p>其实在斯密的观点中，这一点一句话就可以概括，资本是国家财富的主要决定因素。</p><p>对斯密而言，资本积累毫无疑问要求一个自由市场与私人财产的制度框架。在自由市场体制中，既定的投资支出水平，在没有政府指导的运转中被加以分配，以确保最高的经济增长率。在私人财产体制中，对高资本积累率的进一步要求就是不平等的收入分配。</p><h1 id="国际贸易">国际贸易</h1><p>在斯密的观点中，只有错误地认为一个国家的财富取决于它所持有的贵金属和借据，贸易顺差才是有利的。而斯密的主张是非规制的对外贸易，理由是如果英国能够以低于法国的成本生产一种产品，例如羊毛，并且法国可以以低于英国的成本生产另一种产品，例如葡萄酒，那么两国各自用生产成本较低的产品，去交换生产成本较高的产品，这种交换对双方来说都是有利的。也就是现在经济学中的对外贸易的绝对成本学说。实际上这种学说不局限于国际贸易，还适用于一国的内部贸易。</p><p>在现代经济学的观点中，随着劳动越来越专业化，存在收益递增（成本递减）的情况。斯密的对外贸易优势的部分观点，明显基于收益递增这一动态概念。但是这种观点是有非常明显的缺陷的，事实上，任何一国国家都不可能长期保持一种生产方式不变。</p><p>在这个方面，古典经济学和重商主义者有一个重大的区别。重商主义者认为国际贸易是一种零和博弈，而古典经济学认为不是。但是斯密只是认识到贸易对各国都有利，而没有认识到贸易过程中的价格机制。这一点在之后的李嘉图等人的理论中得到解释。</p><h1 id="价值理论">价值理论</h1><p>区分价值与价格困惑了早期经济学家。这主要集中于三个问题：（1）什么决定了产品的价格？也就是什么决定了相对价格？（2）什么决定了价格总水平？（3）什么是福利的最佳度量标准。实际上，斯密也没有非常明确给出答案。</p><h2 id="相对价格">相对价格</h2><p>按照现代的经济学术语，相对价格指的是商品间的价格比例关系。实际上相对价格是由李嘉图提出来的。另外一个概念是绝对价格，绝对价格其实就是用货币单位表示的价格水平。</p><p>斯密认为，市场价格或者短期价格是由供需双方决定的。自然价格或者长期均衡价格通常取决与生产成本。在现代经济学里面，我们认为自然价格是达到供需平衡时候的价格。</p><p>斯密对他所处时代经济体中相对价格形成的分析，区分为两个时间段和经济体的两个宽泛的部门，分别是短期与长期、农业与制造业。在短期或者市场阶段，斯密在制造业与农业中都发现了乡下倾斜的需求曲线与向上倾斜的供给曲线；因此市场价格取决于需求与供给。斯密对长期中发生的更为复杂的“自然价格”的分析，包含着一些矛盾。对于农业部门而言，自然价格取决于供给与需求，原因在于长期供给曲线向上倾斜，表明成本递增。但是对制造业部门来说，长期供给曲线有时假定为完全富有弹性（水平的），表明成本不变；在分析的另一些地方又向下倾斜，表明成本递减。在制造业中，当长期供给曲线完全富有弹性时，价格就完全取决于生产成本；但是，当长期供给曲线向下倾斜时，自然价格就取决于需求与供给双方。不过，无视长期供给曲线在制造业中的形状，主要强调生产成本对自然价格的决定，这是斯密以及后来的古典经济学家的特点。</p><p>经院哲学对相对价格问题感兴趣，因为他们关注交换过程中的道德问题；重商主义者则是认为财富在交换过程中产生。斯密认为，一旦经济体实行专业化和劳动分工，交换就变成必需。如果交换发生在斯密时代的市场中，就会出现一些显而易见的问题。第一，如果交换处于高于物物交换水平的状况下，就会存在交换媒介的问题。第二，存在价值或者相对价格的问题。</p><h2 id="价值的含义">价值的含义</h2><p>斯密认为价值一词有两种不同的含义，它有时表示一些特定物品的效用，有时又表示因占有物品而取得的对其他产品的购买力。前者可以称为“使用价值”，后者被称为“交换价值”。使用价值很大的东西，其交换价值往往很小，甚至没有；相反，交换价值很大的东西，其使用价值往往极小。就像水的使用价值非常高，但是交换价值很低；钻石的交换价值很高，但是使用价值非常低。</p><p>这里的交换价值是指一种商品购买其他产品的能力。这是市场所表达的一种客观度量。他关于使用价值的概念是含糊的。一方面，使用价值有道德内涵，这是对经院哲学的回归。另一方面，使用价值是一件商品满足需要的能力，是因持有或消费一件产品而获得的效用。当一件商品被消费时，可以获得几种效用：（1）它的总效用，（2）它的平均效用，（3）它的边际效用。斯密的关注点是总效用，这就模糊了他对需求如何在价格决定中发挥作用的理解。显然，水的总效用超过了钻石的总效用。然而，因为商品的边际效用经常随着其消费得更多而递减，所以水的边际效用比钻石的边际效用低。</p><p>我们愿意为一件商品所支付的价格——我们对获得又一单位商品所寄予的价值——不仅取决于商品的总效用，而且取决于其边际效用。因为斯密没有意识到这一点，因此他既不能为“钻石-水悖论”找到满意的解决办法，也不能了解其使用价值与交换价值之间的关系。某种程度上是不是认为这是供给量上的区别？大多数情况下水比钻石多，所以边际效用就低。</p><h2 id="斯密关于相对价格">斯密关于相对价格</h2><p>因为斯密对相对价格的决定因素有些困惑，所以，他发展了与这些因素相关的三个独立的理论：（1）劳动成本价值理论，（2）劳动支配价值理论，（3）生产成本价值理论。</p><p>他假设了经济体两种截然不同的状态：初期野蛮状态或者原始社会，它被界定为这样的一个经济体，其中资本还没有被积累起来，土地未被使用；发达经济体，其中资本与土地不再是资源充足的产品（它们具有超过零以上的价格）。</p><h2 id="原始社会中的劳动成本理论">原始社会中的劳动成本理论</h2><p>如果在一个狩猎国家，杀死一头海狸所需的劳动通常两倍于杀死一头野鹿所需要的劳动，那么，一头海狸自然就可以换来或者值两头野鹿。按斯密的劳动成本理论，在还不存在土地与资本的经济体中，或者土地与资本还是无限充足的自然资源的经济体中，一件产品的交换价值或者价格，由生产产品所需的劳动量决定。</p><p>这让我们认识到劳动成本价值理论的第一个难点。我们应当如何度量生产一件商品所需要的劳动量。斯密认识到生产一件产品所需的劳动量不能简单地用时钟表示的时间数量来度量，原因在于，除了时间之外，也必须考虑有关的精巧或者技能，以及任务的艰难与困苦。</p><p>在这一点上，斯密遇到了所有的劳动成本价值理论都遇到的，扔未被后来的经济学家成功地予以解决的一个难题。如果劳动量是一个以上变量的函数，那么，我们必须找到一种方法来说明所有变量的相对重要性。斯密主张时间、艰难程度以及精巧程度上的差异都反映在支付给劳动者的工资中。</p><p>但是斯密的观点只是重申了问题，他是在表明一件产品是依照支付给劳动者的工资，而不是依照包含在产品中的劳动量拥有价值。这是一个循环推论。斯密利用一套价格解释另一套价格。</p><h3 id="原始社会中的劳动支配">原始社会中的劳动支配</h3><p>按照斯密的观点，一件产品的价值“对于那些拥有产品的人以及那些想用它去交换一些新产品的人来说，正好等于产品能够使他们购买或支配的劳动量”。也就是说，如果捕获一头海狸或者两头野鹿需要两小时，那么两头野鹿就等于一头海狸。</p><h3 id="发达经济体中的劳动理论">发达经济体中的劳动理论</h3><p>因为资本已经被积累，土地也已经被利用，并且一件产品的最终价格也必须包括当做利润的资本家的收益以及当做地租的地主的收益。最终价格形成了由工资、利润、地租这些要素报酬构成的收入。</p><h3 id="相对价格的生产成本理论">相对价格的生产成本理论</h3><p>斯密最终放弃了任何劳动价值理论都适用于他所处时代一样发达的经济体。斯密似乎发现，一旦资本被积累起来，土地被加以利用，并且一旦必须支付利润、地租，还有劳动，能唯一适当解释价格的就是生产成本理论。</p><p>在成本理论中，一件商品的价值取决于对所有生产要素的支付：除了劳动之外还有土地和资本。在斯密的体系中，利润这一术语既包括今天的利润，也包括利息。在斯密假设平均成本不随着产量的增加而增加的地方，无论使用总成本还是使用平均成本，通过加总工资、利润和地租，这样的相对价格都是不变的。在平均成本随着产量变化的地方，价格就取决于需求与供给双方。然而，在分析长期自然价格的决定时，即使当供给曲线不被假定为完全富有弹性时，斯密也强调供给与生产成本。斯密主张，竞争占优势的地方，商人、劳动者、地主的私利将导致与生产成本相等的自然价格。</p><h1 id="分配理论">分配理论</h1><p>收入的个人分配取决于个人所出售的生产要素的价格与数量。劳动是大部分家庭拥有的唯一生产要素，因此家庭的收入一般取决于工资率与工作时间的长度。拥有财产的那些家庭所获得的财产收入量，取决于家庭所拥有的资本与土地的数量以及这些要素的价格。因为工资、利润、地租都是经济体中的价格，所以它们的相对价格——连同个人出售的劳动、资本、土地数量一起——决定了收入的分配。</p><h2 id="工资">工资</h2><p>斯密认为，在对工资的讨价还价过程中，劳动处于劣势。因为劳动市场是买方市场，雇主少，可以容易联合起来巩固他们的地位。即使罢工，雇主也有足够资源维持他们的生活，但是没有工作，工人生存困难。在这一部分，斯密削弱了市场力量的有益运作过程，并似乎已经意识到其完全竞争市场的假设受到了限制。</p><h2 id="工资基金">工资基金</h2><p>因为生产过程是耗时的，所以从生产过程开始到最终销售，需要一部分的产品库存或资本来维持劳动者的衣食住行，这部分被称为工资基金，来源于资本家的储蓄或者消费中断。给定劳动力和工资基金的规模，工资率=工资基金/劳动力。</p><h2 id="利润">利润</h2><p>斯密很自然接受了利润是因资本家执行了对社会有用的功能而对他的一种支付，这种功能就是在耗时的生产过程期间，为劳动者提供生活必需品，提供工作所用的原料和机器。按照斯密的观点，劳动者允许从其产量中进行利润的扣除，原因在于，劳动者并不拥有工作所用的原料和独立的支持手段。于是利润分为两部分：纯利息收入和风险收入。在原始经济体中，劳动者获得了全部的产品，但是在发达经济体中，劳动者却需要被扣除利润和地租，这一点斯密并没有做出解释。在深信资本主义制度基本和谐的理论家里，这一点是非常自然无需质疑的。</p><h2 id="地租">地租</h2><p>斯密提出了四种地租理论：（1）地主的需求；（2）垄断；（3）差异化的优势；（4）大自然的施舍。在《国富论》前面的部分，地租被视为决定价格的因素，而后面则视为价格被决定的因素。</p><h2 id="随时间变化的利润率">随时间变化的利润率</h2><p>斯密认为一个国家的经济增长取决于资本积累。他预测，随着时间的推移，利润率会下降，原因有三。（1）劳动市场的竞争。资本家的竞争导致工资上升利润下降。（2）商品市场的竞争。生产者竞争家具，商品价格下降，利润减少。（3）投资市场的竞争，因为投资机会有限，所以资本的积累增加会导致利润下降。</p><h1 id="福利与价格总水平">福利与价格总水平</h1><p>斯密努力去发现：第一，决定价格总水平的因素；第二，不同时期福利变化的最佳度量。其实这个问题还是比较复杂的，对于一个生产两种或者更多产品的经济体来说，有没有可能界定和度量其福利的变化。</p><p>如果用总消费或者产量来界定福利，那么需要解决的问题就是，寻找一种度量总消费或者总产品数量的方式。通常而言，这种度量方式就是国家的货币单位。现在的社会，我们通常会用GDP来衡量。</p><p>但是这里会有另外一个问题，那就是货币的本质是一般等价物。这就意味着，货币也是有价格的，它本身也会变化。所以斯密转向劳动，却发现劳动价格也会变化。最后，他采用劳动的复效用作为衡量标准。也就是说，如果我们能够使用较少的劳动生产相同的产量，那么我们就拥有更多的线下，经济状况就会更好。</p><p>事实上这种度量方式比斯密想象的要复杂的多，现在的经济学家在这一方面有了更长远的度量方式，例如度量“生活质量”。</p><p>斯密在经济学史上如此之重要，他对自由放任体系的推崇引导了西方市场经济数百年的发展。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;“亚当·斯密在政治经济学的历史中占据如此中心的位置，以至于这个谨慎的水手不愿在这样广阔的海洋上开始着手航行。”——亚历山大·格雷&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="https://samaelchen.github.io/categories/ReadingNotes/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数 13</title>
    <link href="https://samaelchen.github.io/linear_algebra_step13/"/>
    <id>https://samaelchen.github.io/linear_algebra_step13/</id>
    <published>2018-07-16T16:00:00.000Z</published>
    <updated>2018-10-09T13:26:34.255Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>两个线性代数里面在机器学习领域大放异彩的算法，SVD和PageRank。</p><a id="more"></a><p>之前对方阵有对角化分解，而且不是所有的方阵都可以对角化，但是SVD是所有矩阵都可以进行分解的。SVD分解的过程如下：</p><p><img src="https://i.imgur.com/ZMo1wcb.png"></p><p>这个公式会有什么特性呢？我们假设<span class="math inline">\(U = \{u_1, u_2, \cdots, u_m\}\)</span>，<span class="math inline">\(V = \{v_1, v_2, \cdots, v_n \}\)</span>，<span class="math inline">\(\Sigma\)</span>是常数<span class="math inline">\(\{\sigma_1, \sigma_2, \cdots, \sigma_k\}\)</span>的对角矩阵。这里有一个点要注意的就是<span class="math inline">\(\Sigma\)</span>的样子大体上会是左上角一个对角矩阵，其余部分都是零的<span class="math inline">\(m \times n\)</span>的矩阵。这些<span class="math inline">\(\sigma\)</span>称为奇异值，这些奇异值会等于<span class="math inline">\(A^{\top}A\)</span>的特征根的平方根。</p><p>那么如果矩阵经过SVD分解以后，一定会得到<span class="math inline">\(Av_i = \begin{cases}\sigma_i u_i &amp; \text{if } 1 \le i \le k \\ 0 &amp; \text{if } i &gt; k \end{cases}\)</span>，<span class="math inline">\(A^{\top}u_i = \begin{cases}\sigma_i v_i &amp; \text{if } 1 \le i \le k \\ 0 &amp; \text{if } i &gt; k \end{cases}\)</span>。</p><p>现在问题来了，如果给一个矩阵，要怎么计算奇异值？假设有一个矩阵<span class="math inline">\(A = \begin{bmatrix} 0 &amp; 1 &amp; 2 \\ 1 &amp; 0 &amp; 1 \end{bmatrix}\)</span>，可以直观看到，这个矩阵是<span class="math inline">\(3 \times 2\)</span>的矩阵，因此需要在<span class="math inline">\(\mathbb{R}^3\)</span>和<span class="math inline">\(\mathbb{R}^2\)</span>都要有orthogonal matrix。所以先构建矩阵<span class="math inline">\(A^{\top}A = \begin{bmatrix}1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 2 \\ 1 &amp; 2 &amp; 5 \end{bmatrix}\)</span>。那么做一个<span class="math inline">\(\mathbb{R}^3\)</span>上面的orthogonal matrix，按照这个<a href="https://samaelchen.github.io/linear_algebra_step11/">博客</a>最后的正交化方法，我们可以将矩阵正交化为<span class="math inline">\(v_1 = \frac{1}{\sqrt{30}} \begin{bmatrix} 1 \\ 2 \\ 5 \end{bmatrix}\)</span>，<span class="math inline">\(v_2 = \frac{1}{\sqrt{5}} \begin{bmatrix} 2 \\ -1 \\ 0 \end{bmatrix}\)</span>，<span class="math inline">\(v_3 = \frac{1}{\sqrt{6}} \begin{bmatrix} 1 \\ 2 \\ -1 \end{bmatrix}\)</span>。然后求一下<span class="math inline">\(A^{\top}A\)</span>的特征根分别是6，1和0。所以奇异值就是<span class="math inline">\(\sqrt{6}\)</span>和1。然后就可以按照上面的公式算出<span class="math inline">\(u_1 = \frac{1}{\sqrt{5}} \begin{bmatrix} 2 \\ 1 \end{bmatrix}\)</span>，<span class="math inline">\(u_1 = \frac{1}{\sqrt{5}} \begin{bmatrix} -1 \\ 2 \end{bmatrix}\)</span>。</p><p>那么事实上算到这里，只要将上面的向量集合和奇异值排列好，就完成了矩阵<span class="math inline">\(A\)</span>的SVD分解。也就是<span class="math display">\[A = \begin{bmatrix} \frac{2}{\sqrt{5}} &amp; \frac{-1}{\sqrt{5}} \\ \frac{1}{\sqrt{5}} &amp; \frac{2}{\sqrt{5}} \end{bmatrix} \begin{bmatrix} \sqrt{6} &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \begin{bmatrix} \frac{1}{\sqrt{30}} &amp; \frac{2}{\sqrt{5}} &amp; \frac{1}{\sqrt{6}} \\ \frac{2}{\sqrt{30}} &amp; \frac{-1}{\sqrt{5}} &amp; \frac{2}{\sqrt{6}} \\ \frac{5}{\sqrt{30}} &amp; 0 &amp; \frac{-1}{\sqrt{6}} \end{bmatrix}^{\top}\]</span></p><p>那么SVD跟PCA是有非常多相似的地方的，如果我们使用了全部的奇异值，那么我们就可以还原原来的矩阵，但是如果我们只取了前面的一部分奇异值，我们得到的就是一个损失了一部分信息的矩阵。SVD在机器学习的领域有非常多的应用，最常用的一个地方就是用在推荐算法里面，另外就是降维。此外，还有一个矩阵分解方法是NMF，解释性会更强一些。这个在之前机器学习的博客里面也有提到。</p><p>然后是PageRank。这个算法缔造了今天的谷歌，也被称作是最贵的eigen value。PageRank实际上是一个蛮复杂的模型，这里讲一个最简单的情况，后面找机会再认真学习一下。所以这里有一些矩阵分析里面的定理（虽然我也不是很懂）就直接记结论，证明过程以后再学吧。</p><p>首先我们假设这个世界上只有四个网页，他们的关系如下：</p><p><img src="https://i.imgur.com/kpsOeB5.png"></p><p>现在假设有一个人随机浏览网页，他到每一个网站的可能性都是一样的，那么根据上图的结果我们可以得到： <span class="math display">\[\begin{align}x_1 &amp; = x_3 + \frac{1}{2} x_4 \\x_2 &amp; = \frac{1}{3} x_1 \\x_3 &amp; = \frac{1}{3} x_1 + \frac{1}{2} x_2 + \frac{1}{2} x_4 \\x_4 &amp; = \frac{1}{3} x_1 + \frac{1}{2} x_2\end{align}\]</span></p><p>所以我们就可以很简单得到这样一个矩阵<span class="math inline">\(A = \begin{bmatrix}0 &amp; 0 &amp; 1 &amp; \frac{1}{2} \\ \frac{1}{3} &amp; 0 &amp; 0 &amp; 0 \\ \frac{1}{3} &amp; \frac{1}{2} &amp; 0 &amp; \frac{1}{2} \\ \frac{1}{3} &amp; \frac{1}{2} &amp; 0 &amp; 0\end{bmatrix}\)</span>。这样的矩阵我们叫做马尔科夫矩阵，或者叫转移矩阵。这种矩阵的特点是每一个行的和为1，或者每一个列的和为1。根据<a href="https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem" target="_blank" rel="noopener">Perron-Frobenius theorem</a>这样的矩阵必然有一个特征值为1。</p><p>所以PageRank实际上在做的事情就是计算<span class="math inline">\(A\)</span>的特征根为1时候的特征向量。这个特征向量最后就是我们的网页排名。</p><p>如果想要对PageRank有多一点的了解可以上Wikipedia看一下<a href="https://en.wikipedia.org/wiki/PageRank" target="_blank" rel="noopener">PageRank的页面</a>，也可以直接看原来的<a href="http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf" target="_blank" rel="noopener">论文</a>。另外有中文的这篇博客<a href="http://blog.codinglabs.org/articles/intro-to-pagerank.html" class="uri" target="_blank" rel="noopener">http://blog.codinglabs.org/articles/intro-to-pagerank.html</a>，介绍比较全面，不过基本上没有数学证明过程。看看以后有没有空自己推导一遍，顺便Python实现一下。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;两个线性代数里面在机器学习领域大放异彩的算法，SVD和PageRank。&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数 12</title>
    <link href="https://samaelchen.github.io/linear_algebra_step12/"/>
    <id>https://samaelchen.github.io/linear_algebra_step12/</id>
    <published>2018-07-15T16:00:00.000Z</published>
    <updated>2018-10-09T13:26:34.255Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>稍微介绍一下两种特殊的矩阵，orthogonal matrix和symmetric matrix。</p><a id="more"></a><p>orthogonal matrix其实就是矩阵里面每个向量相互独立的矩阵，如果是orthonormal的话，这些矩阵里的向量都是单位向量。比如说<span class="math inline">\(\begin{bmatrix}\frac{1}{\sqrt{2}} &amp; 0 &amp; \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} &amp; 0 &amp; \frac{-1}{\sqrt{2}} \\ 0 &amp; 1 &amp; 0 \end{bmatrix}\)</span>。</p><p>这样的矩阵有一些特性，首先，orthogonal matrix <span class="math inline">\(Q\)</span>的transpose和inverse相等。也就是<span class="math inline">\(Q^{\top} = Q^{-1}，且这两个矩阵都是orthogonal的\)</span>，另外，<span class="math inline">\(\det(Q) = \pm 1\)</span>。最后，orthogonal matrix和orthogonal matrix叉乘之后还是orthogonal matrix。</p><p>orthogonal matrix还有一个很特殊的特性，就是向量和orthogonal matrix相乘以后，向量的norm不变。</p><p>另一种特殊矩阵是symmetric matrix，也就是类似<span class="math inline">\(\begin{bmatrix}a &amp; b \\ b &amp; c \end{bmatrix}\)</span>。首先，symmetric matrix一定有实特征根。其次，symmetric matrix一定有orthogonal eigenvectors。最后，symmetric matrix一定是diagonalizable的。这里存在一个等价关系<span class="math inline">\(A \text{ is symmetric等价于} P^{\top}AP = D 或 A = PDP^{\top}\)</span>。而这的<span class="math inline">\(P\)</span>包含<span class="math inline">\(A\)</span>的特征向量，<span class="math inline">\(D\)</span>是<span class="math inline">\(A\)</span>的特征根组成的对角矩阵。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;稍微介绍一下两种特殊的矩阵，orthogonal matrix和symmetric matrix。&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>经济思想史读书笔记——重商主义、重农主义及其他先驱</title>
    <link href="https://samaelchen.github.io/economics_thoughts_notes3/"/>
    <id>https://samaelchen.github.io/economics_thoughts_notes3/</id>
    <published>2018-07-11T16:00:00.000Z</published>
    <updated>2018-10-09T13:26:34.255Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>值得注意的是，发明家们并不具备被称为“科学态度”的客观性。——威廉·莱特温</p><a id="more"></a><p>1600年至1750年的150年间，经济活动极大增加，文艺复兴完成了资本主义的萌芽，教会的权威极大被削弱，这些为后来的工业革命的爆发奠定了基础。</p><p>这一时期，经济思想从简单的个人、家庭、生产者的观点，向更复杂的将经济体视为有其自身规律与相互关系的一个系统的观点演进。</p><h1 id="重商主义">重商主义</h1><p>重商主义的主要贡献出自英国人和法国人之手。经院哲学主要来自中世纪牧师，而重商主义的经济理论主要来自商人。重商主义的理论更像是个人的智力反应，而不是一个成体系的理论框架。</p><p>重商主义时期，由于领地减少，名族国家增多，重商主义理论试图确定能够推动国家权利与财富增加的最佳政策。</p><p>重商主义者的理论是基于世界总财富不变的假设下入手的。经院哲学在这个假设下论证了个人的财富获取，必定伴随另一个人的失去。重商主义将这个观点推广到了国家之间。因此，在他们看来，一个国家的财富是依靠很多国家的贫困来支撑的。重商主义者强调，国际贸易是增加一国财富和权力的一种手段，并且特别集中研究国家之间的贸易平衡问题。</p><p>大多数重商主义者认为，经济活动的目的是生产（古典经济学认为是消费）。因此，他们为了在国际贸易中保持优势，提倡低工资、低消费。乍一听，这不就是拥有劳动力优势的发展中国家的情况么。在重商主义的理论下，国民的贫困将使国家受益。所以在重商主义的思想下，为了实现贸易顺差，一国应该通过关税、配额、津贴和税收等手段来鼓励出口、限制进口；应当通过政府干预国内经济，以及通过对外贸易规则来刺激生产；应当对从国外进口的制造业产品征收保护性关税；应当鼓励进口用于制造出口产品的廉价原材料。诶，是不是很像某国现在在搞的贸易战的样子。</p><p>重商主义的货币观点是，一国的财富等同于国内持有的贵金属存量。他们深信，货币因素而非实际因素是经济活动与经济增长的首要决定因素。后面看到古典经济学的时候再做一个具体的比较。</p><p>从现在的角度来看，重商主义者被利益驱动，利用政府为他们自身获取经济特权。他们通常是商人，支持政府允许垄断，使得商人垄断者可以索要比没有垄断时更高的价格。</p><p>重商主义者最重要的成就或许是认可了对经济体进行分析的可能性。他们意识到了经济体中非常机械的因果关系，并且相信如果一个人弄懂了这些关系的规则就能控制经济体。他们认为政府干预可以达到既定的目的，但是不能随便干预使得一些基本的经济原理变得复杂。后期的重商主义意识到早期重商主义的很多理论不足，例如硬币不能代表一国的财富；对于所有国家不可能存在一种贸易顺差等。后期重商主义出现了早期古典自由主义的观点。但是古典主义和重商主义还是存在一个重要的差别，那就是重商主义认为私人利益和公关福利之间存在根本的冲突，而古典主义经济学家认为公共利益是个人追求自我利益自然而然的结果。</p><h1 id="重农主义">重农主义</h1><p>重农主义主要在18世纪的法国兴起。重农主义的著作与重商主义不同的是，重农主义有显著的一致性。重农主义的发展是短时间在法国集中出现的，而且有共同的知识领袖——弗朗索瓦·魁奈。</p><p>重农主义认为存在自然法则支配着经济体的运作，这些法则独立于人类意志，而人类可以客观发现它们。他们觉得，有必要通过分离主要经济变量来构建理论模型。重农主义并不集中研究货币，而是重点研究导致经济发展的实际力量上。相对于重商主义认为的财富源于交换过程的观点，重农主义推断财富起源于农业或者自然。</p><p>因为重农主义发展的时代，生产的产品用于支付实际生产成本之后，产生了剩余。对这种剩余的探索让他们形成了净产品的概念。根据他们的观点，劳动只能生产出支付劳动成本的产品，只有土地例外。因此土地的生产产生了净产品，而其他的非农业活动不能产生净产品。所以重农主义者集中注意力于物质生产力而不是价值生产力。</p><p>重农主义的理论巅峰是魁奈的《经济表》。事实上，这个表格的价值流动，看上去很像“投入——产出表”。经济表证明了经济体不同部门之间相互依赖的存在。</p><p>在经济政策上，重农主义者认为存在一种比人类设计的秩序更优的自然秩序，所以政府的任务是实行自由放任的政策。他们推断自由竞争将会导致最优价格；如果每个个体都追求自己的私利，那么社会将从中获利。</p><p>事实上，重商主义者发现净产品的源泉是交换，尤其是国家贸易形式的交换，因此他们提倡贸易顺差。而重农主义者则认为净产品源自农业，因此主张放任自由会引发农业生产的增加，最终引起更大的经济增长。</p><h1 id="其他思想先驱">其他思想先驱</h1><h2 id="托马斯孟">托马斯·孟</h2><p>托马斯·孟是一个主要的重商主义者，但是他的观点跟原始的重商主义不一样。作为一个英国人，他指出，尽管与所有其他国家实现了贸易顺差是合意的，贵金属流出到其他国家是不合意的。但是在与印度的贸易逆差以及出口贵金属到印度却有利于英国，因为这些实践扩大了英国与所有国家的贸易平衡，增加了金银的流入。虽然托马斯·孟是一个重商主义者，但是他以及看到了早期重商主义范例的严重错误。</p><h2 id="威廉配第">威廉·配第</h2><p>配第是第一个提倡测量经济变量的经济学家。配第最早明确提倡用我们所谓的统计方法来度量社会现象。他设法度量一国的人口、国民收入、出口、进口、资本量。尽管配第对统计学的早期应用显得有些原始，但是，他所代表的方法论立场却具有一种世系，这种世系始于他所处时代的经验归纳，止于当代经济学期刊上盛行的计量经济学线代应用。</p><h2 id="伯纳德曼德维尔">伯纳德·曼德维尔</h2><p>曼德维尔发现世界是邪恶的，但主张“在一个熟练政治家的灵巧管理下，私人恶习有可能变成公众利益”。重商主义的信仰具体表现为对产品的恐惧，对生产过剩与消费不足的关注。个人储蓄并不受欢迎，因为它会引发更低的消费、更低的产量，以及更低的就业。曼德维尔是一个纯粹的重商主义者，他坚决主张政府管制对外贸易，从而保证出口总是超过进口。因为社会的目标是生产，所以曼德维尔甚至主张大量拥有人口和童工，并谴责懒惰。他注意到了一条向下倾斜的劳动力供给曲线。根据曼德维尔的观点，较高的工资将减少劳动供给。他的主要观点就是应当接受满身恶习的人类，并通过规则和制度将其引导到社会利益上来。</p><h2 id="大卫休谟">大卫·休谟</h2><p>休谟被称作自由的重商主义者。他认为一个经济体的经济活动水平取决于货币数量及其周转速度，并对一国的贸易平衡、货币数量以及价格总水平之间的关系做出了相当完整的描述。<strong>黄金流动价格机制</strong>被认为是休谟在国际贸易理论中的重要贡献。</p><p>休谟不是一个纯粹的重商主义者在于，他指出，一个经济体不可能持续保持贸易顺差。贸易顺差将导致经济体内金银的增加。货币增加将使得具有贸易顺差的经济体价格上升。那么具有贸易逆差的经济体就会货币减少，价格下降。那么在这样的情况下，原来具有贸易顺差的经济体出口就会减少，进口就会增加。逆差的经济体则相反。这一过程最终将会导致贸易平衡的自动调整。</p><p>休谟认为，尽管一国的货币绝对量不能影响实际产量，但是货币供给的逐渐增加将会引起产量的增加。这一点，休谟并没有跳出重商主义的框架。</p><p>最后休谟主张经济自由，他认为经济自由和政治自由的增长是结合在一起的。</p><h2 id="理查德坎蒂隆">理查德·坎蒂隆</h2><p>理查德·坎蒂隆是部分重商主义，部分重农主义，以及部分的重农主义-古典学派。坎蒂隆通过推理的过程来建立经济学基本原理，并试图收集数据，并在检验原理的过程中加以使用。他最具有影响力的见解是关于市场体制的，该体制通过个人私利这一媒介来协调生产者和消费者的活动。直观感受上，非常接近完全竞争市场。</p><p>他总是倾向于将任何经济成分当做是一个完整结构的一部分。例如，在他的体制中人口变化是内生的，而不是外生的。他区分了由短期因素决定的市场价格与他所谓的内在价值即长期均衡价格。他最熟练的技术分析主要在宏观经济学中，即货币供给变化对价格和生产的影响。他将经济体划分为部门，分析收入在部门之间的流动。他注意到，新的资金进入经济体，价格总水平可能改变，但是相对价格也可能改变，并对经济体的不同部门产生影响。</p><h1 id="西班牙思想">西班牙思想</h1><p>因为地理大发现带来了大量的金银，黄金大量流入西班牙，西班牙国内的价格水平上升，西班牙的知识分子开始评价这些迅速变换的经济现象。在西班牙思想中，货币数量理论表明，货币价值即货币购买力是由流通中的货币数量决定的。其中，路易斯·摩里纳对于市场机制的描述就是我们今天的需求与供给定理，以及货币数量理论。</p><blockquote><p>……产品短缺，促使公平价格上升……丰裕使得公平价格下降。进入市场的购买者的数量在一些时候比另一些时候多一些，他们热切的购买愿望引起价格上升……一个地方缺少货币，会导致其他物品价格下降，货币充裕则会使价格上升……</p></blockquote><h1 id="小结">小结</h1><p>重商主义者和重农主义者都认为经济体可以被正式地加以研究，并且发展了一种抽象方法来发觉能够调节经济体的法则。他们第一次让经济理论立足于抽象的模型构建过程。</p><p>重商主义者就货币在确定价格总水平中的作用，以及对外贸易平衡对国内经济活动的影响方面取得了最初的尝试性见解。重农主义者则是提出了经济体不同部门相关性的概念。在面对经济体的基本冲突上，重商主义者和经院哲学都提倡，要么借助政府，要么借助教会，对经济体施加干预。但是重农主义者则认识到利益冲突的结果基本上是协调的，是相对稀缺性所固有的，他们不提倡政府干预，而是提倡自由放任。</p><p>这一时期的一些英国经济学家既不完全符合重商主义，也不完全符合古典阵营。他们否决了交换中的固有冲突这一较为原始的重商主义观点；反驳了永远保持有利贸易平衡的必要性；也正是他们了解了市场是如何运作来调整个别经济活动的。</p><p>这些重要的思想者没能完成理论的大一统，这就是未来亚当·斯密的任务。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;值得注意的是，发明家们并不具备被称为“科学态度”的客观性。——威廉·莱特温&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="https://samaelchen.github.io/categories/ReadingNotes/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数 11</title>
    <link href="https://samaelchen.github.io/linear_algebra_step11/"/>
    <id>https://samaelchen.github.io/linear_algebra_step11/</id>
    <published>2018-07-10T16:00:00.000Z</published>
    <updated>2018-10-09T13:26:34.255Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>向量正交，如果从几何的角度来看，向量的正交可以看作是两个向量垂直。</p><a id="more"></a><p>首先，我们下一些定义。我们将向量的长度叫做norm，记做<span class="math inline">\(\| v \| = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}\)</span>。那么两个向量之间的距离我们用两个向量差的norm表示，记做<span class="math inline">\(\| v - u \|\)</span>。</p><p>然后向量有两种乘积，一种是点乘，一种是叉乘。叉乘就看作是只有一列的矩阵，然后用矩阵的叉乘方法就好了。至于点乘，实际上也可以看作是叉乘。定义如下： <span class="math display">\[v \cdot u = \sum_i^n v_i u_i = v^{\top} u\]</span> 这里再说明一下，默认向量是列向量。</p><p>现在进入正题，向量正交就是两个向量的点内积为0，也就是<span class="math inline">\(u \cdot v = 0\)</span>。那么很自然就会知道，零向量与所有的向量正交。</p><p>那么向量的点内积有一些运算性质： 假设有向量<span class="math inline">\(u，v\)</span>，矩阵<span class="math inline">\(A\)</span>， 常数<span class="math inline">\(c\)</span></p><blockquote><ol type="1"><li><span class="math inline">\(u \cdot u = \| u \|^2\)</span></li><li><span class="math inline">\(u \cdot u = 0\)</span> if and only if <span class="math inline">\(u = 0\)</span></li><li><span class="math inline">\(u \cdot v = v \cdot u\)</span></li><li><span class="math inline">\(u \cdot (v + w) = u \cdot v + u \cdot w\)</span></li><li><span class="math inline">\((v + w) \cdot u = v \cdot u + w \cdot u\)</span></li><li><span class="math inline">\(cu \cdot v = u \cdot cv\)</span></li><li><span class="math inline">\(\| cu \| = |c| \| u \|\)</span></li><li><span class="math inline">\(Au \cdot v = (Au)^{\top} v = u^{\top}A^{\top}v = u \cdot A^{\top}v\)</span></li><li><span class="math inline">\(\| u+v \| \le \|u\| + \|v\|\)</span></li></ol></blockquote><p>如果我们现在有个向量集合，集合里所有的向量互相正交，那么我们就叫这个集合是orthogonal set。那么如果刚好这里的向量都是单位向量，这个集合就可以叫做orthonomal basis。</p><p>现在回过头来看，这样的一个集合有什么用呢？这个向量集合是不是非常像之前的坐标系。然后进一步来看，假设现在有一个集合<span class="math inline">\(S = \{ v_1 \; v_2 \; \cdots \; v_n \}\)</span>是一个orthogonal basis，有一个向量<span class="math inline">\(u\)</span>是这些向量的线性组合，也就是说<span class="math inline">\(u = c_1 v_1 + c_2 v_2 + \cdots + c_n v_n\)</span>，那么如果我们要求<span class="math inline">\(c_i\)</span>，其实非常简单就是<span class="math inline">\(c_i = \frac{u \cdot v_i}{\| v_i \|^2}\)</span>。如果现在再从几何的角度来看，这个<span class="math inline">\(c_i\)</span>其实就是<span class="math inline">\(u\)</span>在<span class="math inline">\(v_i\)</span>上投影的长度。</p><p>那如果现在随便给一个basis，<span class="math inline">\(\{u_1 \; u_2 \; \cdots \; u_n \}\)</span>，现在要将这个basis变成orthogonal basis，要做的是： <span class="math display">\[\begin{align}v_1 &amp; = u_1 \\v_2 &amp; = u_2 - \frac{u_2 \cdot v_1}{\|v_1\|^2}v_1 \\v_3 &amp; = u_3 - \frac{u_3 \cdot v_2}{\|v_2\|^2}v_2 - \frac{u_3 \cdot v_1}{\|v_1\|^2}v_1 \\&amp; \vdots \\v_n &amp; = u_n - \frac{u_n \cdot v_{n-1}}{\|v_{n-1}\|^2}v_{n-1} - \cdots - \frac{u_n \cdot v_1}{\|v_1\|^2}v_1\end{align}\]</span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;向量正交，如果从几何的角度来看，向量的正交可以看作是两个向量垂直。&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数 10</title>
    <link href="https://samaelchen.github.io/linear_algebra_step10/"/>
    <id>https://samaelchen.github.io/linear_algebra_step10/</id>
    <published>2018-07-09T16:00:00.000Z</published>
    <updated>2018-10-09T13:26:34.255Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>在讲矩阵可对角化前，先引入一个概念，矩阵相似。如果存在方阵<span class="math inline">\(A，B\)</span>，一个可逆矩阵<span class="math inline">\(P\)</span>，使得<span class="math inline">\(P^{-1} A P = B\)</span>，那么我们称<span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>是相似的。那么如果现在<span class="math inline">\(B\)</span>是一个对角矩阵的话，那么我们就称<span class="math inline">\(A\)</span>是可对角化的（diagonalizable）。一般而言，这里会用<span class="math inline">\(D\)</span>来表示对角矩阵。</p><a id="more"></a><p>那么对角化有什么意义呢？我们从公式出发看一下，将<span class="math inline">\(P\)</span>表示为<span class="math inline">\([p_1 \; \cdots \; p_n]\)</span>，将<span class="math inline">\(D\)</span>表示为<span class="math inline">\(\begin{bmatrix} d_1 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; d_n \end{bmatrix}\)</span>。我们之前的公式是<span class="math inline">\(P^{-1} A P = D\)</span>，所以<span class="math inline">\(AP = PD\)</span>。</p><p>先看左边，<span class="math inline">\(AP = [Ap_1 \; \cdots \; Ap_n]\)</span>，再看右边<span class="math inline">\(PD = P[d_1 e_1 \; \cdots \; d_n e_n] = [P d_1 e_1 \; \cdots \; P d_n e_n] = [d_1 P e_1 \; \cdots \; d_n P e_n] = [d_1 p_1 \; \cdots \; d_n p_n]\)</span>。这不就是特征根么。</p><p>所以我们就看到<span class="math inline">\(A\)</span>的特征向量可以组成一个向量空间<span class="math inline">\(\mathbb{R}^n\)</span>。</p><p>那么如何对角化呢，只要找到n个线性无关的向量<span class="math inline">\(p_i\)</span>，然后将这些向量组成一个矩阵，就可以得到可逆矩阵<span class="math inline">\(P\)</span>。然后特征根只要按对角线排列就是<span class="math inline">\(D\)</span>。</p><p>解法就是计算<span class="math inline">\(\det(A - tI) = (t-\lambda_1)^{m_1} (t-\lambda_2)^{m_2} \cdots\)</span>。那么因为每个<span class="math inline">\(\lambda\)</span>对应能有的eigenvector数量是小于等于指数<span class="math inline">\(m\)</span>的，只要每一个指数<span class="math inline">\(m\)</span>都等于eigenspace，那么我们就说<span class="math inline">\(A\)</span>可以对角化。</p><p>比如矩阵<span class="math inline">\(A = \begin{bmatrix} -1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 2 \\ 0 &amp; 2 &amp; 1 \end{bmatrix}\)</span>，那么<span class="math inline">\(A\)</span>的因式分解是<span class="math inline">\(-(t+1)^2 (t-3)\)</span>。所以特征根是3和-1。而对应的特征向量就是<span class="math inline">\(\begin{bmatrix}0 \\ 1 \\ 1 \end{bmatrix} \; \begin{bmatrix}1 \\ 0 \\ 0 \end{bmatrix} \; \begin{bmatrix}0 \\ 1 \\ -1 \end{bmatrix}\)</span>。这样我们就完成了对角化。</p><p>矩阵对角化的好处是如果要做连乘的时候，对角矩阵的连乘是非常简单的，这样就可以极大减少计算开销。也就是说<span class="math inline">\(A^m = P^{-1} D^m P\)</span>。</p><p>最后其实回想一下之前的坐标系变换，对角化的过程其实就是一次坐标系的变换过程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在讲矩阵可对角化前，先引入一个概念，矩阵相似。如果存在方阵&lt;span class=&quot;math inline&quot;&gt;\(A，B\)&lt;/span&gt;，一个可逆矩阵&lt;span class=&quot;math inline&quot;&gt;\(P\)&lt;/span&gt;，使得&lt;span class=&quot;math inline&quot;&gt;\(P^{-1} A P = B\)&lt;/span&gt;，那么我们称&lt;span class=&quot;math inline&quot;&gt;\(A\)&lt;/span&gt;和&lt;span class=&quot;math inline&quot;&gt;\(B\)&lt;/span&gt;是相似的。那么如果现在&lt;span class=&quot;math inline&quot;&gt;\(B\)&lt;/span&gt;是一个对角矩阵的话，那么我们就称&lt;span class=&quot;math inline&quot;&gt;\(A\)&lt;/span&gt;是可对角化的（diagonalizable）。一般而言，这里会用&lt;span class=&quot;math inline&quot;&gt;\(D\)&lt;/span&gt;来表示对角矩阵。&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>经济思想史读书笔记——早期古典经济学</title>
    <link href="https://samaelchen.github.io/economics_thoughts_notes2/"/>
    <id>https://samaelchen.github.io/economics_thoughts_notes2/</id>
    <published>2018-07-07T16:00:00.000Z</published>
    <updated>2018-10-09T13:26:34.255Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>“我们从哪儿出发？”红皇后问。 “从开始出发。”渡渡鸟答道。——Lewis Carroll</p><a id="more"></a><p>早期的经济研究并不系统，没有出现重大的分析体系。直到18世纪中期，随着Adam Smith引领的“古典经济学”的出现，经济学才向着成熟的社会科学状态大踏步前进。</p><p>古典经济学始于1776年Adam Smith《国富论》的出版，通常将1776年之前划分为两个部分。公元前800年到1500年的早期前古典阶段，以及从1500年1776年的前古典时期。</p><p>从时间线上看，早期前古典经济思想可以分为中国、希腊、阿拉伯——伊斯兰思想以及经院哲学。</p><h1 id="东方经济思想">东方经济思想</h1><p>中国的早期经济思想以管仲为代表。熊彼特认为早期的著作“基本上被限制在道德的框架内考察公共事务管理，而不是严格的、‘科学的’研究”。但是《管子》极大程度上超越了公共事务管理的模子。</p><p>管仲的经济思想核心可以用“轻/重”理论来表达。他提出当一种产品充裕时，就会变“轻”，价格就会下降。当产品被“锁藏”时，就会变“重”，价格上涨。产品会依据这种变化进入或退出市场，最后向一个均衡的价格运动。</p><p>事实上，这种理论是非常类似微观经济学里的供需理论。只不过，这样的供需理论需要基于一个基本的前提，就是市场是完全竞争的市场。</p><p>管仲甚至将这一理论延伸到了货币数量。也就是说，货币“重”的时候，货币价格上涨，产品价格就下降了，反之，货币价格下降，产品价格上涨。如果类比到现在的概念，就是通货紧缩和通货膨胀。而管仲认为，应对通货紧缩时候国家应该购买产品，目的是让价格上涨；而通货膨胀的时候就应该卖出产品，让产品价格下降。</p><p>当代应对通货膨胀或者通货紧缩的问题一般都是采取货币政策，也就是央妈通过操控市场上的现金解决问题。最简单的，通货膨胀时，减少货币供给量，提高银行准备金率，提高储蓄利率等等。而通货紧缩就反过来。</p><p>齐桓公曾向管仲询问，丰收之年如何来遏制粮食价格的过度下滑。管仲认为应当减少粮食的流通量，为此必须采取政府强制手段，他说：“请以令与大夫城藏，使卿诸侯藏千钟，令大夫藏五百钟，列大夫藏百钟，富商蓄贾藏五十钟。内可以为国委，外可以益农夫之事。”这里需要注意的是，在春秋时代粮食实际上不仅是一种商品，也具备一定的货币功能。管仲认为强制减少粮食流通量，自然可以制止价格的下滑。既可以增加国家的粮食储备，又可以维护农民的利益；相反，在歉收之年，则将储备之粮食投入市场。以遏制粮商囤积哄抬价格，又可以使政府得到一笔收入，所以管仲说：“夫民有余则轻之，故人君敛之以轻；民不足则重之，故人君散之以重。敛积之以轻，散行之以重，故君必有十倍之利。”</p><p>如果将粮食当做是一种流通货币，那么这不就是现在的货币政策么？</p><p>在管仲的思想体系下，一方面存在市场“无形的手”，另一方面，又存在国家调控这样“有形的手”。是不是听上去又很像某种经济形式。</p><h1 id="希腊思想">希腊思想</h1><h2 id="赫西奥德和色诺芬">赫西奥德和色诺芬</h2><p>赫西奥德和色诺芬都对效率感兴趣。经济学家通常用产出和投入的比率衡量效率。早期的经济学家对社会层面的效率问题并不感兴趣。他们更多关注一组和生产者与家庭层面的效率相关的问题。而色诺芬则在赫西奥德之后采用了有效管理的概念，并且在家庭、生产者、军事和公关事务层面使用了这些概念。这种思想的进步在于，他充分认识到了劳动分工可以提高效率。</p><p>这种思想影响了包括亚里士多德在内的很多希腊经济学家。并且在之后的时间里也影响了经院哲学。</p><h2 id="亚里士多德">亚里士多德</h2><p>德谟克利特认为，不仅需要劳动分工，而且认为财产私有化可以促进更多的经济活动。但是亚里士多德的老师柏拉图则认为，统治者不应该拥有私人财产，而应该掌控公共财产，避免对财产的争夺。</p><p>但是亚里士多德则不一样。亚里士多德一方面谴责追求经济利益的行为，一方面又认可私有财产的权利。亚里士多德对经济思想的最大贡献在于商品交换与交换中货币的使用。</p><p>亚里士多德认为人的需要是适度的，但是欲望是无穷的。因此满足需要的商品生产是恰当的，但是力图满足无穷欲望的产品生产就是不正常的。他推崇以物易物的交易方式，而不是通过货币这一媒介，通过交易获取货币收益。事实上，现在主流的经济学家不区分人类的需求和欲望，在高度分工化的现在，客观上区分需求和欲望是不太现实的。</p><p>亚里士多德一个值得关注的论点是，减少消费来改变人们的态度，希望通过排除稀缺性固有的冲突来解决社会冲突。这是大多数乌托邦人士和社会主义者的有力观点。</p><h1 id="阿拉伯伊斯兰思想">阿拉伯——伊斯兰思想</h1><p>阿拉伯——伊斯兰思想填补了亚里士多德到经院哲学之间的空白。</p><p>艾布·哈米德·安萨里认识到了物物交换的困难，以及货币对交易的便利。同时他也考察了很多其他的经济话题，例如公共支出、征税与借贷等等。</p><p>而伊本·赫勒敦考察的更多是人口、利润、供给、需求、价格、奢侈品、总剩余、资本等。</p><h1 id="经院哲学">经院哲学</h1><p>经院哲学的基础是封建社会。经院哲学经济学家们试图提供适用于世俗活动的宗教指导方针。他们的的目的不是分析发生了什么样的经济活动，而是订立与宗教教义相一致的经济行为规则。经院哲学关注价格体系中的公正或公正缺失。最重要的经院哲学经济学家是圣托马斯·阿奎那。</p><p>经院哲学的核心经济问题还是：私有财产制度以及公平价格与高利贷的概念。</p><p>在私有财产方面，阿奎那的杰出贡献在于，他融合了宗教教义与亚里士多德的著作。在《圣经》中，基督教思想谴责私人财产，但是他认为，私人财产不是违背自然法则，而是自然法则的一种补充。他论证说，裸体是自然法则，但是服装是对自然法则的补充，私有财产也是为了人们的利益而设计的。他认可私人财产的不平等分配，但是对于坚定投身宗教的人，短缺与公共生活是一种理想状态。</p><p>而对于产品价格，经院哲学经济学家不关心经济体中价格的形成，或者去了解价格在稀缺资源配置中所扮演的角色。他们关心价格的道德性，也就是价格的公平与公正的概念。不同的理论史学家对公正价格的理解是不一样的。但是经院哲学的公正价格可以看作是李嘉图——马克思劳动价值理论、边际效用观点，以及古典——新古典理论中按时的竞争性市场产生理想公正价格观点的先驱。另一些主张也认为，所谓的公正价格就是市场上的所有价格。但是经院哲学对于经济分析的缺失让我们难以确定究竟“公正价格”意味着什么。</p><p>从公正价格会很自然推论到高利贷的观点。高利贷在现在代表着索要过高的利率，但是在经院哲学和亚里士多德的著作里，高利贷代表着任意的（any）获利行为。但是相对于亚里士多德对借贷的谴责以及宗教对通过货币获利的严厉禁止，经院哲学至少在为了商业目的通过货币获利这点上观点逐渐缓和。</p><p>阿奎那本身也是一个充满矛盾的思想家，一方面他强调道德问题来抑制经济思想；另一方面，又推动了经济学与所有社会科学的前进。</p><h1 id="小结">小结</h1><p>事实上，早期的经济思想并没有关注价格系统的特性与关注，只有管仲是例外的。而希腊思想则考察了私人财产的作用，亚里士多德的许多观点成为后来经院哲学的考察焦点。阿拉伯——伊斯兰思想有效填补了希腊思想到经院哲学之间的历史空白。而经院哲学的目的更多在于确定宗教标准，借此判断经济行为，而不是分析经济体。不过，经院哲学的存在背景是封建制度，随着技术变革的破坏，经济生活对精神生活构成了极大的挑战。但是从历史的进程上看，将经济体从教会下解放出来，既发生在实践层面上，也发生在学术层面上。实际上，这种解放极大促进了西方资本主义的发展，从而构建起现代金融货币体系。关于货币从宗教解放的资料可以看看央视的纪录片《货币》第二集，犹太人最早从宗教总解脱出来，施展金融才华，这一步在货币历史上至关重要。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;“我们从哪儿出发？”红皇后问。 “从开始出发。”渡渡鸟答道。——Lewis Carroll&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="https://samaelchen.github.io/categories/ReadingNotes/"/>
    
    
  </entry>
  
  <entry>
    <title>经济思想史读书笔记——导言</title>
    <link href="https://samaelchen.github.io/economics_thoughts_notes1/"/>
    <id>https://samaelchen.github.io/economics_thoughts_notes1/</id>
    <published>2018-07-02T16:00:00.000Z</published>
    <updated>2018-10-09T13:26:34.255Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>人类一切事业的开始和终结都是凌乱的——John Galsworthy</p><a id="more"></a><h1 id="现代经济学核心思想">现代经济学核心思想</h1><p>经济学是一门研究稀缺性问题的学科。历史上人类曾经有过四种机制来解决稀缺性问题。</p><blockquote><p>第一种，强制方式。<br>第二种，传统方式。也就是依据以往的惯例方式分配。<br>第三种，权威方式。也就是政府，教会的方式。<br>第四种，市场方式。这也是现在在不断发展的分配方式。</p></blockquote><p>一般而言，这些方式之间并不相互排斥。市场方式是现在最主流的分配方式。</p><p>现代经济学分为微观经济学和宏观经济学。微观经济学研究配置问题（生产什么和如何生产）与分配问题（实际收入如何在社会成员间分配）。因此微观经济学更多关注于供给和需求的理论，希望解释决定相对价格的力量。</p><p>而宏观经济学关注的社会的总体分析，自上而下到个体。主要聚焦于经济体的总体分析，主要聚焦于经济体的稳定和增长，关注整体经济的总变量：收入和就业水平、价格总水平、经济增长率等。</p><h1 id="研究经济思想史的方法">研究经济思想史的方法</h1><p>研究经济思想史的方法有两种，一种是相对论史学家，一种是绝对论史学家。</p><p>相对论史学家关心的是：（1）引起人们考察某些经济问题的历史、经济、社会和政治力量是什么。（2）这些力量塑造新兴理论内容的方式。他们主张在每个经济理论的发展过程中，历史都扮演一定的角色。</p><p>绝对论史学家又被称作辉格党，强调内在力量对经济理论发展的解释。绝对论者认为，理论的进步不仅仅反映了历史环境，而且取决于训练有素的专业人员对未决问题或似是而非论点的发现与解释。</p><p>相对论者和绝对论者在经济学历史上交互站上历史舞台，但是经济思想史一定是学科的外在力量和内在力量相互作用的动态过程。</p><p>研究经济思想史可以帮助我们更好理解正统经济理论和非正统经济理论。正统经济理论家主要集中在资源配置、分配、稳定和增长这四个问题上面；而非正统经济理论家则研究社会与经济中产生变换的力量。正统经济学家认为具体的社会制度、政治制度与经济制度是既定的，并在这些制度背景下研究经济行为；非正统经济学家则试图去进行解释。</p><p>正统经济学家和非正统经济学家的差别主要在于他们关注的问题不一致，而不是理论本身的直接对立。</p><h1 id="非正统经济学家的地位">非正统经济学家的地位</h1><p>所谓的正统非正统，或者说主流非主流，只是在竞争中最受欢迎的一组成为主流，不太成功的一组成为非主流。主流研究院更倾向采纳主流思想中较为狭窄的观点，而相比之下，非正统经济学家可能会更重视思想的多样性。</p><h1 id="方法论问题">方法论问题</h1><p>经济思想中需要区分经济学艺术（the art of economics），实证经济学（positive economics）和规范经济学（normative economics）。</p><p>实证经济学关心的是支配经济活动的力量。而经济学艺术关心的是政策问题。规范经济学则明确地关注应当是什么的问题。</p><p>实证经济学的方法是形式化的、抽象的，试图将经济力量与政治和社会力量分开。而经济学艺术则需要致力于政治社会力量与经济力量相互关系的研究。</p><p>以史为鉴，才能看懂现在莫名其妙的经济形式。</p><h1 id="经济学方法论进化路线">经济学方法论进化路线</h1><h2 id="逻辑实证主义logical-positivism的兴起">逻辑实证主义（logical positivism）的兴起</h2><p>逻辑实证主义家的典型代表是维也纳派，他们试图通过描述科学家实际遵循的方法，来使科学家的方法形式化。他们认为，只有当一种演绎理论在经验上被检验与核实之后，它才能被认同为正确的。逻辑实证主义将“科学的目的是确立‘真理’”的观点推到了极致。</p><h2 id="证伪主义">证伪主义</h2><p>证伪主义在波普尔的著作中得到很好的表达。她提出，经验检验不能确定一种理论的真相，只能确定假象。波普尔声称，科学的目的应当是运用经验上可检验的假设来发展理论，然后对理论进行证伪，放弃那些被证明是错误的理论。</p><p>但是证伪主义存在三个问题。第一个问题是，一些理论的经验预言并不能被检验，因为尚不存在对它们进行检验的方法。第二个问题是，难以决定是否理论被证伪或者没有被证伪。第三个问题是研究者的心态，他们未能检验已确立理论的含义，便假定理论的含义是正确的（让我想起李祥林的高斯相依函数，号称摧毁华尔街的公式）。</p><h2 id="范式">范式</h2><p>托马斯·库恩将范式引入方法论的争论中，将方法论远离了证伪主义。范式是一种既定的方法以及构成研究者分析组成部分的知识题，它遵循着任何既定时期所公认的对主流科学思想教科书陈述。</p><p>范式隐藏着这样的观点，现有理论可能并不包含真理。</p><h2 id="研究纲领">研究纲领</h2><p>Imre Lakatos发现，科学家们从事发展竞争性研究纲领时，每个研究纲领不仅包括一系列的数据进行分析和证伪，而且包括无可非议地接受一系列的硬核逻辑假设。每项研究都从硬核中得出一系列周边假设，并试图对它们进行证伪。只有当“足够多”的周边假设被证伪，硬核假设才会被重新考虑。Lakatos认为，如果对周边假设进行证伪的过程在继续，那就是进步（progressive）的，否则就是退化（degenerative）的。</p><p>他的研究有两个特性：（1）它承认了理论证伪过程的复杂性；（2）早起的分析要求某一种理论成为主流，Lakatos则提出多种可利用理论同时存在，这些理论的优点不太容易辨别。</p><h2 id="社会学方法与修辞方法">社会学方法与修辞方法</h2><p>社会学方法与修辞方法拒绝了假设存在终极的、神圣的真理。修辞方法强调语言的说服力。该方法主张，一种理论被接受，不是因为它本身是正确的，而是因为理论的提倡者借助他们出众的修辞，成功地使其他人相信理论的价值。</p><p>社会学方法考察社会与制度约束，这些约束影响着对一种理论的认可。</p><p>这两种方法都对人们发现真理的能力表示怀疑，甚至怀疑真理是否存在。在这些方法中，理论的发展并不一定因为离真理近，理论的发展有多种理由，而真理——如果存在——仅仅只是其中之一。</p><p>这两种方法最有代表性的观点就是费耶阿本德的“一切尽随其便”。</p><h2 id="后修辞学方法">后修辞学方法</h2><p>后修辞经济学家会以怀疑论来看待与研究者自身利益或预想观点相符的研究结果。他们非常有可能遵循贝叶斯统计而不是古典统计。</p><p>贝叶斯主义者认为，人们能够发现语句中更高级或更低级的真理，但不是终极真理。</p><p>事实上，在大多数教科书中，经济学的主流方式依旧是逻辑实证主义，而它在学术杂志上已经死了很久了。形式主义更有可能运用逻辑实证主义或者证伪主义，并且相信绝对论方法。而非形式主义更可能运用社会的或修辞的方法，并相信相对主义方法。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;人类一切事业的开始和终结都是凌乱的——John Galsworthy&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="https://samaelchen.github.io/categories/ReadingNotes/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数 09</title>
    <link href="https://samaelchen.github.io/linear_algebra_step9/"/>
    <id>https://samaelchen.github.io/linear_algebra_step9/</id>
    <published>2018-07-02T16:00:00.000Z</published>
    <updated>2018-10-09T13:26:34.259Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>矩阵的特征根与特征向量。这两个是线性代数里面非常重要的概念。</p><a id="more"></a><p>我们对这两个概念的定义是这样的，如果存在一个矩阵<span class="math inline">\(A\)</span>可以使得常数<span class="math inline">\(\lambda\)</span>和向量<span class="math inline">\(v\)</span>满足： <span class="math display">\[Av = \lambda v\]</span> 那么<span class="math inline">\(\lambda\)</span>就是矩阵的特征根，而<span class="math inline">\(v\)</span>就是特征向量。但是这里需要注意的是，<span class="math inline">\(A\)</span>一定是方阵。举个例子： <span class="math display">\[\begin{bmatrix}5 &amp; 2 &amp; 1 \\ -2 &amp; 1 &amp; -1 \\ 2 &amp; 2 &amp; 4\end{bmatrix} \begin{bmatrix}1 \\ -1 \\ 1\end{bmatrix} = \begin{bmatrix}4 \\ -4 \\ 4\end{bmatrix} = 4\begin{bmatrix}1 \\ -1 \\ 1\end{bmatrix}\]</span></p><p>不过要注意的是，零向量不能作为特征向量。</p><p>所以直观的感受上，特征向量就是经过线性变换以后，只是改变向量长度或者是变成相反的方向。</p><p>那么在特征根特征向量在图像中有什么意义呢？下面举四个例子。</p><p>首先是扭曲，如下图：</p><p><img src="https://i.imgur.com/4z9YFVw.png"></p><p>那么这样的变换过程中，落在横轴上的向量是没有发生任何变换的，因此图片中蓝色的向量就是特征向量，特征根是1。</p><p>第二个变换是映射，如下：</p><p><img src="https://i.imgur.com/KFeF5NV.png"></p><p>在这个变换中，<span class="math inline">\(b_1\)</span>是没有变化的，所以是一个特征根为1的特征向量。而<span class="math inline">\(b_2\)</span>则是刚好反了一个方向，因此是一个特征根为-1的特征向量。</p><p>第三种变换是缩放，如下：</p><p><img src="https://i.imgur.com/hSWgUwf.png"></p><p>在这种情况下，图片的所有向量都是特征向量，特征根就是缩放倍数。</p><p>第四种是旋转，如下：</p><p><img src="https://i.imgur.com/03ETOyW.png"></p><p>因为上面的旋转过程中，没有一个向量保持了原来的方向，或者转到完全相反的方向，因此这样的变换过程中，没有特征向量。</p><p>从上面的四个例子我们还可以发现一个很重要的事情，就是一个特征向量只有唯一对应一个特征根，但是一个特征根可以有多个特征向量。然后，我们就可以顺势定义一个新的概念，eigenspace。也就是<span class="math inline">\(\lambda\)</span>对应的所有特征向量加上零向量构成的subspace。</p><p>那么我们要怎么去找到特征向量和特征根呢？</p><p>首先我们回顾一下之前的公式： <span class="math display">\[Av = \lambda v = \lambda I v\]</span> 所以<span class="math inline">\((A-\lambda I) v = 0\)</span>。这样一来，我们就知道，当我们知道<span class="math inline">\(\lambda\)</span>的时候，只要找到上面那个等式的非零解，就是我们的特征根。</p><p>那么如果现在要判断一个常数是不是特征根，我们依照上面那个等式一步步向下推理，因为<span class="math inline">\((A-\lambda I)v = 0\)</span>有多个解，因此我们可以知道<span class="math inline">\(\text{Rank} (A - \lambda I) &lt; n\)</span>，所以<span class="math inline">\(A - \lambda I\)</span>不可逆，也就是说它的行列式为0。</p><p>比如说矩阵<span class="math inline">\(A = \begin{bmatrix}-4 &amp; 3 \\ 3 &amp; 6 \end{bmatrix}\)</span>，我们计算行列式<span class="math inline">\(\begin{bmatrix}-4-\lambda &amp; 3 \\ 3 &amp; 6-\lambda \end{bmatrix} = 0\)</span>。也就是说<span class="math inline">\((-4-\lambda)(6-\lambda) - 9 = 0\)</span>。所以我们就可以求出来，<span class="math inline">\(\lambda = -3\)</span>或<span class="math inline">\(\lambda = 5\)</span>。</p><p>那么特征根有一些特性。首先，一般来说，一个矩阵跟它的RREF的特征根是不一样的。如果是两个矩阵的因式分解一样，那么就有一样的特征根。</p><p>假设现在有个矩阵<span class="math inline">\(A\)</span>有<span class="math inline">\(n\)</span>个特征根（这里只考虑实数根），那么特征根的和刚好就是<span class="math inline">\(\text{trace } A\)</span>，也就是<span class="math inline">\(A\)</span>的对角线元素的和；特征根的乘积刚好就是<span class="math inline">\(\det A\)</span>。</p><p>实际上，理解一下特征根和特征向量在图像中的意义，然后知道特征根的解法是<span class="math inline">\(\det(A - \lambda I)\)</span>，特征向量的解法是<span class="math inline">\((A - \lambda I)v = 0\)</span>就好了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;矩阵的特征根与特征向量。这两个是线性代数里面非常重要的概念。&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数 08</title>
    <link href="https://samaelchen.github.io/linear_algebra_step8/"/>
    <id>https://samaelchen.github.io/linear_algebra_step8/</id>
    <published>2018-07-01T16:00:00.000Z</published>
    <updated>2018-10-09T13:26:34.255Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Coordinate system，就是坐标系。其实就是一组vector。</p><a id="more"></a><p>能够拿来做坐标系的vector set，很显然，按照上一篇博客里的内容，我们可以很自然想到，一个vector set必须符合两个条件：</p><ol type="1"><li>这个vector set是<span class="math inline">\(\mathbb{R}^n\)</span>的span</li><li>这个vector set里面的vector是independent的</li></ol><p>那其实这个vector set就是basis。</p><p>如果现在我们的basis刚刚好每个vector都是相互垂直的单位向量，那么我们就会把这个坐标系叫做直角坐标系。</p><p>那么从这个角度来看，其实，我们就可以将矩阵乘法看作是坐标系转换。而且坐标系转换，矩阵一定是可逆的。</p><p>所以如果我们要做任意的坐标系和直角坐标系之间的转换，我们遵守如下的公式：</p><p>从<span class="math inline">\(v_{B}\)</span>到<span class="math inline">\(v\)</span>就是<span class="math inline">\(v = B v_{B}\)</span>，反过来就是<span class="math inline">\(v_{B} = B^{-1} v\)</span>。</p><p>事实上，坐标系转换，或者说线性变换是机器学习里面非常常见的一种情况。比如说PCA就是这样的一种变换。PCA有一点像是在找basis。另外如果了解NMF的话，NMF看上去更像是将一组数据的basis找出来。不过要注意的是，仅仅是看上去很像而已。</p><p>线性变换具有很显著的意义，将一个在原来坐标系下面很复杂的函数，通过线性变换以后就可能得到一个非常简单的函数。</p><p>如下图：</p><p><img src="https://i.imgur.com/HJTQBeg.png"></p><p>我们要做一个关于直线<span class="math inline">\(y = \frac{1}{2} x\)</span>的映射关系。如果这个映射在直角坐标系下面，那么我们的变换矩阵是<span class="math inline">\(\begin{bmatrix} 0.6 &amp;0.8 \\ 0.8 &amp;-0.6 \end{bmatrix}\)</span>。但是如果我们用这条直线作为横轴，垂直于这条直线的向量为纵轴，就会发现，其实在这个坐标系<span class="math inline">\(\begin{bmatrix} 2 &amp;-1 \\ 1 &amp;2 \end{bmatrix}\)</span>内，变换只是<span class="math inline">\(\begin{bmatrix} 1 &amp;0 \\ 0 &amp;-1 \end{bmatrix}\)</span>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Coordinate system，就是坐标系。其实就是一组vector。&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
</feed>
