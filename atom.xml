<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>碎碎念</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://samaelchen.github.io/"/>
  <updated>2018-08-15T12:37:15.365Z</updated>
  <id>https://samaelchen.github.io/</id>
  
  <author>
    <name>Samael Chen</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PyTorch实现LSTM情感分析</title>
    <link href="https://samaelchen.github.io/pytorch_lstm_sentiment/"/>
    <id>https://samaelchen.github.io/pytorch_lstm_sentiment/</id>
    <published>2018-08-14T16:00:00.000Z</published>
    <updated>2018-08-15T12:37:15.365Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>尝试使用LSTM做情感分析，这个gluon有非常详细的例子，可以直接参考gluon的<a href="http://zh.gluon.ai/chapter_natural-language-processing/sentiment-analysis.html" target="_blank" rel="noopener">官方教程</a>。这里尝试使用PyTorch复现一个。数据用的是IMDB的数据<a href="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz" class="uri" target="_blank" rel="noopener">http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz</a></p><a id="more"></a><p>首先我们导入相关的package：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.utils.data.dataloader <span class="keyword">as</span> dataloader</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torchtext.vocab <span class="keyword">as</span> torchvocab</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> snowballstemmer</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> chain</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br></pre></td></tr></table></figure><p>然后我们定义读数的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readIMDB</span><span class="params">(path, seg=<span class="string">'train'</span>)</span>:</span></span><br><span class="line">    pos_or_neg = [<span class="string">'pos'</span>, <span class="string">'neg'</span>]</span><br><span class="line">    data = []</span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> pos_or_neg:</span><br><span class="line">        files = os.listdir(os.path.join(path, seg, label))</span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">            <span class="keyword">with</span> open(os.path.join(path, seg, label, file), <span class="string">'r'</span>, encoding=<span class="string">'utf8'</span>) <span class="keyword">as</span> rf:</span><br><span class="line">                review = rf.read().replace(<span class="string">'\n'</span>, <span class="string">''</span>)</span><br><span class="line">                <span class="keyword">if</span> label == <span class="string">'pos'</span>:</span><br><span class="line">                    data.append([review, <span class="number">1</span>])</span><br><span class="line">                <span class="keyword">elif</span> label == <span class="string">'neg'</span>:</span><br><span class="line">                    data.append([review, <span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">train_data = readIMDB(<span class="string">'aclImdb'</span>)</span><br><span class="line">test_data = readIMDB(<span class="string">'aclImdb'</span>, <span class="string">'test'</span>)</span><br></pre></td></tr></table></figure><p>接着是分词，这里只做非常简单的分词，也就是按照空格分词。当然按照一些传统的清洗方式效果会更好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenizer</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [tok.lower() <span class="keyword">for</span> tok <span class="keyword">in</span> text.split(<span class="string">' '</span>)]</span><br><span class="line"></span><br><span class="line">train_tokenized = []</span><br><span class="line">test_tokenized = []</span><br><span class="line"><span class="keyword">for</span> review, score <span class="keyword">in</span> train_data:</span><br><span class="line">    train_tokenized.append(tokenizer(review))</span><br><span class="line"><span class="keyword">for</span> review, score <span class="keyword">in</span> test_data:</span><br><span class="line">    test_tokenized.append(tokenizer(review))</span><br><span class="line"></span><br><span class="line">vocab = set(chain(*train_tokenized))</span><br><span class="line">vocab_size = len(vocab)</span><br></pre></td></tr></table></figure><p>因为这个数据集非常小，所以如果我们用这个数据集做word embedding有可能过拟合，而且模型没有通用性，所以我们传入一个已经学好的word embedding。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">wvmodelwvmodel = gensim.models.KeyedVectors.load_word2vec_format(<span class="string">'test_word.txt'</span>,</span><br><span class="line">                                                          binary=<span class="keyword">False</span>, encoding=<span class="string">'utf-8'</span>)</span><br></pre></td></tr></table></figure><p>这里的“test_word.txt”是我将glove的词向量转换后的结果，当时测试gensim的这个功能瞎起的名字，用的是glove的6B，100维的预训练数据。</p><p>然后一样要定义一个word to index的词典：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">word_to_idxword_to  = &#123;word: i+<span class="number">1</span> <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(vocab)&#125;</span><br><span class="line">word_to_idx[<span class="string">'&lt;unk&gt;'</span>] = <span class="number">0</span></span><br><span class="line">idx_to_word = &#123;i+<span class="number">1</span>: word <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(vocab)&#125;</span><br><span class="line">idx_to_word[<span class="number">0</span>] = <span class="string">'&lt;unk&gt;'</span></span><br></pre></td></tr></table></figure><p>定义的目的是为了将预训练的weight跟我们的词库拼上。另外我们定义了一个unknown的词，也就是说没有出现在训练集里的词，我们都叫做unknown，词向量就定义为0。</p><p>然后就是编码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode_samples</span><span class="params">(tokenized_samples, vocab)</span>:</span></span><br><span class="line">    features = []</span><br><span class="line">    <span class="keyword">for</span> sample <span class="keyword">in</span> tokenized_samples:</span><br><span class="line">        feature = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> sample:</span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">in</span> word_to_idx:</span><br><span class="line">                feature.append(word_to_idx[token])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                feature.append(<span class="number">0</span>)</span><br><span class="line">        features.append(feature)</span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_samples</span><span class="params">(features, maxlen=<span class="number">500</span>, PAD=<span class="number">0</span>)</span>:</span></span><br><span class="line">    padded_features = []</span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> features:</span><br><span class="line">        <span class="keyword">if</span> len(feature) &gt;= maxlen:</span><br><span class="line">            padded_feature = feature[:maxlen]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            padded_feature = feature</span><br><span class="line">            <span class="keyword">while</span>(len(padded_feature) &lt; maxlen):</span><br><span class="line">                padded_feature.append(PAD)</span><br><span class="line">        padded_features.append(padded_feature)</span><br><span class="line">    <span class="keyword">return</span> padded_features</span><br></pre></td></tr></table></figure><p>我们这里为了解决评论长度不一致的问题，将所有的评论都取500个词，超过的就取前500个，不足的补0。</p><p>整理一下训练数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_features = torch.tensor(pad_samples(encode_samples(train_tokenized, vocab)))</span><br><span class="line">train_labels = torch.tensor([score <span class="keyword">for</span> _, score <span class="keyword">in</span> train_data])</span><br><span class="line">test_features = torch.tensor(pad_samples(encode_samples(test_tokenized, vocab)))</span><br><span class="line">test_labels = torch.tensor([score <span class="keyword">for</span> _, score <span class="keyword">in</span> test_data])</span><br></pre></td></tr></table></figure><p>然后就是定义网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentimentNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 bidirectional, weight, labels, use_gpu, **kwargs)</span>:</span></span><br><span class="line">        super(SentimentNet, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.use_gpu = use_gpu</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(weight)</span><br><span class="line">        self.embedding.weight.requires_grad = <span class="keyword">False</span></span><br><span class="line">        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=self.num_hiddens,</span><br><span class="line">                               num_layers=num_layers, bidirectional=self.bidirectional,</span><br><span class="line">                               dropout=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            self.decoder = nn.Linear(num_hiddens * <span class="number">4</span>, labels)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.decoder = nn.Linear(num_hiddens * <span class="number">2</span>, labels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        embeddings = self.embedding(inputs)</span><br><span class="line">        states, hidden = self.encoder(embeddings.permute([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]))</span><br><span class="line">        encoding = torch.cat([states[<span class="number">0</span>], states[<span class="number">-1</span>]], dim=<span class="number">1</span>)</span><br><span class="line">        outputs = self.decoder(encoding)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><p>那这里需要注意几个点，第一，LSTM可以不initialize hidden，如果不initialize的话，那么PyTorch会默认初始为0。</p><p>另外就是LSTM这里传进去的数据格式是[seq_len, batch_size, embedded_size]。而我们传进去的数据是[batch_size, seq_len]的样子，那经过embedding之后的结果是[batch_size, seq_len, embedded_size]。所以我们这里要将第二个维度和第一个维度做个调换。这样返回的数据就是[batch_size, embedded_size]。不过LSTM有个参数叫batch_first，如果设为True，那么返回的就会是[seq_len, embedded_size]。这里非常的绕，我在这里卡了好久(=<span class="citation" data-cites="__">@__</span>@=)</p><p>第三就是我这里用了最初始的状态和最后的状态拼起来作为分类的输入。</p><p>另外有一点吐槽的就是，MXNet的dense层比较强大啊，不用定义输入的维度，只要定义输出的维度就可以了，操作比较骚啊。</p><p>然后我们把weight导进来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">weight = torch.zeros(vocab_size+<span class="number">1</span>, embed_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(wvmodel.index2word)):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        index = word_to_idx[wvmodel.index2word[i]]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    weight[index, :] = torch.from_numpy(wvmodel.get_vector(</span><br><span class="line">        idx_to_word[word_to_idx[wvmodel.index2word[i]]]))</span><br></pre></td></tr></table></figure><p>这里我们将不在glove里面的词全部填为0，后面想了一下，其实也可以试试这些全部随机试试。</p><p>接着定义参数就可以训练了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">embed_size = <span class="number">100</span></span><br><span class="line">num_hiddens = <span class="number">100</span></span><br><span class="line">num_layers = <span class="number">2</span></span><br><span class="line">bidirectional = <span class="keyword">True</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">labels = <span class="number">2</span></span><br><span class="line">lr = <span class="number">0.8</span></span><br><span class="line">device = torch.device(<span class="string">'cuda:0'</span>)</span><br><span class="line">use_gpu = <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">net = SentimentNet(vocab_size=(vocab_size+<span class="number">1</span>), embed_size=embed_size,</span><br><span class="line">                   num_hiddens=num_hiddens, num_layers=num_layers,</span><br><span class="line">                   bidirectional=bidirectional, weight=weight,</span><br><span class="line">                   labels=labels, use_gpu=use_gpu)</span><br><span class="line">net.to(device)</span><br><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=lr)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train_set = torch.utils.data.TensorDataset(train_features, train_labels)</span><br><span class="line">test_set = torch.utils.data.TensorDataset(test_features, test_labels)</span><br><span class="line"></span><br><span class="line">train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size,</span><br><span class="line">                                         shuffle=<span class="keyword">True</span>)</span><br><span class="line">test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size,</span><br><span class="line">                                        shuffle=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p>这个位置需要注意的是，我们在train加了一个shuffle，如果不加shuffle的话，模型会学到奇奇怪怪的地方去。</p><p>最后训练一下就好了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    start = time.time()</span><br><span class="line">    train_loss, test_losses = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    train_acc, test_acc = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    n, m = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> feature, label <span class="keyword">in</span> train_iter:</span><br><span class="line">        n += <span class="number">1</span></span><br><span class="line">        net.zero_grad()</span><br><span class="line">        feature = Variable(feature.cuda())</span><br><span class="line">        label = Variable(label.cuda())</span><br><span class="line">        score = net(feature)</span><br><span class="line">        loss = loss_function(score, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        train_acc += accuracy_score(torch.argmax(score.cpu().data,</span><br><span class="line">                                                 dim=<span class="number">1</span>), label.cpu())</span><br><span class="line">        train_loss += loss</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> test_feature, test_label <span class="keyword">in</span> test_iter:</span><br><span class="line">            m += <span class="number">1</span></span><br><span class="line">            test_feature = test_feature.cuda()</span><br><span class="line">            test_label = test_label.cuda()</span><br><span class="line">            test_score = net(test_feature)</span><br><span class="line">            test_loss = loss_function(test_score, test_label)</span><br><span class="line">            test_acc += accuracy_score(torch.argmax(test_score.cpu().data,</span><br><span class="line">                                                    dim=<span class="number">1</span>), test_label.cpu())</span><br><span class="line">            test_losses += test_loss</span><br><span class="line">    end = time.time()</span><br><span class="line">    runtime = end - start</span><br><span class="line">    print(<span class="string">'epoch: %d, train loss: %.4f, train acc: %.2f, test loss: %.4f, test acc: %.2f, time: %.2f'</span> %</span><br><span class="line">          (epoch, train_loss.data / n, train_acc / n, test_losses.data / m, test_acc / m, runtime))</span><br></pre></td></tr></table></figure><p>也可以直接看我的<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/langage%20model/lstm-sentiment.ipynb" target="_blank" rel="noopener">notebook</a></p><p>后面试试textCNN，感觉也挺骚气的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;尝试使用LSTM做情感分析，这个gluon有非常详细的例子，可以直接参考gluon的&lt;a href=&quot;http://zh.gluon.ai/chapter_natural-language-processing/sentiment-analysis.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官方教程&lt;/a&gt;。这里尝试使用PyTorch复现一个。数据用的是IMDB的数据&lt;a href=&quot;http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz&quot; class=&quot;uri&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="PyTorch" scheme="https://samaelchen.github.io/categories/PyTorch/"/>
    
    
  </entry>
  
  <entry>
    <title>word2vec的PyTorch实现</title>
    <link href="https://samaelchen.github.io/word2vec_pytorch/"/>
    <id>https://samaelchen.github.io/word2vec_pytorch/</id>
    <published>2018-08-01T16:00:00.000Z</published>
    <updated>2018-08-15T11:58:12.584Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Word Embedding现在是现在NLP的入门必备，这里简单实现一个CBOW的W2V。</p><p>2018-07-06更新一发用一篇小说来训练模型的脚本。</p><p>2018-08-02更新一发negative sampling版本。</p><a id="more"></a><h1 id="negtive-sampling版本">negtive sampling版本</h1><p><strong>2018-08-02更新基于negative sampling方法的W2V</strong></p><p>翻了之前项亮实现的MXNet版本的NCE，看的不甚理解，感觉他写的那个是NEG的样子，然后还是自己写一个简单的negative sampling来做这个事情。关于NCE和NEG的区别，其实NEG就像是NCE的一个特殊情况，这个可以看<a href="https://arxiv.org/pdf/1410.8251.pdf" target="_blank" rel="noopener">Notes on Noise Contrastive Estimation and Negative Sampling</a>，或者是谷歌的一篇<a href="https://www.tensorflow.org/extras/candidate_sampling.pdf" target="_blank" rel="noopener">总结</a>。</p><p>关于negative sampling这里简单介绍一下，其实负采样的思路非常的简单，就是原来我们有多少个词，那么softmax就要算多少个词的概率，用负采样的方法就是将原来这样的巨量分类问题变成一个简单的二分类问题。也就是说，原来正确的label依然保留，接着只要sample出一小部分的负样本出来，然后做一个二分类问题就可以了。至于需要sample多少负样本，谷歌的C版本中是用了5个，好像哪里见过说不超过25个就可以了，但是现在忘了是哪篇文章了，可能不准确O__O &quot;…</p><p>具体的公式推导其实很简单，可以看一下gluon关于<a href="http://zh.gluon.ai/chapter_natural-language-processing/word2vec.html" target="_blank" rel="noopener">负采样的介绍</a>。</p><p>所以实际上要实现这个负采样非常的容易，只要设计一个抽样分布，然后开始抽样就可以了。在很多词向量的资料里面都说到了，采样分布选用的是： <span class="math display">\[P(w_i) = \frac{f(w_i)^{0.75}}{\sum(f(w_j)^{0.75})}\]</span> 这个其实非常像softmax，就是说用单个词的词频除以全部词频的和，原来的代码中加入了0.75的这个幂指数，完全是炼丹经验。</p><p>然后网上参考了一个开源的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NEGLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ix_to_word, word_freqs, num_negative_samples=<span class="number">5</span>,)</span>:</span></span><br><span class="line">        super(NEGLoss, self).__init__()</span><br><span class="line">        self.num_negative_samples = num_negative_samples</span><br><span class="line">        self.vocab_size = len(word_freqs)</span><br><span class="line">        self.dist = F.normalize(torch.Tensor(</span><br><span class="line">            [word_freqs[ix_to_word[i]] <span class="keyword">for</span> i <span class="keyword">in</span> range(self.vocab_size)]).pow(<span class="number">0.75</span>), dim=<span class="number">0</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, num_samples, positives=[])</span>:</span></span><br><span class="line">        weights = torch.zeros((self.vocab_size, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> positives:</span><br><span class="line">            weights[w] += <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_samples):</span><br><span class="line">            w = torch.multinomial(self.dist, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">while</span>(w <span class="keyword">in</span> positives):</span><br><span class="line">                w = torch.multinomial(self.dist, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">            weights[w] += <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, target)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.nll_loss(input, target,</span><br><span class="line">                          self.sample(self.num_negative_samples,</span><br><span class="line">                                      positives=target.data.numpy()))</span><br></pre></td></tr></table></figure><p>但是有个小问题就是，这里采用的其实是很取巧的一个方法，就是说，我每次会生成一个矩阵告诉pytorch究竟有哪6个sample被我拿到了，然后算negative log likelihood的时候就只算这6个。结果上来说，是实现了负采样，但是从算法效率上来说，其实并没有起到减少计算量的效果。</p><p>所以这里我们实现一个非常简单，类似nagative sampling，但是不是非常严格的采样函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neg_sample</span><span class="params">(num_samples, positives=[])</span>:</span></span><br><span class="line">    freqs_pow = torch.Tensor([freqs[ix_to_word[i]] <span class="keyword">for</span> i <span class="keyword">in</span> range(vocab_size)]).pow(<span class="number">0.75</span>)</span><br><span class="line">    dist = freqs_pow / freqs_pow.sum()</span><br><span class="line">    w = np.random.choice(len(dist), (len(positives), num_samples), p=dist.numpy())</span><br><span class="line">    <span class="keyword">if</span> positives.is_cuda:</span><br><span class="line">        <span class="keyword">return</span> torch.tensor(w).to(device)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.tensor(w)</span><br></pre></td></tr></table></figure><p>然后相应的，我们需要将我们的CBOW也变一下，按照 <span class="math display">\[-\text{log} \frac{1}{1+\text{exp}\left(-u_c^\top (v_{o_1} + \ldots + v_{o_{2m}}) /(2m)\right)}  - \sum_{k=1, w_k \sim \mathbb{P}(w)}^K \text{log} \frac{1}{1+\text{exp}\left((u_{i_k}^\top (v_{o_1} + \ldots + v_{o_{2m}}) /(2m)\right)}.\]</span> 这个公式计算最后的loss。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOW</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, context_size)</span>:</span></span><br><span class="line">        super(CBOW, self).__init__()</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.embeddings.weight.data.uniform_(<span class="number">-0.5</span> / vocab_size, <span class="number">0.5</span> / vocab_size)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, label)</span>:</span></span><br><span class="line">        negs = neg_sample(<span class="number">5</span>, label)</span><br><span class="line">        u_embeds = self.embeddings(label).view(len(label), <span class="number">-1</span>)</span><br><span class="line">        v_embeds_pos = self.embeddings(inputs).mean(dim=<span class="number">1</span>)</span><br><span class="line">        v_embeds_neg = self.embeddings(negs).mean(dim=<span class="number">1</span>)</span><br><span class="line">        loss1 = torch.diag(torch.matmul(u_embeds, v_embeds_pos.transpose(<span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">        loss2 = torch.diag(torch.matmul(u_embeds, v_embeds_neg.transpose(<span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">        loss1 = -torch.log(<span class="number">1</span> / (<span class="number">1</span> + torch.exp(-loss1)))</span><br><span class="line">        loss2 = -torch.log(<span class="number">1</span> / (<span class="number">1</span> + torch.exp(loss2)))</span><br><span class="line">        loss = (loss1.mean() + loss2.mean())</span><br><span class="line">        <span class="keyword">return</span>(loss)</span><br></pre></td></tr></table></figure><p>这里我将embedding层的权重进行了标准化，通过这样的标准化可以避免后面计算loss的时候出现无穷大的情况。然后其他参数不用做什么变化，开始训练看看效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    total_loss = torch.Tensor([<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> context, target <span class="keyword">in</span> tqdm(data_iter):</span><br><span class="line">        context_ids = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(context[<span class="number">0</span>])):</span><br><span class="line">            context_ids.append(make_context_vector([context[j][i] <span class="keyword">for</span> j <span class="keyword">in</span> range(len(context))], word_to_ix))</span><br><span class="line">        context_ids = torch.stack(context_ids)</span><br><span class="line">        context_ids = context_ids.to(device)</span><br><span class="line">        model.zero_grad()</span><br><span class="line">        label = make_context_vector(target, word_to_ix)</span><br><span class="line">        label = label.to(device)</span><br><span class="line">        loss = model(context_ids, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    losses.append(total_loss)</span><br><span class="line">    print(<span class="string">'epoch %d loss %.4f'</span> %(epoch, total_loss))</span><br><span class="line">print(losses)</span><br></pre></td></tr></table></figure><p>完整的notebook可以看<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/langage%20model/w2v_ngs.ipynb" target="_blank" rel="noopener">这个</a>，效率上有质的提升。batchsize还是1024的时候大概压缩到15分钟左右，放到8192的时候大概一个epoch是10分钟。一本满足。</p><hr><h1 id="toy-版本">toy 版本</h1><p>首先import必要的模块： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br></pre></td></tr></table></figure></p><p>CBOW的全称是continuous bag of words。和传统的N-gram相比，CBOW会同时左右各看一部分词。也就是说，根据左右两边的词，猜测中间的词是什么。而传统的N-gram是根据前面的词，猜后面的词是什么。在PyTorch的官网上给出了N-gram的实现。因此我们只需要在这个基础上进行简单的修改就可以得到基于CBOW的W2V模型。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">CONTEXT_SIZE = <span class="number">2</span>  <span class="comment"># 2 words to the left, 2 to the right</span></span><br><span class="line">raw_text = <span class="string">"""We are about to study the idea of a computational process.</span></span><br><span class="line"><span class="string">Computational processes are abstract beings that inhabit computers.</span></span><br><span class="line"><span class="string">As they evolve, processes manipulate other abstract things called data.</span></span><br><span class="line"><span class="string">The evolution of a process is directed by a pattern of rules</span></span><br><span class="line"><span class="string">called a program. People create programs to direct processes. In effect,</span></span><br><span class="line"><span class="string">we conjure the spirits of the computer with our spells."""</span>.split()</span><br><span class="line"></span><br><span class="line"><span class="comment"># By deriving a set from `raw_text`, we deduplicate the array</span></span><br><span class="line">vocab = set(raw_text)</span><br><span class="line">vocab_size = len(vocab)</span><br><span class="line"></span><br><span class="line">word_to_ix = &#123;word: i <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(vocab)&#125;</span><br><span class="line">data = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, len(raw_text) - <span class="number">2</span>):</span><br><span class="line">    context = [raw_text[i - <span class="number">2</span>], raw_text[i - <span class="number">1</span>],</span><br><span class="line">               raw_text[i + <span class="number">1</span>], raw_text[i + <span class="number">2</span>]]</span><br><span class="line">    target = raw_text[i]</span><br><span class="line">    data.append((context, target))</span><br><span class="line">print(data[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure><p>首先定义我们需要的数据。这里的CBOW的Windows是2。因为单词没法直接拿来训练，因此这里我们用id来唯一标识每一个单词。然后我们需要做的一个事情就是将这些id编码成向量。14年谷歌放出来的C那一版我印象中是用的霍夫曼树再降维，现在的PyTorch和gluon都有embedding的类，可以将分类的数据直接编码成向量。所以我们现在用框架实现这个事情就非常简单了。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOW</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, context_size)</span>:</span></span><br><span class="line">        super(CBOW, self).__init__()</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.linear1 = nn.Linear(context_size * embedding_dim, <span class="number">128</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">128</span>, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        embeds = self.embeddings(inputs).view((<span class="number">1</span>, <span class="number">-1</span>))</span><br><span class="line">        out = F.relu(self.linear1(embeds))</span><br><span class="line">        out = self.linear2(out)</span><br><span class="line">        log_probs = F.log_softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span>(log_probs)</span><br></pre></td></tr></table></figure><p>这个CBOW的类很简单，继承了PyTorch的Module类，然后第一步我们就做了一个embedding，然后做了一个隐藏层和一个输出层。最后我们做了一个softmax的动作来得到probability。这就是我们需要训练的神经网络。所以一直说W2V是一个单层的神经网络就是这个原因。</p><p>然后我们定义一个简单的函数，将单词转变成id <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_context_vector</span><span class="params">(context, word_to_ix)</span>:</span></span><br><span class="line">    idxs = [word_to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> context]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(idxs, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">make_context_vector(data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix)</span><br></pre></td></tr></table></figure></p><p>接着定义一些需要的参数： <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">'cuda:0'</span>)</span><br><span class="line">losses = []</span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">model = CBOW(len(vocab), embedding_dim=<span class="number">10</span>, context_size=CONTEXT_SIZE*<span class="number">2</span>)</span><br><span class="line">model.to(device)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure></p><p>这里需要注意一点，context_size需要windows的大小乘2，因为CBOW同时左右都看了这些词，所以我们放进来的词实际上是windows乘2的数量。</p><p>这里我用了GPU来加速计算。如果没有GPU的可以注释掉所有跟device相关的代码，这个数据量不大，体会不到GPU的优势。</p><p>然后就是正式训练 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    total_loss = torch.Tensor([<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> context, target <span class="keyword">in</span> data:</span><br><span class="line">        context_ids = make_context_vector(context, word_to_ix)</span><br><span class="line">        context_ids = context_ids.to(device)</span><br><span class="line">        model.zero_grad()</span><br><span class="line">        log_probs = model(context_ids)</span><br><span class="line">        label = torch.tensor([word_to_ix[target]], dtype=torch.long)</span><br><span class="line">        label = label.to(device)</span><br><span class="line">        loss = loss_function(log_probs, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    losses.append(total_loss)</span><br><span class="line">print(losses)</span><br></pre></td></tr></table></figure></p><p>这样就是一个词向量的训练过程。如果我们需要得到embedding之后的结果，只需要将数据过一遍embedding这一层就可以了。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.embeddings(make_context_vector(data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix))</span><br></pre></td></tr></table></figure><p>可以对比一下训练前和训练后向量的差异。</p><hr><h1 id="softmax低效率版本">softmax低效率版本</h1><p>2018-07-06更新内容：</p><p>之前写的那个是一个非常toy的网络，本质上就是了解一下word2vec是怎么一回事。不过完全不具备实操的能力。下面找了一些开源的语料，稍微修改了一下之前的脚本，还是基于CBOW的模型，这样就可以正常跑日常的数据。语料地址<a href="https://github.com/lxrogers/CS221SAT/tree/master/data/Holmes_Training_Data" class="uri" target="_blank" rel="noopener">https://github.com/lxrogers/CS221SAT/tree/master/data/Holmes_Training_Data</a>。</p><p>先import一些必要的包，这里的tqdm是显示进度的。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data.dataloader <span class="keyword">as</span> dataloader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure></p><p>然后读入语料数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">text = []</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(<span class="string">'Holmes_Training_Data/'</span>):</span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(<span class="string">'Holmes_Training_Data'</span>, file), <span class="string">'r'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        text.extend(f.read().splitlines())</span><br><span class="line"></span><br><span class="line">text = [x.replace(<span class="string">'*'</span>, <span class="string">''</span>) <span class="keyword">for</span> x <span class="keyword">in</span> text]</span><br><span class="line">text = [re.sub(<span class="string">'[^ \fA-Za-z0-9_]'</span>, <span class="string">''</span>, x) <span class="keyword">for</span> x <span class="keyword">in</span> text]</span><br><span class="line">text = [x <span class="keyword">for</span> x <span class="keyword">in</span> text <span class="keyword">if</span> x != <span class="string">''</span>]</span><br><span class="line">print(text[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>这里我ignore了一些文本读入的错误，然后过滤掉了符号。</p><p>因为语料是英文的，所以这里按照空格分割单词，比中文方便太多。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">raw_text = []</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> text:</span><br><span class="line">    raw_text.extend(x.split(<span class="string">' '</span>))</span><br><span class="line">raw_text = [x <span class="keyword">for</span> x <span class="keyword">in</span> raw_text <span class="keyword">if</span> x != <span class="string">''</span>]</span><br></pre></td></tr></table></figure><p>分好词以后就可以开始构建词库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vocab = set(raw_text)</span><br><span class="line">vocab_size = len(vocab)</span><br></pre></td></tr></table></figure><p>接着跟之前一样，构建一个提供训练数据的函数，并准备好训练数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_context_vector</span><span class="params">(context, word_to_ix)</span>:</span></span><br><span class="line">    idxs = [word_to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> context]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(idxs, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">word_to_ix = &#123;word: i <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(vocab)&#125;</span><br><span class="line">data = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, len(raw_text) - <span class="number">2</span>):</span><br><span class="line">    context = [raw_text[i - <span class="number">2</span>], raw_text[i - <span class="number">1</span>],</span><br><span class="line">               raw_text[i + <span class="number">1</span>], raw_text[i + <span class="number">2</span>]]</span><br><span class="line">    target = raw_text[i]</span><br><span class="line">    data.append((context, target))</span><br><span class="line">print(data[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure><p>定义网络，这里要注意的是，因为数据比较大，我们是分batch喂进来的，因此之前forward的时候，我们把embedding的数据摊开的时候是摊成一行的，这里需要摊成每个batch_size的大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOW</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, context_size)</span>:</span></span><br><span class="line">        super(CBOW, self).__init__()</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.linear1 = nn.Linear(context_size * embedding_dim, <span class="number">128</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">128</span>, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        embeds = self.embeddings(inputs).view(len(inputs), <span class="number">-1</span>)</span><br><span class="line">        out = F.relu(self.linear1(embeds))</span><br><span class="line">        out = self.linear2(out)</span><br><span class="line">        log_probs = F.log_softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span>(log_probs)</span><br></pre></td></tr></table></figure><p>定义各种参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">CONTEXT_SIZE = <span class="number">2</span></span><br><span class="line">batch_size = <span class="number">1024</span></span><br><span class="line">device = torch.device(<span class="string">'cuda:0'</span>)</span><br><span class="line">losses = []</span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">model = CBOW(vocab_size, embedding_dim=<span class="number">100</span>,</span><br><span class="line">             context_size=CONTEXT_SIZE*<span class="number">2</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"><span class="comment"># model = torch.nn.DataParallel(model, device_ids=[0, 1]).cuda()</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>我这里本来写了多卡的跑法，但是不知道是不是我写法有问题还是为什么，每次我跑第二块卡的时候，PyTorch都会去第一块卡开一块空间出来，就算我只是在第二块卡跑也会在第一块卡开一些空间。比较神奇，后面再研究一下。</p><p>然后定义一下data iterator。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_iter = torch.utils.data.DataLoader(data, batch_size=batch_size,</span><br><span class="line">                                        shuffle=<span class="keyword">False</span>, num_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>然后这里要注意的是，shuffle参数会影响每次iter的速度，shuffle会慢很多。另外num_workers越多速度越快，但是很可能会内存爆炸，需要自己调一个合适的。</p><p>然后就可以开始训练了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    total_loss = torch.Tensor([<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> context, target <span class="keyword">in</span> tqdm(data_iter):</span><br><span class="line">        context_ids = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(context[<span class="number">0</span>])):</span><br><span class="line">            context_ids.append(make_context_vector([context[j][i] <span class="keyword">for</span> j <span class="keyword">in</span> range(len(context))], word_to_ix))</span><br><span class="line">        context_ids = torch.stack(context_ids)</span><br><span class="line">        context_ids = context_ids.to(device)</span><br><span class="line"><span class="comment">#         context_ids = torch.autograd.Variable(context_ids.cuda())</span></span><br><span class="line">        model.zero_grad()</span><br><span class="line">        log_probs = model(context_ids)</span><br><span class="line">        label = make_context_vector(target, word_to_ix)</span><br><span class="line">        label = label.to(device)</span><br><span class="line"><span class="comment">#         label = torch.autograd.Variable(label.cuda())</span></span><br><span class="line">        loss = loss_function(log_probs, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    losses.append(total_loss)</span><br><span class="line">    print(<span class="string">'epoch %d loss %.4f'</span> %(epoch, total_loss))</span><br><span class="line">print(losses)</span><br></pre></td></tr></table></figure><p>如果要多卡可以把to(device)的代码改成注释的代码就可以了。</p><p>然后就是需要<strong>注意</strong>的点了。</p><p><strong>这个网络的确是work的，训练完可以试一下发现queen-woman+man和king的cosine similarity的确比monkey或者其他的单词要高。但是这个网络的效率很低！很低！很低！（你觉得我会告诉你一个epoch需要跑一个半小时么）。</strong></p><p>原因在哪呢？其实很简单因为我这里使用的是softmax，也就是说，这个网络每一次训练都需要预测所有的词，比如我这个训练集里面有接近37万个词，那么每次就需要预测37万个类，效率之低可想而知。那么有什么解决方案呢？最早的时候，也就是谷歌C版本的解决方案是基于霍夫曼树的hierarchical softmax。后来DeepMind有一篇介绍把NCE（Noise-contrastive estimation）用来加速的论文<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" data-aria-label="http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf">[1]</span></a></sup>。再后来又出现了negative sampling的论文<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top-right hint--error hint--large" data-aria-label="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">[2]</span></a></sup>。不过直观感受上，NCE和negative sampling是很像的，算是殊途同归吧。</p><p>后面过段时间更新对这两种方法的理解和代码。</p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf<a href="#fnref:2" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Word Embedding现在是现在NLP的入门必备，这里简单实现一个CBOW的W2V。&lt;/p&gt;
&lt;p&gt;2018-07-06更新一发用一篇小说来训练模型的脚本。&lt;/p&gt;
&lt;p&gt;2018-08-02更新一发negative sampling版本。&lt;/p&gt;
    
    </summary>
    
      <category term="PyTorch" scheme="https://samaelchen.github.io/categories/PyTorch/"/>
    
    
  </entry>
  
  <entry>
    <title>台大李宏毅深度学习——神经网络一些骚操作</title>
    <link href="https://samaelchen.github.io/deep_learning_step4/"/>
    <id>https://samaelchen.github.io/deep_learning_step4/</id>
    <published>2018-07-22T16:00:00.000Z</published>
    <updated>2018-08-15T11:58:12.584Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>神经网络自2012年躲过世界末日后开始寒武纪大爆发的既视感，从最原始的FC开始出现了各种很风骚的操作。这里介绍三种特殊的结构：spatial transformer layer，highway network &amp; grid LSTM，还有recursive network。</p><a id="more"></a><h1 id="spatial-transformer">Spatial Transformer</h1><p>CNN是这一次深度学习大爆发的导火线，但是CNN有非常明显的缺陷。如果一个图像里的元素发生了旋转、位移、缩放等等变换，那么CNN的filter就不认识同样的元素了。也就是说，对于CNN而言，如果现在训练数字识别，喂进去的数据全是规整的，那么倾斜的数字可能就不认识了。</p><p>其实从某种意义上来说，这个就是过拟合了，每个filter能做的事情是非常固定的。不过换个角度来看，是不是也能理解为数据过分干净了？</p><p>那么为了解决这样的问题，其实有很多解决方案，比如说增加样本量是最简单粗暴的方法，通过image augment就可以得到海量的训练数据。另外一般CNN里面的pooling层也是解决这个问题的，不过受限于pooling filter的size大小，一般来说很难做到全图级别的transform。另外一种做法就是spatial transformer。实际上，spatial transformer layer我感觉上就是嵌入网络的image augment，或者说是有导向性的image augment。这样的做法可以减少无脑augment带来太多的噪音。个人理解不一定对。这种方法是DeepMind提出的，论文就是<a href="https://arxiv.org/pdf/1506.02025.pdf" target="_blank" rel="noopener">Spatial Transformer Networks</a>。</p><p>首先看一下如果要对图像进行transform的操作，我们应该怎么做？对于一个图像里的像素而言有两个下标<span class="math inline">\((x,y)\)</span>来表示位置，那么我们就可以将这个看作是一个向量。这样以来，我们只需要通过一个二阶方阵就可以操作图像的缩放和旋转，然后加上一个二维向量就可以控制图片的平移。也就是说 <span class="math display">\[\begin{bmatrix}x&#39; \\ y&#39; \end{bmatrix} = \begin{bmatrix}a &amp; b \\ c &amp; d \end{bmatrix} \begin{bmatrix}x \\ y \end{bmatrix} + \begin{bmatrix}e \\ f \end{bmatrix}\]</span></p><p>当然，简洁一点可以写成： <span class="math display">\[\begin{bmatrix}x&#39; \\ y&#39; \end{bmatrix} = \begin{bmatrix}a &amp; b &amp; c \\ d &amp; e &amp; f \end{bmatrix} \begin{bmatrix}x \\ y \\ 1 \end{bmatrix}\]</span> 两个公式的元素没有严格对应，不过意思一样。</p><p>但是这里需要注意的事情是，比如我们原来输入的图片是<span class="math inline">\(3 \times 3\)</span>的，我们输出的还是一个<span class="math inline">\(3 \times 3\)</span>的图片，而位置变换后超出下标的部分我们就直接丢弃掉，而原来有值，现在没有值的部分就填0。示意图如下：</p><p><img src="https://i.imgur.com/CSnxAU6.png"></p><p>然后有一点我一直没理解的点就是，在论文里面上一个等式的左边是source layer，右边的是target layer。直观上从forward的方向上看，数据从上一层到下一层，那么变化就应该是第一层经过变化后变到第二层。</p><p>论文里面没有太解释为什么会是这样的操作，看了一些别人的博客，大部分人也说得不清不楚的。个人的感觉吧，为了这么做是为了保证输出的feature map的维度能够保持不变，论文里面有一个示例图：</p><p align="center"><img src="https://i.imgur.com/XIBMatZ.png" width="70%"></p><p>从图上面看的话，target和source都是保持不变的，唯一变换的是source上的sampling grid（感觉这么说也不太对，sampling grid的像素点数量其实也没变，就是位置或者说形状变了）。而这个sampling grid就是将target的网格坐标通过上面的公式做仿射变换得到的。那如果反过来，也就是说我们直接用source做放射变换的话，很可能得到的target是不规整的。所以应该说spatial transformer layer做的事情是学习我们正常理解的仿射变换的逆矩阵。比较神奇的是这个用bp居然可以自己学出来。</p><p>那么这里就会有个问题，因为sampling grid的像素点其实是没有变过的，所以这就意味着说仿射变换的结果很可能得到是小数的index。比如说<span class="math inline">\(\begin{bmatrix}1.6 \\ 2.4 \end{bmatrix} = \begin{bmatrix}0 &amp; 0.5 \\ 1 &amp; 0 \end{bmatrix} \begin{bmatrix}2 \\ 2 \end{bmatrix} + \begin{bmatrix}0.6 \\ 0.4 \end{bmatrix}\)</span>，那么这个时候要怎么办呢？如果我们按照就近原则的话，那么这个位置又会被定位到原图的<span class="math inline">\([2, 2]\)</span>这个位置，那么梯度就会变成0。所以这样是不行的，那么为了可以进行bp，论文里面采用了双线性插值的方法。也就是说，用离这个位置最近的四个顶点的像素，按照距离的比例作为权重，然后加权平均来填补这个位置的像素。</p><p>这个算法大概原理如下：</p><p align="center"><img src="https://i.imgur.com/b7IprgN.png" width="50%"></p><p>我们现在想要求中间绿色点的像素，那么我们先算出<span class="math inline">\(R_1\)</span>和<span class="math inline">\(R_2\)</span>的像素： <span class="math display">\[R_1 = \frac{x_2 - x}{x_2 - x_1}Q_{11} + \frac{x - x_1}{x_2 - x_1}Q_{21} \\R_2 = \frac{x_2 - x}{x_2 - x_1}Q_{12} + \frac{x - x_1}{x_2 - x_1}Q_{22}\]</span> 然后计算<span class="math inline">\(P\)</span>的像素： <span class="math display">\[\boxed{P = \frac{y_2 - y}{y_2 - y_1}R_1 + \frac{y - y_1}{y_2 - y_1}R_2}\]</span></p><p>那么在DeepMind的试验里面，在卷基层里面加入了ST层之后，收敛以后target得到的输出大体上都是不变的。就像下图：</p><p align="center"><img src="https://i.imgur.com/x0Za3Tx.gif"></p><p>另外就是这个变换矩阵，如果我们强行让这个矩阵长成<span class="math inline">\(\begin{bmatrix}1 &amp; 0 &amp; a \\ 0 &amp; 1 &amp; b \end{bmatrix}\)</span>，那么就会变成attention模式，网络自己会去原图上面扫描，这样就会知道模型在训练的时候关注图片的哪个位置。看起来就像下图：</p><p align="center"><img src="https://i.imgur.com/IDeic8W.png" width="70%"></p><p>上面那一排的网络有两个ST layer，大体上可以看出来，红色的框都是在鸟头的位置，绿色的框都是在鸟身的位置。</p><h1 id="highway-network-grid-lstm">Highway network &amp; Grid LSTM</h1><p>Highway network最早是<a href="https://arxiv.org/pdf/1505.00387.pdf" target="_blank" rel="noopener">Highway Networks</a>和<a href="https://arxiv.org/pdf/1507.06228.pdf" target="_blank" rel="noopener">Training Very Deep Networks</a>这两篇论文提出的。Highway network实际上受到了LSTM的启发，从结构上来看，深层的前馈网络其实和LSTM非常的像，如下图：</p><p align="center"><img src="https://i.imgur.com/L4cqtdk.png" width="80%"></p><p>所以二者的差别就在于，在前馈中只有一个input，而LSTM中每一层都要把这一个时刻的x也作为输入。所以很自然的一个想法，在LSTM中有一个forget gate决定要记住以前多久的信息，那么在前馈网络中也可以引入一个gate来决定有哪些之前的信息干脆就不要了，又或者有哪些以前的信息直接在后面拿来用。那最简单LSTM变种是GRU，所以highway network借鉴了GRU的方法，把reset gate拿掉，再把每个阶段的x拿掉。</p><p>所以将GRU简化一下再竖起来，我们就可以得到highway network：</p><p align="center"><img src="https://i.imgur.com/SsDSDuy.png" width="70%"></p><p>那么模仿GRU的计算方法，我们计算<span class="math inline">\(h&#39; = \sigma(Wa^{t-1})\)</span>，<span class="math inline">\(z = \sigma(W^z a^{t-1})\)</span>，所以<span class="math inline">\(a^t = z \odot a^{t-1} + (1-z) \odot h&#39;\)</span>。</p><p>而后面微软的<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">ResNet</a>其实就是一个highway network的特别版本：</p><p align="center"><img src="https://i.imgur.com/hDTBRrE.png" width="60%"></p><p>当然感觉也可以将ResNet看做是竖起来的LSTM。那ResNet里面的变换可以是很多层的，所以在现在的实现中，很常见的一个情况是将这个东西叫做一个residual block。</p><p>所以利用highway network有一个非常明显的好处就是可以避免前馈网络太深的时候会导致梯度消失的问题。另外有一个好处就是通过highway network可以让网络自己去学习到底哪个layer是有用的。</p><p>那既然可以将深度的记忆传递下去，那么这样的操作也可以用到LSTM里面，也就是grid LSTM。一般的LSTM是通过forget gate将时间方向上的信息传递下去的，但是并没有将layer之间的信息传递下去。因此grid LSTM就是加一个参数纵向传递，从而将layer的信息传递下去，直观上来说，就是在<span class="math inline">\(y\)</span>后面再拼一个vector，然后这个vector的作用跟<span class="math inline">\(c\)</span>一样。具体的可以看一下DeepMind的这篇论文，<a href="https://arxiv.org/pdf/1507.01526v1.pdf" target="_blank" rel="noopener">Grid LSTM</a>。粗略来说，结构上像这样：</p><p align="center"><img src="https://i.imgur.com/BUWr2kn.png" width="60%"></p><p>那有2D的grid LSTM很自然就会有3D的grid LSTM，套路都是差不多的。不过我还没想到的是，3D的grid LSTM要用在什么场景当中，多个output？！</p><h1 id="recursive-structure">Recursive Structure</h1><p>遥想当年刚接触RNN的时候根本分不清recursive network和recurrent network，一个是递归神经网络，一个是循环神经网络，傻傻分不清。但是实际上，recurrent network可以看作是recursive network的特殊结构。Recursive network本身是需要事先定义好结构的，比如：</p><p align="center"><img src="https://i.imgur.com/SgEEBbw.png" width="60%"></p><p>那常见的recurrent network其实也可以看做是这样的一个树结构的recursive network。Recursive network感觉上好像也没什么特别有意思的东西，比较有趣的就是这边<span class="math inline">\(f\)</span>的设计。比如说现在想要让机器学会做句子的情感分析，那么很简单的一个想法就是把每一个词embedding，然后放到网络里面训练，那么我们可以用这样的一个结构：</p><p align="center"><img src="https://i.imgur.com/YFhqVos.png" width="60%"></p><p>因为在自然语言里面会有一些类似否定之否定的语法，所以我们希望说very出现的时候是加强语气，但是not出现的时候就是否定之前的。如果用数学的语言来表达，这不就是乘以一个系数嘛。所以在这样的情况下，如果我们只是简单的加减操作，那么就没有办法实现这种“乘法”操作。所以这个时候，我们的<span class="math inline">\(f\)</span>设计就会有点技巧：</p><p align="center"><img src="https://i.imgur.com/E4Gt3UC.png" width="60%"></p><p>那么看一下这个设计。如果我们直接采用最传统的做法，就是将<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>直接concat起来，然后乘以一个矩阵<span class="math inline">\(W\)</span>，再经过一个激活函数变换，这样的操作其实只能做到线性的关系，个人感觉，实际上这样的设计会将<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>的一些交互特性变成隐藏特征保存在<span class="math inline">\(W\)</span>当中，但是一旦输入变化了，这些隐藏的特征却不能被传递出来，所以效果不好。</p><p>因此下面的一种设计就比较骚气，后面还是传统的做法，但是前面加上了一个vector。这个vector的元素就是来学习这些词之间的interaction，然后将这些interaction变成bias，因为recursive network的function都是不变的，因此这些bias就这样被传递下去了。那么这里有一个需要注意的就是，我们这里有几个词，那我们就需要多少个bias，而且每个bias中间的这个矩阵<span class="math inline">\(W\)</span>都是不一样的。</p><p>这是三个比较骚气的网络结构变换，感觉看了这么多，好多网络之间都是殊途同归啊，会不会最后有一个非常general的网络结构出现，使得现在的每一种网络都是其一种特殊情况呢？</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;神经网络自2012年躲过世界末日后开始寒武纪大爆发的既视感，从最原始的FC开始出现了各种很风骚的操作。这里介绍三种特殊的结构：spatial transformer layer，highway network &amp;amp; grid LSTM，还有recursive network。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://samaelchen.github.io/categories/DeepLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>经济思想史读书笔记——亚当·斯密</title>
    <link href="https://samaelchen.github.io/economics_thoughts_notes4/"/>
    <id>https://samaelchen.github.io/economics_thoughts_notes4/</id>
    <published>2018-07-17T16:00:00.000Z</published>
    <updated>2018-08-15T11:58:12.584Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>“亚当·斯密在政治经济学的历史中占据如此中心的位置，以至于这个谨慎的水手不愿在这样广阔的海洋上开始着手航行。”——亚历山大·格雷</p><a id="more"></a><p>亚当·斯密是经济思想史上一个非常重要的人物，他的著作是经济思想发展中的一道分水岭。</p><h1 id="博学多才的经济学家">博学多才的经济学家</h1><p>斯密的思想涉及到很多领域，包括经济、政治还有哲学。在斯密所处的时代，从事研究的知识精英被要求掌握最宽广的人类知识，而不是仅仅专攻某一个领域的知识。这种多学科方法的一个后果是，像亚当·斯密探求如今称为社会科学的人认为，牛顿在物理学中建立的科学严密性，他们也可以做到。</p><p>斯密常被称为经济学之父，尽管很多先贤看到了经济学很多方面，但是没有人能够将决定国民财富的力量、培育经济增长与发展的适宜的政策以及通过市场力量有效协调大量经济决策的方式，整合成一个全面的观点。</p><p>斯密对经济学范围的认识继承了英国重商主义的观点。他对解释国民财富的性质与原因感兴趣。但是斯密考察的范围比现代经济学家研究的要宽泛，他用政治的、社会的、历史的材料来填充其经济模型。</p><p>虽然斯密在经济学史上的地位非常重要，但是斯密的理论模型缺少优雅与严密。</p><h1 id="斯密的市场分析与政策结论">斯密的市场分析与政策结论</h1><p>斯密在经济思想史的重要性基于（1）他对经济体相互关联性的广泛了解和（2）他对经济政策的影响。斯密作为一位经济学家的强大实力在于，他洞察了：（1）经济体组成部分的相互依赖；（2）用来推动已过财富的政策。他不仅是一个经济学家，而且是一位指出了经济发展与富足方法的哲学家。</p><h2 id="前后关联的经济政策">前后关联的经济政策</h2><p>斯密的方法论塑造了他对经济体的分析，以及他关注政府政策的决心。斯密对自由放任的主张部分依赖于市场如何产生某些结果的理论模型，还来自于他对现有历史与制度环境的观察。所以，斯密的经济政策是前后关联的。斯密对自由市场的拥护并不是因为他认为市场是完美的，而是因为他所处的时代，英国的历史与制度结构导致，市场通常比政府干预产生更好的结果。</p><p>在之前讨论过，经济学科学处理经济变量之间实证的、事务性的关系，即探讨“是什么”。规范经济学涉及的是“应当怎样”的问题。经济学艺术是以政策为导向的。亚当·斯密是一个超群的经济学艺术大师。因而，前后关联的经济政策，就是另一种表达经济学艺术观点的方式。</p><h2 id="自然秩序和谐和自由放任">自然秩序、和谐和自由放任</h2><p>斯密的经济学与重商主义有很多相同的基本因素。他们认为通过科学的调查能够揭示事务性的因果关系。斯密也像重商主义一样，提出人类本性的基本假设：人类是理性的，是有私心的，很大程度上受经济利己主义的驱使。也就是古典经济学里面的一个基本假设——理性人假设。</p><p>但是斯密体系和重商主义有一个很大的不同点在于，他假设竞争性市场在极大程度上是存在的，在这些市场内部，生产要素自由流动，从而提升了它们的经济优势。第二个区别是，经济体的自然运作，能够比人类做出的任何安排更有效地解决冲突。这也是斯密体系里面一个非常重要的假设基础，就是存在完全竞争市场。实际上，斯密的自由放任政策只有在完全竞争市场下才能成立。</p><p>斯密的推论过程非常简单。人类是理性的，是有私心的，受利己主义驱使。如果放任不管，每个个体都会追求自己的私利，在促进私利的同时也促进了社会利益。政府不应当干预这一过程，而应当遵循自由放任的政策。在斯密体系内，私利与公共利益是和谐的。也就是说，在斯密看来，在没有政府干预的竞争性市场上会出现资源的最佳配置。</p><h2 id="竞争性市场的运作">竞争性市场的运作</h2><p>斯密对经济理论最重要的贡献是他对竞争性市场运作的分析。他能比以前的经济学家更详细说明，源于竞争的价格在长期中为什么等于生产成本这一道理。他对价格形成与资源配置的分析中，他将短期价格称为“市场价格”， 将长期价格称为“自然价格”。他认为，竞争从根本上要求有大量的销售者；数值经济体中利润、工资、租金的一群资源拥有者；资源在行业间自由流动。给定这些条件，资源拥有者的私利将会形成长期自然价格，该价格将经济体不同部门之间的利润率、工资、租金均等化。</p><p>确立了竞争性市场的优越性后，斯密毫无困难地构建起他反对垄断与政府干预的论据。但是斯密的自由放任是基于竞争市场存在的假设的。</p><p>斯密认为，尽管重商主义者关于政府干预的很多主张都声称是促进了社会利益，但是其实是增进了个人私利，这种管制不是有利于国家，而是利于商人。但是斯密也并不主张完全的自由放任，他认为在他所处时代历史的、政治的、制度的背景下，他认为，保护幼稚产业的关税是有必要的；国防的贸易管制也是有必要的；政府还应该提供具有极大社会收益但是私人市场因为没有充足利润而不去提供的产品，以此来限定他对自由放任的主张。</p><h2 id="资本与资本家">资本与资本家</h2><p>斯密提出了关于资本在财富生产过程中和在经济发展中的作用的一些重要概念。首先，他指出，一国的现有财富取决于资本积累，原因在于资本积累决定了劳动分工和参加生产性劳动的人口比例。其次，斯密断定资本积累也会导致经济发展。再次，与资本积累相结合的个人私利导致资本在各产业之间的最佳配置。</p><p>在斯密体系中，资本家在经济体运行中扮演主要的角色。资本家对财富与利润的追逐，引导经济体实现资源的有效配置和经济增长。在私有财产经济体中，资本的来源是个人储蓄。斯密认为，劳动并不能够积累资本，原因在于工资水平仅仅能够满足直接的消费欲望。</p><p>斯密断定，恰恰是一部分正在星期的产业接济是对社会有益的人，他们为了利润而奋斗，努力积累资本，通过储蓄和投资来增加他们的财富。因此有利于资本家的收入不平等分配具有巨大的社会重要性。没有收入的不平等分配，就不可能有经济增长，因为全部的年产出都会被消费掉。</p><h2 id="斯密对政策的影响">斯密对政策的影响</h2><p>斯密的重要贡献在于他对市场经济在多种用途之间配置稀缺资源方式的广泛看法。他的主要政策结论是政府应当接受自由放任的政策。</p><h1 id="国民财富的性质与原因">国民财富的性质与原因</h1><p>斯密不认同重商主义的一国的贵金属就等同于一国财富的观点。他认为，财富是产品与服务的年流量，而不是贵金属的累计储备量。他也解释了出口与进口之间的关系，认识到出口的基本作用是支付进口。此外他暗示，经济活动的最终目的是消费。这将斯密的经济学与重商主义的经济学加以区分，后者将生产视为目的。而在国家财富源泉的认识上，斯密与重农主义也不一样，后者强调的是消费。</p><p>斯密继而建议，国家的财富应当按照人均指标来衡量，这就是现在经常被提及的人均GDP。</p><h2 id="国民财富的原因">国民财富的原因</h2><p>斯密主张，一个国家的财富，也就是国家的收入取决于：（1）劳动生产力；（2）有效使用或者生产性使用的劳动者的比例。因为在他的完全竞争市场下，经济体将自动实现资源的充分利用，因此他只需要考察那些决定一国产品与服务生产能力的因素。</p><h3 id="劳动生产力">劳动生产力</h3><p>斯密认为，劳动生产力取决于劳动分工。专业化与劳动分工提高了劳动生产力。尽管斯密认为专业化与劳动分工的经济利益，但是他也察觉到一些严重的社会成本。劳动分工的一个社会缺点是公认被赋予重复性的任务，这些任务很快变得单调乏味。这一点其实与管理学里面的科学管理法非常相似，另外可以看一下霍桑试验。但是我们不得不承认，劳动分工增加了人类福利。</p><p>劳动分工一次取决于斯密所谓的市场的范围与资本的积累。市场越大，可销售的数量越多，劳动分工的机会就越多。另一方面，有限的市场仅允许有限的劳动分工。劳动分工收到资本积累的限制，原因在于生产过程是耗时的：在生产开始与成品的最终销售之间存在时间间隔。</p><p>在一个简单的经济体中，劳动分工是微小的，只需要很少的资本来维持劳动力。但是随着劳动分工的增加，劳动者不再为其自身的消费生产产品，在耗时的生产过程期间，必须保持一定的消费品储备来维持劳动者。这一定数量的产品来自储蓄，也就是斯密所说的资本。资本家的一个主要功能是缩短生产开始与最终产品销售之间的时间间隔而提供手段。因此可以使用劳动分工的生产过程的范围，收到可以利用的资本积累数量的限制。换而言之，斯密认为，劳动分工随着越来越多的储蓄而越来越细分。如果将这一个观点放到小一点的经济体上，比如公司，我们就会很直观理解，创业公司，一个人当好几个人用，当公司庞大到商业帝国的程度，几个人当一个人用。</p><h3 id="生产性与非生产性劳动">生产性与非生产性劳动</h3><p>按照斯密的观点，资本积累也决定了生产性：劳动者与非生产性劳动者的比例。他主张，生产可销售商品所使用的劳动是生产性劳动，而生产服务所使用的劳动则是非生产性劳动。斯密的观点其实非常的朴素，资本应当用于再生产。他的观点，如果一个做法对个体是正确的，那么对国家也是正确的，因此，国家的资本越多，就更应该用于支持生产性劳动。在斯密的时代，他并没有意识到第三产业的作用。</p><p>斯密强调，将大量收入分配给进行储蓄和投资的资本家，将少量收入分配给地主，可以获得最高的经济增长率。某种程度上，是不是也可以看作是需要大力扶持实业。此外，因为经济增长收到政府非生产性劳动支出的约束，例如军队，所以拥有较小的政府，就可以对资本家征收较少的税，以便他们可以积累更多的资本。这里有一点点小国寡民的意味。</p><h2 id="对国民财富的总结">对国民财富的总结</h2><p>其实在斯密的观点中，这一点一句话就可以概括，资本是国家财富的主要决定因素。</p><p>对斯密而言，资本积累毫无疑问要求一个自由市场与私人财产的制度框架。在自由市场体制中，既定的投资支出水平，在没有政府指导的运转中被加以分配，以确保最高的经济增长率。在私人财产体制中，对高资本积累率的进一步要求就是不平等的收入分配。</p><h1 id="国际贸易">国际贸易</h1><p>在斯密的观点中，只有错误地认为一个国家的财富取决于它所持有的贵金属和借据，贸易顺差才是有利的。而斯密的主张是非规制的对外贸易，理由是如果英国能够以低于法国的成本生产一种产品，例如羊毛，并且法国可以以低于英国的成本生产另一种产品，例如葡萄酒，那么两国各自用生产成本较低的产品，去交换生产成本较高的产品，这种交换对双方来说都是有利的。也就是现在经济学中的对外贸易的绝对成本学说。实际上这种学说不局限于国际贸易，还适用于一国的内部贸易。</p><p>在现代经济学的观点中，随着劳动越来越专业化，存在收益递增（成本递减）的情况。斯密的对外贸易优势的部分观点，明显基于收益递增这一动态概念。但是这种观点是有非常明显的缺陷的，事实上，任何一国国家都不可能长期保持一种生产方式不变。</p><p>在这个方面，古典经济学和重商主义者有一个重大的区别。重商主义者认为国际贸易是一种零和博弈，而古典经济学认为不是。但是斯密只是认识到贸易对各国都有利，而没有认识到贸易过程中的价格机制。这一点在之后的李嘉图等人的理论中得到解释。</p><h1 id="价值理论">价值理论</h1><p>区分价值与价格困惑了早期经济学家。这主要集中于三个问题：（1）什么决定了产品的价格？也就是什么决定了相对价格？（2）什么决定了价格总水平？（3）什么是福利的最佳度量标准。实际上，斯密也没有非常明确给出答案。</p><h2 id="相对价格">相对价格</h2><p>按照现代的经济学术语，相对价格指的是商品间的价格比例关系。实际上相对价格是由李嘉图提出来的。另外一个概念是绝对价格，绝对价格其实就是用货币单位表示的价格水平。</p><p>斯密认为，市场价格或者短期价格是由供需双方决定的。自然价格或者长期均衡价格通常取决与生产成本。在现代经济学里面，我们认为自然价格是达到供需平衡时候的价格。</p><p>斯密对他所处时代经济体中相对价格形成的分析，区分为两个时间段和经济体的两个宽泛的部门，分别是短期与长期、农业与制造业。在短期或者市场阶段，斯密在制造业与农业中都发现了乡下倾斜的需求曲线与向上倾斜的供给曲线；因此市场价格取决于需求与供给。斯密对长期中发生的更为复杂的“自然价格”的分析，包含着一些矛盾。对于农业部门而言，自然价格取决于供给与需求，原因在于长期供给曲线向上倾斜，表明成本递增。但是对制造业部门来说，长期供给曲线有时假定为完全富有弹性（水平的），表明成本不变；在分析的另一些地方又向下倾斜，表明成本递减。在制造业中，当长期供给曲线完全富有弹性时，价格就完全取决于生产成本；但是，当长期供给曲线向下倾斜时，自然价格就取决于需求与供给双方。不过，无视长期供给曲线在制造业中的形状，主要强调生产成本对自然价格的决定，这是斯密以及后来的古典经济学家的特点。</p><p>经院哲学对相对价格问题感兴趣，因为他们关注交换过程中的道德问题；重商主义者则是认为财富在交换过程中产生。斯密认为，一旦经济体实行专业化和劳动分工，交换就变成必需。如果交换发生在斯密时代的市场中，就会出现一些显而易见的问题。第一，如果交换处于高于物物交换水平的状况下，就会存在交换媒介的问题。第二，存在价值或者相对价格的问题。</p><h2 id="价值的含义">价值的含义</h2><p>斯密认为价值一词有两种不同的含义，它有时表示一些特定物品的效用，有时又表示因占有物品而取得的对其他产品的购买力。前者可以称为“使用价值”，后者被称为“交换价值”。使用价值很大的东西，其交换价值往往很小，甚至没有；相反，交换价值很大的东西，其使用价值往往极小。就像水的使用价值非常高，但是交换价值很低；钻石的交换价值很高，但是使用价值非常低。</p><p>这里的交换价值是指一种商品购买其他产品的能力。这是市场所表达的一种客观度量。他关于使用价值的概念是含糊的。一方面，使用价值有道德内涵，这是对经院哲学的回归。另一方面，使用价值是一件商品满足需要的能力，是因持有或消费一件产品而获得的效用。当一件商品被消费时，可以获得几种效用：（1）它的总效用，（2）它的平均效用，（3）它的边际效用。斯密的关注点是总效用，这就模糊了他对需求如何在价格决定中发挥作用的理解。显然，水的总效用超过了钻石的总效用。然而，因为商品的边际效用经常随着其消费得更多而递减，所以水的边际效用比钻石的边际效用低。</p><p>我们愿意为一件商品所支付的价格——我们对获得又一单位商品所寄予的价值——不仅取决于商品的总效用，而且取决于其边际效用。因为斯密没有意识到这一点，因此他既不能为“钻石-水悖论”找到满意的解决办法，也不能了解其使用价值与交换价值之间的关系。某种程度上是不是认为这是供给量上的区别？大多数情况下水比钻石多，所以边际效用就低。</p><h2 id="斯密关于相对价格">斯密关于相对价格</h2><p>因为斯密对相对价格的决定因素有些困惑，所以，他发展了与这些因素相关的三个独立的理论：（1）劳动成本价值理论，（2）劳动支配价值理论，（3）生产成本价值理论。</p><p>他假设了经济体两种截然不同的状态：初期野蛮状态或者原始社会，它被界定为这样的一个经济体，其中资本还没有被积累起来，土地未被使用；发达经济体，其中资本与土地不再是资源充足的产品（它们具有超过零以上的价格）。</p><h2 id="原始社会中的劳动成本理论">原始社会中的劳动成本理论</h2><p>如果在一个狩猎国家，杀死一头海狸所需的劳动通常两倍于杀死一头野鹿所需要的劳动，那么，一头海狸自然就可以换来或者值两头野鹿。按斯密的劳动成本理论，在还不存在土地与资本的经济体中，或者土地与资本还是无限充足的自然资源的经济体中，一件产品的交换价值或者价格，由生产产品所需的劳动量决定。</p><p>这让我们认识到劳动成本价值理论的第一个难点。我们应当如何度量生产一件商品所需要的劳动量。斯密认识到生产一件产品所需的劳动量不能简单地用时钟表示的时间数量来度量，原因在于，除了时间之外，也必须考虑有关的精巧或者技能，以及任务的艰难与困苦。</p><p>在这一点上，斯密遇到了所有的劳动成本价值理论都遇到的，扔未被后来的经济学家成功地予以解决的一个难题。如果劳动量是一个以上变量的函数，那么，我们必须找到一种方法来说明所有变量的相对重要性。斯密主张时间、艰难程度以及精巧程度上的差异都反映在支付给劳动者的工资中。</p><p>但是斯密的观点只是重申了问题，他是在表明一件产品是依照支付给劳动者的工资，而不是依照包含在产品中的劳动量拥有价值。这是一个循环推论。斯密利用一套价格解释另一套价格。</p><h3 id="原始社会中的劳动支配">原始社会中的劳动支配</h3><p>按照斯密的观点，一件产品的价值“对于那些拥有产品的人以及那些想用它去交换一些新产品的人来说，正好等于产品能够使他们购买或支配的劳动量”。也就是说，如果捕获一头海狸或者两头野鹿需要两小时，那么两头野鹿就等于一头海狸。</p><h3 id="发达经济体中的劳动理论">发达经济体中的劳动理论</h3><p>因为资本已经被积累，土地也已经被利用，并且一件产品的最终价格也必须包括当做利润的资本家的收益以及当做地租的地主的收益。最终价格形成了由工资、利润、地租这些要素报酬构成的收入。</p><h3 id="相对价格的生产成本理论">相对价格的生产成本理论</h3><p>斯密最终放弃了任何劳动价值理论都适用于他所处时代一样发达的经济体。斯密似乎发现，一旦资本被积累起来，土地被加以利用，并且一旦必须支付利润、地租，还有劳动，能唯一适当解释价格的就是生产成本理论。</p><p>在成本理论中，一件商品的价值取决于对所有生产要素的支付：除了劳动之外还有土地和资本。在斯密的体系中，利润这一术语既包括今天的利润，也包括利息。在斯密假设平均成本不随着产量的增加而增加的地方，无论使用总成本还是使用平均成本，通过加总工资、利润和地租，这样的相对价格都是不变的。在平均成本随着产量变化的地方，价格就取决于需求与供给双方。然而，在分析长期自然价格的决定时，即使当供给曲线不被假定为完全富有弹性时，斯密也强调供给与生产成本。斯密主张，竞争占优势的地方，商人、劳动者、地主的私利将导致与生产成本相等的自然价格。</p><h1 id="分配理论">分配理论</h1><p>收入的个人分配取决于个人所出售的生产要素的价格与数量。劳动是大部分家庭拥有的唯一生产要素，因此家庭的收入一般取决于工资率与工作时间的长度。拥有财产的那些家庭所获得的财产收入量，取决于家庭所拥有的资本与土地的数量以及这些要素的价格。因为工资、利润、地租都是经济体中的价格，所以它们的相对价格——连同个人出售的劳动、资本、土地数量一起——决定了收入的分配。</p><h2 id="工资">工资</h2><p>斯密认为，在对工资的讨价还价过程中，劳动处于劣势。因为劳动市场是买方市场，雇主少，可以容易联合起来巩固他们的地位。即使罢工，雇主也有足够资源维持他们的生活，但是没有工作，工人生存困难。在这一部分，斯密削弱了市场力量的有益运作过程，并似乎已经意识到其完全竞争市场的假设受到了限制。</p><h2 id="工资基金">工资基金</h2><p>因为生产过程是耗时的，所以从生产过程开始到最终销售，需要一部分的产品库存或资本来维持劳动者的衣食住行，这部分被称为工资基金，来源于资本家的储蓄或者消费中断。给定劳动力和工资基金的规模，工资率=工资基金/劳动力。</p><h2 id="利润">利润</h2><p>斯密很自然接受了利润是因资本家执行了对社会有用的功能而对他的一种支付，这种功能就是在耗时的生产过程期间，为劳动者提供生活必需品，提供工作所用的原料和机器。按照斯密的观点，劳动者允许从其产量中进行利润的扣除，原因在于，劳动者并不拥有工作所用的原料和独立的支持手段。于是利润分为两部分：纯利息收入和风险收入。在原始经济体中，劳动者获得了全部的产品，但是在发达经济体中，劳动者却需要被扣除利润和地租，这一点斯密并没有做出解释。在深信资本主义制度基本和谐的理论家里，这一点是非常自然无需质疑的。</p><h2 id="地租">地租</h2><p>斯密提出了四种地租理论：（1）地主的需求；（2）垄断；（3）差异化的优势；（4）大自然的施舍。在《国富论》前面的部分，地租被视为决定价格的因素，而后面则视为价格被决定的因素。</p><h2 id="随时间变化的利润率">随时间变化的利润率</h2><p>斯密认为一个国家的经济增长取决于资本积累。他预测，随着时间的推移，利润率会下降，原因有三。（1）劳动市场的竞争。资本家的竞争导致工资上升利润下降。（2）商品市场的竞争。生产者竞争家具，商品价格下降，利润减少。（3）投资市场的竞争，因为投资机会有限，所以资本的积累增加会导致利润下降。</p><h1 id="福利与价格总水平">福利与价格总水平</h1><p>斯密努力去发现：第一，决定价格总水平的因素；第二，不同时期福利变化的最佳度量。其实这个问题还是比较复杂的，对于一个生产两种或者更多产品的经济体来说，有没有可能界定和度量其福利的变化。</p><p>如果用总消费或者产量来界定福利，那么需要解决的问题就是，寻找一种度量总消费或者总产品数量的方式。通常而言，这种度量方式就是国家的货币单位。现在的社会，我们通常会用GDP来衡量。</p><p>但是这里会有另外一个问题，那就是货币的本质是一般等价物。这就意味着，货币也是有价格的，它本身也会变化。所以斯密转向劳动，却发现劳动价格也会变化。最后，他采用劳动的复效用作为衡量标准。也就是说，如果我们能够使用较少的劳动生产相同的产量，那么我们就拥有更多的线下，经济状况就会更好。</p><p>事实上这种度量方式比斯密想象的要复杂的多，现在的经济学家在这一方面有了更长远的度量方式，例如度量“生活质量”。</p><p>斯密在经济学史上如此之重要，他对自由放任体系的推崇引导了西方市场经济数百年的发展。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;“亚当·斯密在政治经济学的历史中占据如此中心的位置，以至于这个谨慎的水手不愿在这样广阔的海洋上开始着手航行。”——亚历山大·格雷&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="https://samaelchen.github.io/categories/ReadingNotes/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数 13</title>
    <link href="https://samaelchen.github.io/linear_algebra_step13/"/>
    <id>https://samaelchen.github.io/linear_algebra_step13/</id>
    <published>2018-07-16T16:00:00.000Z</published>
    <updated>2018-08-15T11:58:12.584Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>两个线性代数里面在机器学习领域大放异彩的算法，SVD和PageRank。</p><a id="more"></a><p>之前对方阵有对角化分解，而且不是所有的方阵都可以对角化，但是SVD是所有矩阵都可以进行分解的。SVD分解的过程如下：</p><p><img src="https://i.imgur.com/ZMo1wcb.png"></p><p>这个公式会有什么特性呢？我们假设<span class="math inline">\(U = \{u_1, u_2, \cdots, u_m\}\)</span>，<span class="math inline">\(V = \{v_1, v_2, \cdots, v_n \}\)</span>，<span class="math inline">\(\Sigma\)</span>是常数<span class="math inline">\(\{\sigma_1, \sigma_2, \cdots, \sigma_k\}\)</span>的对角矩阵。这里有一个点要注意的就是<span class="math inline">\(\Sigma\)</span>的样子大体上会是左上角一个对角矩阵，其余部分都是零的<span class="math inline">\(m \times n\)</span>的矩阵。这些<span class="math inline">\(\sigma\)</span>称为奇异值，这些奇异值会等于<span class="math inline">\(A^{\top}A\)</span>的特征根的平方根。</p><p>那么如果矩阵经过SVD分解以后，一定会得到<span class="math inline">\(Av_i = \begin{cases}\sigma_i u_i &amp; \text{if } 1 \le i \le k \\ 0 &amp; \text{if } i &gt; k \end{cases}\)</span>，<span class="math inline">\(A^{\top}u_i = \begin{cases}\sigma_i v_i &amp; \text{if } 1 \le i \le k \\ 0 &amp; \text{if } i &gt; k \end{cases}\)</span>。</p><p>现在问题来了，如果给一个矩阵，要怎么计算奇异值？假设有一个矩阵<span class="math inline">\(A = \begin{bmatrix} 0 &amp; 1 &amp; 2 \\ 1 &amp; 0 &amp; 1 \end{bmatrix}\)</span>，可以直观看到，这个矩阵是<span class="math inline">\(3 \times 2\)</span>的矩阵，因此需要在<span class="math inline">\(\mathbb{R}^3\)</span>和<span class="math inline">\(\mathbb{R}^2\)</span>都要有orthogonal matrix。所以先构建矩阵<span class="math inline">\(A^{\top}A = \begin{bmatrix}1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 2 \\ 1 &amp; 2 &amp; 5 \end{bmatrix}\)</span>。那么做一个<span class="math inline">\(\mathbb{R}^3\)</span>上面的orthogonal matrix，按照这个<a href="https://samaelchen.github.io/linear_algebra_step11/">博客</a>最后的正交化方法，我们可以将矩阵正交化为<span class="math inline">\(v_1 = \frac{1}{\sqrt{30}} \begin{bmatrix} 1 \\ 2 \\ 5 \end{bmatrix}\)</span>，<span class="math inline">\(v_2 = \frac{1}{\sqrt{5}} \begin{bmatrix} 2 \\ -1 \\ 0 \end{bmatrix}\)</span>，<span class="math inline">\(v_3 = \frac{1}{\sqrt{6}} \begin{bmatrix} 1 \\ 2 \\ -1 \end{bmatrix}\)</span>。然后求一下<span class="math inline">\(A^{\top}A\)</span>的特征根分别是6，1和0。所以奇异值就是<span class="math inline">\(\sqrt{6}\)</span>和1。然后就可以按照上面的公式算出<span class="math inline">\(u_1 = \frac{1}{\sqrt{5}} \begin{bmatrix} 2 \\ 1 \end{bmatrix}\)</span>，<span class="math inline">\(u_1 = \frac{1}{\sqrt{5}} \begin{bmatrix} -1 \\ 2 \end{bmatrix}\)</span>。</p><p>那么事实上算到这里，只要将上面的向量集合和奇异值排列好，就完成了矩阵<span class="math inline">\(A\)</span>的SVD分解。也就是<span class="math display">\[A = \begin{bmatrix} \frac{2}{\sqrt{5}} &amp; \frac{-1}{\sqrt{5}} \\ \frac{1}{\sqrt{5}} &amp; \frac{2}{\sqrt{5}} \end{bmatrix} \begin{bmatrix} \sqrt{6} &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \begin{bmatrix} \frac{1}{\sqrt{30}} &amp; \frac{2}{\sqrt{5}} &amp; \frac{1}{\sqrt{6}} \\ \frac{2}{\sqrt{30}} &amp; \frac{-1}{\sqrt{5}} &amp; \frac{2}{\sqrt{6}} \\ \frac{5}{\sqrt{30}} &amp; 0 &amp; \frac{-1}{\sqrt{6}} \end{bmatrix}^{\top}\]</span></p><p>那么SVD跟PCA是有非常多相似的地方的，如果我们使用了全部的奇异值，那么我们就可以还原原来的矩阵，但是如果我们只取了前面的一部分奇异值，我们得到的就是一个损失了一部分信息的矩阵。SVD在机器学习的领域有非常多的应用，最常用的一个地方就是用在推荐算法里面，另外就是降维。此外，还有一个矩阵分解方法是NMF，解释性会更强一些。这个在之前机器学习的博客里面也有提到。</p><p>然后是PageRank。这个算法缔造了今天的谷歌，也被称作是最贵的eigen value。PageRank实际上是一个蛮复杂的模型，这里讲一个最简单的情况，后面找机会再认真学习一下。所以这里有一些矩阵分析里面的定理（虽然我也不是很懂）就直接记结论，证明过程以后再学吧。</p><p>首先我们假设这个世界上只有四个网页，他们的关系如下：</p><p><img src="https://i.imgur.com/kpsOeB5.png"></p><p>现在假设有一个人随机浏览网页，他到每一个网站的可能性都是一样的，那么根据上图的结果我们可以得到： <span class="math display">\[\begin{align}x_1 &amp; = x_3 + \frac{1}{2} x_4 \\x_2 &amp; = \frac{1}{3} x_1 \\x_3 &amp; = \frac{1}{3} x_1 + \frac{1}{2} x_2 + \frac{1}{2} x_4 \\x_4 &amp; = \frac{1}{3} x_1 + \frac{1}{2} x_2\end{align}\]</span></p><p>所以我们就可以很简单得到这样一个矩阵<span class="math inline">\(A = \begin{bmatrix}0 &amp; 0 &amp; 1 &amp; \frac{1}{2} \\ \frac{1}{3} &amp; 0 &amp; 0 &amp; 0 \\ \frac{1}{3} &amp; \frac{1}{2} &amp; 0 &amp; \frac{1}{2} \\ \frac{1}{3} &amp; \frac{1}{2} &amp; 0 &amp; 0\end{bmatrix}\)</span>。这样的矩阵我们叫做马尔科夫矩阵，或者叫转移矩阵。这种矩阵的特点是每一个行的和为1，或者每一个列的和为1。根据<a href="https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem" target="_blank" rel="noopener">Perron-Frobenius theorem</a>这样的矩阵必然有一个特征值为1。</p><p>所以PageRank实际上在做的事情就是计算<span class="math inline">\(A\)</span>的特征根为1时候的特征向量。这个特征向量最后就是我们的网页排名。</p><p>如果想要对PageRank有多一点的了解可以上Wikipedia看一下<a href="https://en.wikipedia.org/wiki/PageRank" target="_blank" rel="noopener">PageRank的页面</a>，也可以直接看原来的<a href="http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf" target="_blank" rel="noopener">论文</a>。另外有中文的这篇博客<a href="http://blog.codinglabs.org/articles/intro-to-pagerank.html" class="uri" target="_blank" rel="noopener">http://blog.codinglabs.org/articles/intro-to-pagerank.html</a>，介绍比较全面，不过基本上没有数学证明过程。看看以后有没有空自己推导一遍，顺便Python实现一下。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;两个线性代数里面在机器学习领域大放异彩的算法，SVD和PageRank。&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数 12</title>
    <link href="https://samaelchen.github.io/linear_algebra_step12/"/>
    <id>https://samaelchen.github.io/linear_algebra_step12/</id>
    <published>2018-07-15T16:00:00.000Z</published>
    <updated>2018-08-15T11:58:12.584Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>稍微介绍一下两种特殊的矩阵，orthogonal matrix和symmetric matrix。</p><a id="more"></a><p>orthogonal matrix其实就是矩阵里面每个向量相互独立的矩阵，如果是orthonormal的话，这些矩阵里的向量都是单位向量。比如说<span class="math inline">\(\begin{bmatrix}\frac{1}{\sqrt{2}} &amp; 0 &amp; \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} &amp; 0 &amp; \frac{-1}{\sqrt{2}} \\ 0 &amp; 1 &amp; 0 \end{bmatrix}\)</span>。</p><p>这样的矩阵有一些特性，首先，orthogonal matrix <span class="math inline">\(Q\)</span>的transpose和inverse相等。也就是<span class="math inline">\(Q^{\top} = Q^{-1}，且这两个矩阵都是orthogonal的\)</span>，另外，<span class="math inline">\(\det(Q) = \pm 1\)</span>。最后，orthogonal matrix和orthogonal matrix叉乘之后还是orthogonal matrix。</p><p>orthogonal matrix还有一个很特殊的特性，就是向量和orthogonal matrix相乘以后，向量的norm不变。</p><p>另一种特殊矩阵是symmetric matrix，也就是类似<span class="math inline">\(\begin{bmatrix}a &amp; b \\ b &amp; c \end{bmatrix}\)</span>。首先，symmetric matrix一定有实特征根。其次，symmetric matrix一定有orthogonal eigenvectors。最后，symmetric matrix一定是diagonalizable的。这里存在一个等价关系<span class="math inline">\(A \text{ is symmetric等价于} P^{\top}AP = D 或 A = PDP^{\top}\)</span>。而这的<span class="math inline">\(P\)</span>包含<span class="math inline">\(A\)</span>的特征向量，<span class="math inline">\(D\)</span>是<span class="math inline">\(A\)</span>的特征根组成的对角矩阵。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;稍微介绍一下两种特殊的矩阵，orthogonal matrix和symmetric matrix。&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>经济思想史读书笔记——重商主义、重农主义及其他先驱</title>
    <link href="https://samaelchen.github.io/economics_thoughts_notes3/"/>
    <id>https://samaelchen.github.io/economics_thoughts_notes3/</id>
    <published>2018-07-11T16:00:00.000Z</published>
    <updated>2018-08-15T11:58:12.584Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>值得注意的是，发明家们并不具备被称为“科学态度”的客观性。——威廉·莱特温</p><a id="more"></a><p>1600年至1750年的150年间，经济活动极大增加，文艺复兴完成了资本主义的萌芽，教会的权威极大被削弱，这些为后来的工业革命的爆发奠定了基础。</p><p>这一时期，经济思想从简单的个人、家庭、生产者的观点，向更复杂的将经济体视为有其自身规律与相互关系的一个系统的观点演进。</p><h1 id="重商主义">重商主义</h1><p>重商主义的主要贡献出自英国人和法国人之手。经院哲学主要来自中世纪牧师，而重商主义的经济理论主要来自商人。重商主义的理论更像是个人的智力反应，而不是一个成体系的理论框架。</p><p>重商主义时期，由于领地减少，名族国家增多，重商主义理论试图确定能够推动国家权利与财富增加的最佳政策。</p><p>重商主义者的理论是基于世界总财富不变的假设下入手的。经院哲学在这个假设下论证了个人的财富获取，必定伴随另一个人的失去。重商主义将这个观点推广到了国家之间。因此，在他们看来，一个国家的财富是依靠很多国家的贫困来支撑的。重商主义者强调，国际贸易是增加一国财富和权力的一种手段，并且特别集中研究国家之间的贸易平衡问题。</p><p>大多数重商主义者认为，经济活动的目的是生产（古典经济学认为是消费）。因此，他们为了在国际贸易中保持优势，提倡低工资、低消费。乍一听，这不就是拥有劳动力优势的发展中国家的情况么。在重商主义的理论下，国民的贫困将使国家受益。所以在重商主义的思想下，为了实现贸易顺差，一国应该通过关税、配额、津贴和税收等手段来鼓励出口、限制进口；应当通过政府干预国内经济，以及通过对外贸易规则来刺激生产；应当对从国外进口的制造业产品征收保护性关税；应当鼓励进口用于制造出口产品的廉价原材料。诶，是不是很像某国现在在搞的贸易战的样子。</p><p>重商主义的货币观点是，一国的财富等同于国内持有的贵金属存量。他们深信，货币因素而非实际因素是经济活动与经济增长的首要决定因素。后面看到古典经济学的时候再做一个具体的比较。</p><p>从现在的角度来看，重商主义者被利益驱动，利用政府为他们自身获取经济特权。他们通常是商人，支持政府允许垄断，使得商人垄断者可以索要比没有垄断时更高的价格。</p><p>重商主义者最重要的成就或许是认可了对经济体进行分析的可能性。他们意识到了经济体中非常机械的因果关系，并且相信如果一个人弄懂了这些关系的规则就能控制经济体。他们认为政府干预可以达到既定的目的，但是不能随便干预使得一些基本的经济原理变得复杂。后期的重商主义意识到早期重商主义的很多理论不足，例如硬币不能代表一国的财富；对于所有国家不可能存在一种贸易顺差等。后期重商主义出现了早期古典自由主义的观点。但是古典主义和重商主义还是存在一个重要的差别，那就是重商主义认为私人利益和公关福利之间存在根本的冲突，而古典主义经济学家认为公共利益是个人追求自我利益自然而然的结果。</p><h1 id="重农主义">重农主义</h1><p>重农主义主要在18世纪的法国兴起。重农主义的著作与重商主义不同的是，重农主义有显著的一致性。重农主义的发展是短时间在法国集中出现的，而且有共同的知识领袖——弗朗索瓦·魁奈。</p><p>重农主义认为存在自然法则支配着经济体的运作，这些法则独立于人类意志，而人类可以客观发现它们。他们觉得，有必要通过分离主要经济变量来构建理论模型。重农主义并不集中研究货币，而是重点研究导致经济发展的实际力量上。相对于重商主义认为的财富源于交换过程的观点，重农主义推断财富起源于农业或者自然。</p><p>因为重农主义发展的时代，生产的产品用于支付实际生产成本之后，产生了剩余。对这种剩余的探索让他们形成了净产品的概念。根据他们的观点，劳动只能生产出支付劳动成本的产品，只有土地例外。因此土地的生产产生了净产品，而其他的非农业活动不能产生净产品。所以重农主义者集中注意力于物质生产力而不是价值生产力。</p><p>重农主义的理论巅峰是魁奈的《经济表》。事实上，这个表格的价值流动，看上去很像“投入——产出表”。经济表证明了经济体不同部门之间相互依赖的存在。</p><p>在经济政策上，重农主义者认为存在一种比人类设计的秩序更优的自然秩序，所以政府的任务是实行自由放任的政策。他们推断自由竞争将会导致最优价格；如果每个个体都追求自己的私利，那么社会将从中获利。</p><p>事实上，重商主义者发现净产品的源泉是交换，尤其是国家贸易形式的交换，因此他们提倡贸易顺差。而重农主义者则认为净产品源自农业，因此主张放任自由会引发农业生产的增加，最终引起更大的经济增长。</p><h1 id="其他思想先驱">其他思想先驱</h1><h2 id="托马斯孟">托马斯·孟</h2><p>托马斯·孟是一个主要的重商主义者，但是他的观点跟原始的重商主义不一样。作为一个英国人，他指出，尽管与所有其他国家实现了贸易顺差是合意的，贵金属流出到其他国家是不合意的。但是在与印度的贸易逆差以及出口贵金属到印度却有利于英国，因为这些实践扩大了英国与所有国家的贸易平衡，增加了金银的流入。虽然托马斯·孟是一个重商主义者，但是他以及看到了早期重商主义范例的严重错误。</p><h2 id="威廉配第">威廉·配第</h2><p>配第是第一个提倡测量经济变量的经济学家。配第最早明确提倡用我们所谓的统计方法来度量社会现象。他设法度量一国的人口、国民收入、出口、进口、资本量。尽管配第对统计学的早期应用显得有些原始，但是，他所代表的方法论立场却具有一种世系，这种世系始于他所处时代的经验归纳，止于当代经济学期刊上盛行的计量经济学线代应用。</p><h2 id="伯纳德曼德维尔">伯纳德·曼德维尔</h2><p>曼德维尔发现世界是邪恶的，但主张“在一个熟练政治家的灵巧管理下，私人恶习有可能变成公众利益”。重商主义的信仰具体表现为对产品的恐惧，对生产过剩与消费不足的关注。个人储蓄并不受欢迎，因为它会引发更低的消费、更低的产量，以及更低的就业。曼德维尔是一个纯粹的重商主义者，他坚决主张政府管制对外贸易，从而保证出口总是超过进口。因为社会的目标是生产，所以曼德维尔甚至主张大量拥有人口和童工，并谴责懒惰。他注意到了一条向下倾斜的劳动力供给曲线。根据曼德维尔的观点，较高的工资将减少劳动供给。他的主要观点就是应当接受满身恶习的人类，并通过规则和制度将其引导到社会利益上来。</p><h2 id="大卫休谟">大卫·休谟</h2><p>休谟被称作自由的重商主义者。他认为一个经济体的经济活动水平取决于货币数量及其周转速度，并对一国的贸易平衡、货币数量以及价格总水平之间的关系做出了相当完整的描述。<strong>黄金流动价格机制</strong>被认为是休谟在国际贸易理论中的重要贡献。</p><p>休谟不是一个纯粹的重商主义者在于，他指出，一个经济体不可能持续保持贸易顺差。贸易顺差将导致经济体内金银的增加。货币增加将使得具有贸易顺差的经济体价格上升。那么具有贸易逆差的经济体就会货币减少，价格下降。那么在这样的情况下，原来具有贸易顺差的经济体出口就会减少，进口就会增加。逆差的经济体则相反。这一过程最终将会导致贸易平衡的自动调整。</p><p>休谟认为，尽管一国的货币绝对量不能影响实际产量，但是货币供给的逐渐增加将会引起产量的增加。这一点，休谟并没有跳出重商主义的框架。</p><p>最后休谟主张经济自由，他认为经济自由和政治自由的增长是结合在一起的。</p><h2 id="理查德坎蒂隆">理查德·坎蒂隆</h2><p>理查德·坎蒂隆是部分重商主义，部分重农主义，以及部分的重农主义-古典学派。坎蒂隆通过推理的过程来建立经济学基本原理，并试图收集数据，并在检验原理的过程中加以使用。他最具有影响力的见解是关于市场体制的，该体制通过个人私利这一媒介来协调生产者和消费者的活动。直观感受上，非常接近完全竞争市场。</p><p>他总是倾向于将任何经济成分当做是一个完整结构的一部分。例如，在他的体制中人口变化是内生的，而不是外生的。他区分了由短期因素决定的市场价格与他所谓的内在价值即长期均衡价格。他最熟练的技术分析主要在宏观经济学中，即货币供给变化对价格和生产的影响。他将经济体划分为部门，分析收入在部门之间的流动。他注意到，新的资金进入经济体，价格总水平可能改变，但是相对价格也可能改变，并对经济体的不同部门产生影响。</p><h1 id="西班牙思想">西班牙思想</h1><p>因为地理大发现带来了大量的金银，黄金大量流入西班牙，西班牙国内的价格水平上升，西班牙的知识分子开始评价这些迅速变换的经济现象。在西班牙思想中，货币数量理论表明，货币价值即货币购买力是由流通中的货币数量决定的。其中，路易斯·摩里纳对于市场机制的描述就是我们今天的需求与供给定理，以及货币数量理论。</p><blockquote><p>……产品短缺，促使公平价格上升……丰裕使得公平价格下降。进入市场的购买者的数量在一些时候比另一些时候多一些，他们热切的购买愿望引起价格上升……一个地方缺少货币，会导致其他物品价格下降，货币充裕则会使价格上升……</p></blockquote><h1 id="小结">小结</h1><p>重商主义者和重农主义者都认为经济体可以被正式地加以研究，并且发展了一种抽象方法来发觉能够调节经济体的法则。他们第一次让经济理论立足于抽象的模型构建过程。</p><p>重商主义者就货币在确定价格总水平中的作用，以及对外贸易平衡对国内经济活动的影响方面取得了最初的尝试性见解。重农主义者则是提出了经济体不同部门相关性的概念。在面对经济体的基本冲突上，重商主义者和经院哲学都提倡，要么借助政府，要么借助教会，对经济体施加干预。但是重农主义者则认识到利益冲突的结果基本上是协调的，是相对稀缺性所固有的，他们不提倡政府干预，而是提倡自由放任。</p><p>这一时期的一些英国经济学家既不完全符合重商主义，也不完全符合古典阵营。他们否决了交换中的固有冲突这一较为原始的重商主义观点；反驳了永远保持有利贸易平衡的必要性；也正是他们了解了市场是如何运作来调整个别经济活动的。</p><p>这些重要的思想者没能完成理论的大一统，这就是未来亚当·斯密的任务。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;值得注意的是，发明家们并不具备被称为“科学态度”的客观性。——威廉·莱特温&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="https://samaelchen.github.io/categories/ReadingNotes/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数 11</title>
    <link href="https://samaelchen.github.io/linear_algebra_step11/"/>
    <id>https://samaelchen.github.io/linear_algebra_step11/</id>
    <published>2018-07-10T16:00:00.000Z</published>
    <updated>2018-08-15T11:58:12.584Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>向量正交，如果从几何的角度来看，向量的正交可以看作是两个向量垂直。</p><a id="more"></a><p>首先，我们下一些定义。我们将向量的长度叫做norm，记做<span class="math inline">\(\| v \| = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}\)</span>。那么两个向量之间的距离我们用两个向量差的norm表示，记做<span class="math inline">\(\| v - u \|\)</span>。</p><p>然后向量有两种乘积，一种是点乘，一种是叉乘。叉乘就看作是只有一列的矩阵，然后用矩阵的叉乘方法就好了。至于点乘，实际上也可以看作是叉乘。定义如下： <span class="math display">\[v \cdot u = \sum_i^n v_i u_i = v^{\top} u\]</span> 这里再说明一下，默认向量是列向量。</p><p>现在进入正题，向量正交就是两个向量的点内积为0，也就是<span class="math inline">\(u \cdot v = 0\)</span>。那么很自然就会知道，零向量与所有的向量正交。</p><p>那么向量的点内积有一些运算性质： 假设有向量<span class="math inline">\(u，v\)</span>，矩阵<span class="math inline">\(A\)</span>， 常数<span class="math inline">\(c\)</span></p><blockquote><ol type="1"><li><span class="math inline">\(u \cdot u = \| u \|^2\)</span></li><li><span class="math inline">\(u \cdot u = 0\)</span> if and only if <span class="math inline">\(u = 0\)</span></li><li><span class="math inline">\(u \cdot v = v \cdot u\)</span></li><li><span class="math inline">\(u \cdot (v + w) = u \cdot v + u \cdot w\)</span></li><li><span class="math inline">\((v + w) \cdot u = v \cdot u + w \cdot u\)</span></li><li><span class="math inline">\(cu \cdot v = u \cdot cv\)</span></li><li><span class="math inline">\(\| cu \| = |c| \| u \|\)</span></li><li><span class="math inline">\(Au \cdot v = (Au)^{\top} v = u^{\top}A^{\top}v = u \cdot A^{\top}v\)</span></li><li><span class="math inline">\(\| u+v \| \le \|u\| + \|v\|\)</span></li></ol></blockquote><p>如果我们现在有个向量集合，集合里所有的向量互相正交，那么我们就叫这个集合是orthogonal set。那么如果刚好这里的向量都是单位向量，这个集合就可以叫做orthonomal basis。</p><p>现在回过头来看，这样的一个集合有什么用呢？这个向量集合是不是非常像之前的坐标系。然后进一步来看，假设现在有一个集合<span class="math inline">\(S = \{ v_1 \; v_2 \; \cdots \; v_n \}\)</span>是一个orthogonal basis，有一个向量<span class="math inline">\(u\)</span>是这些向量的线性组合，也就是说<span class="math inline">\(u = c_1 v_1 + c_2 v_2 + \cdots + c_n v_n\)</span>，那么如果我们要求<span class="math inline">\(c_i\)</span>，其实非常简单就是<span class="math inline">\(c_i = \frac{u \cdot v_i}{\| v_i \|^2}\)</span>。如果现在再从几何的角度来看，这个<span class="math inline">\(c_i\)</span>其实就是<span class="math inline">\(u\)</span>在<span class="math inline">\(v_i\)</span>上投影的长度。</p><p>那如果现在随便给一个basis，<span class="math inline">\(\{u_1 \; u_2 \; \cdots \; u_n \}\)</span>，现在要将这个basis变成orthogonal basis，要做的是： <span class="math display">\[\begin{align}v_1 &amp; = u_1 \\v_2 &amp; = u_2 - \frac{u_2 \cdot v_1}{\|v_1\|^2}v_1 \\v_3 &amp; = u_3 - \frac{u_3 \cdot v_2}{\|v_2\|^2}v_2 - \frac{u_3 \cdot v_1}{\|v_1\|^2}v_1 \\&amp; \vdots \\v_n &amp; = u_n - \frac{u_n \cdot v_{n-1}}{\|v_{n-1}\|^2}v_{n-1} - \cdots - \frac{u_n \cdot v_1}{\|v_1\|^2}v_1\end{align}\]</span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;向量正交，如果从几何的角度来看，向量的正交可以看作是两个向量垂直。&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数 10</title>
    <link href="https://samaelchen.github.io/linear_algebra_step10/"/>
    <id>https://samaelchen.github.io/linear_algebra_step10/</id>
    <published>2018-07-09T16:00:00.000Z</published>
    <updated>2018-08-15T11:58:12.584Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>在讲矩阵可对角化前，先引入一个概念，矩阵相似。如果存在方阵<span class="math inline">\(A，B\)</span>，一个可逆矩阵<span class="math inline">\(P\)</span>，使得<span class="math inline">\(P^{-1} A P = B\)</span>，那么我们称<span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>是相似的。那么如果现在<span class="math inline">\(B\)</span>是一个对角矩阵的话，那么我们就称<span class="math inline">\(A\)</span>是可对角化的（diagonalizable）。一般而言，这里会用<span class="math inline">\(D\)</span>来表示对角矩阵。</p><a id="more"></a><p>那么对角化有什么意义呢？我们从公式出发看一下，将<span class="math inline">\(P\)</span>表示为<span class="math inline">\([p_1 \; \cdots \; p_n]\)</span>，将<span class="math inline">\(D\)</span>表示为<span class="math inline">\(\begin{bmatrix} d_1 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; d_n \end{bmatrix}\)</span>。我们之前的公式是<span class="math inline">\(P^{-1} A P = D\)</span>，所以<span class="math inline">\(AP = PD\)</span>。</p><p>先看左边，<span class="math inline">\(AP = [Ap_1 \; \cdots \; Ap_n]\)</span>，再看右边<span class="math inline">\(PD = P[d_1 e_1 \; \cdots \; d_n e_n] = [P d_1 e_1 \; \cdots \; P d_n e_n] = [d_1 P e_1 \; \cdots \; d_n P e_n] = [d_1 p_1 \; \cdots \; d_n p_n]\)</span>。这不就是特征根么。</p><p>所以我们就看到<span class="math inline">\(A\)</span>的特征向量可以组成一个向量空间<span class="math inline">\(\mathbb{R}^n\)</span>。</p><p>那么如何对角化呢，只要找到n个线性无关的向量<span class="math inline">\(p_i\)</span>，然后将这些向量组成一个矩阵，就可以得到可逆矩阵<span class="math inline">\(P\)</span>。然后特征根只要按对角线排列就是<span class="math inline">\(D\)</span>。</p><p>解法就是计算<span class="math inline">\(\det(A - tI) = (t-\lambda_1)^{m_1} (t-\lambda_2)^{m_2} \cdots\)</span>。那么因为每个<span class="math inline">\(\lambda\)</span>对应能有的eigenvector数量是小于等于指数<span class="math inline">\(m\)</span>的，只要每一个指数<span class="math inline">\(m\)</span>都等于eigenspace，那么我们就说<span class="math inline">\(A\)</span>可以对角化。</p><p>比如矩阵<span class="math inline">\(A = \begin{bmatrix} -1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 2 \\ 0 &amp; 2 &amp; 1 \end{bmatrix}\)</span>，那么<span class="math inline">\(A\)</span>的因式分解是<span class="math inline">\(-(t+1)^2 (t-3)\)</span>。所以特征根是3和-1。而对应的特征向量就是<span class="math inline">\(\begin{bmatrix}0 \\ 1 \\ 1 \end{bmatrix} \; \begin{bmatrix}1 \\ 0 \\ 0 \end{bmatrix} \; \begin{bmatrix}0 \\ 1 \\ -1 \end{bmatrix}\)</span>。这样我们就完成了对角化。</p><p>矩阵对角化的好处是如果要做连乘的时候，对角矩阵的连乘是非常简单的，这样就可以极大减少计算开销。也就是说<span class="math inline">\(A^m = P^{-1} D^m P\)</span>。</p><p>最后其实回想一下之前的坐标系变换，对角化的过程其实就是一次坐标系的变换过程。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在讲矩阵可对角化前，先引入一个概念，矩阵相似。如果存在方阵&lt;span class=&quot;math inline&quot;&gt;\(A，B\)&lt;/span&gt;，一个可逆矩阵&lt;span class=&quot;math inline&quot;&gt;\(P\)&lt;/span&gt;，使得&lt;span class=&quot;math inline&quot;&gt;\(P^{-1} A P = B\)&lt;/span&gt;，那么我们称&lt;span class=&quot;math inline&quot;&gt;\(A\)&lt;/span&gt;和&lt;span class=&quot;math inline&quot;&gt;\(B\)&lt;/span&gt;是相似的。那么如果现在&lt;span class=&quot;math inline&quot;&gt;\(B\)&lt;/span&gt;是一个对角矩阵的话，那么我们就称&lt;span class=&quot;math inline&quot;&gt;\(A\)&lt;/span&gt;是可对角化的（diagonalizable）。一般而言，这里会用&lt;span class=&quot;math inline&quot;&gt;\(D\)&lt;/span&gt;来表示对角矩阵。&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>经济思想史读书笔记——早期古典经济学</title>
    <link href="https://samaelchen.github.io/economics_thoughts_notes2/"/>
    <id>https://samaelchen.github.io/economics_thoughts_notes2/</id>
    <published>2018-07-07T16:00:00.000Z</published>
    <updated>2018-08-15T11:58:12.584Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>“我们从哪儿出发？”红皇后问。 “从开始出发。”渡渡鸟答道。——Lewis Carroll</p><a id="more"></a><p>早期的经济研究并不系统，没有出现重大的分析体系。直到18世纪中期，随着Adam Smith引领的“古典经济学”的出现，经济学才向着成熟的社会科学状态大踏步前进。</p><p>古典经济学始于1776年Adam Smith《国富论》的出版，通常将1776年之前划分为两个部分。公元前800年到1500年的早期前古典阶段，以及从1500年1776年的前古典时期。</p><p>从时间线上看，早期前古典经济思想可以分为中国、希腊、阿拉伯——伊斯兰思想以及经院哲学。</p><h1 id="东方经济思想">东方经济思想</h1><p>中国的早期经济思想以管仲为代表。熊彼特认为早期的著作“基本上被限制在道德的框架内考察公共事务管理，而不是严格的、‘科学的’研究”。但是《管子》极大程度上超越了公共事务管理的模子。</p><p>管仲的经济思想核心可以用“轻/重”理论来表达。他提出当一种产品充裕时，就会变“轻”，价格就会下降。当产品被“锁藏”时，就会变“重”，价格上涨。产品会依据这种变化进入或退出市场，最后向一个均衡的价格运动。</p><p>事实上，这种理论是非常类似微观经济学里的供需理论。只不过，这样的供需理论需要基于一个基本的前提，就是市场是完全竞争的市场。</p><p>管仲甚至将这一理论延伸到了货币数量。也就是说，货币“重”的时候，货币价格上涨，产品价格就下降了，反之，货币价格下降，产品价格上涨。如果类比到现在的概念，就是通货紧缩和通货膨胀。而管仲认为，应对通货紧缩时候国家应该购买产品，目的是让价格上涨；而通货膨胀的时候就应该卖出产品，让产品价格下降。</p><p>当代应对通货膨胀或者通货紧缩的问题一般都是采取货币政策，也就是央妈通过操控市场上的现金解决问题。最简单的，通货膨胀时，减少货币供给量，提高银行准备金率，提高储蓄利率等等。而通货紧缩就反过来。</p><p>齐桓公曾向管仲询问，丰收之年如何来遏制粮食价格的过度下滑。管仲认为应当减少粮食的流通量，为此必须采取政府强制手段，他说：“请以令与大夫城藏，使卿诸侯藏千钟，令大夫藏五百钟，列大夫藏百钟，富商蓄贾藏五十钟。内可以为国委，外可以益农夫之事。”这里需要注意的是，在春秋时代粮食实际上不仅是一种商品，也具备一定的货币功能。管仲认为强制减少粮食流通量，自然可以制止价格的下滑。既可以增加国家的粮食储备，又可以维护农民的利益；相反，在歉收之年，则将储备之粮食投入市场。以遏制粮商囤积哄抬价格，又可以使政府得到一笔收入，所以管仲说：“夫民有余则轻之，故人君敛之以轻；民不足则重之，故人君散之以重。敛积之以轻，散行之以重，故君必有十倍之利。”</p><p>如果将粮食当做是一种流通货币，那么这不就是现在的货币政策么？</p><p>在管仲的思想体系下，一方面存在市场“无形的手”，另一方面，又存在国家调控这样“有形的手”。是不是听上去又很像某种经济形式。</p><h1 id="希腊思想">希腊思想</h1><h2 id="赫西奥德和色诺芬">赫西奥德和色诺芬</h2><p>赫西奥德和色诺芬都对效率感兴趣。经济学家通常用产出和投入的比率衡量效率。早期的经济学家对社会层面的效率问题并不感兴趣。他们更多关注一组和生产者与家庭层面的效率相关的问题。而色诺芬则在赫西奥德之后采用了有效管理的概念，并且在家庭、生产者、军事和公关事务层面使用了这些概念。这种思想的进步在于，他充分认识到了劳动分工可以提高效率。</p><p>这种思想影响了包括亚里士多德在内的很多希腊经济学家。并且在之后的时间里也影响了经院哲学。</p><h2 id="亚里士多德">亚里士多德</h2><p>德谟克利特认为，不仅需要劳动分工，而且认为财产私有化可以促进更多的经济活动。但是亚里士多德的老师柏拉图则认为，统治者不应该拥有私人财产，而应该掌控公共财产，避免对财产的争夺。</p><p>但是亚里士多德则不一样。亚里士多德一方面谴责追求经济利益的行为，一方面又认可私有财产的权利。亚里士多德对经济思想的最大贡献在于商品交换与交换中货币的使用。</p><p>亚里士多德认为人的需要是适度的，但是欲望是无穷的。因此满足需要的商品生产是恰当的，但是力图满足无穷欲望的产品生产就是不正常的。他推崇以物易物的交易方式，而不是通过货币这一媒介，通过交易获取货币收益。事实上，现在主流的经济学家不区分人类的需求和欲望，在高度分工化的现在，客观上区分需求和欲望是不太现实的。</p><p>亚里士多德一个值得关注的论点是，减少消费来改变人们的态度，希望通过排除稀缺性固有的冲突来解决社会冲突。这是大多数乌托邦人士和社会主义者的有力观点。</p><h1 id="阿拉伯伊斯兰思想">阿拉伯——伊斯兰思想</h1><p>阿拉伯——伊斯兰思想填补了亚里士多德到经院哲学之间的空白。</p><p>艾布·哈米德·安萨里认识到了物物交换的困难，以及货币对交易的便利。同时他也考察了很多其他的经济话题，例如公共支出、征税与借贷等等。</p><p>而伊本·赫勒敦考察的更多是人口、利润、供给、需求、价格、奢侈品、总剩余、资本等。</p><h1 id="经院哲学">经院哲学</h1><p>经院哲学的基础是封建社会。经院哲学经济学家们试图提供适用于世俗活动的宗教指导方针。他们的的目的不是分析发生了什么样的经济活动，而是订立与宗教教义相一致的经济行为规则。经院哲学关注价格体系中的公正或公正缺失。最重要的经院哲学经济学家是圣托马斯·阿奎那。</p><p>经院哲学的核心经济问题还是：私有财产制度以及公平价格与高利贷的概念。</p><p>在私有财产方面，阿奎那的杰出贡献在于，他融合了宗教教义与亚里士多德的著作。在《圣经》中，基督教思想谴责私人财产，但是他认为，私人财产不是违背自然法则，而是自然法则的一种补充。他论证说，裸体是自然法则，但是服装是对自然法则的补充，私有财产也是为了人们的利益而设计的。他认可私人财产的不平等分配，但是对于坚定投身宗教的人，短缺与公共生活是一种理想状态。</p><p>而对于产品价格，经院哲学经济学家不关心经济体中价格的形成，或者去了解价格在稀缺资源配置中所扮演的角色。他们关心价格的道德性，也就是价格的公平与公正的概念。不同的理论史学家对公正价格的理解是不一样的。但是经院哲学的公正价格可以看作是李嘉图——马克思劳动价值理论、边际效用观点，以及古典——新古典理论中按时的竞争性市场产生理想公正价格观点的先驱。另一些主张也认为，所谓的公正价格就是市场上的所有价格。但是经院哲学对于经济分析的缺失让我们难以确定究竟“公正价格”意味着什么。</p><p>从公正价格会很自然推论到高利贷的观点。高利贷在现在代表着索要过高的利率，但是在经院哲学和亚里士多德的著作里，高利贷代表着任意的（any）获利行为。但是相对于亚里士多德对借贷的谴责以及宗教对通过货币获利的严厉禁止，经院哲学至少在为了商业目的通过货币获利这点上观点逐渐缓和。</p><p>阿奎那本身也是一个充满矛盾的思想家，一方面他强调道德问题来抑制经济思想；另一方面，又推动了经济学与所有社会科学的前进。</p><h1 id="小结">小结</h1><p>事实上，早期的经济思想并没有关注价格系统的特性与关注，只有管仲是例外的。而希腊思想则考察了私人财产的作用，亚里士多德的许多观点成为后来经院哲学的考察焦点。阿拉伯——伊斯兰思想有效填补了希腊思想到经院哲学之间的历史空白。而经院哲学的目的更多在于确定宗教标准，借此判断经济行为，而不是分析经济体。不过，经院哲学的存在背景是封建制度，随着技术变革的破坏，经济生活对精神生活构成了极大的挑战。但是从历史的进程上看，将经济体从教会下解放出来，既发生在实践层面上，也发生在学术层面上。实际上，这种解放极大促进了西方资本主义的发展，从而构建起现代金融货币体系。关于货币从宗教解放的资料可以看看央视的纪录片《货币》第二集，犹太人最早从宗教总解脱出来，施展金融才华，这一步在货币历史上至关重要。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;“我们从哪儿出发？”红皇后问。 “从开始出发。”渡渡鸟答道。——Lewis Carroll&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="https://samaelchen.github.io/categories/ReadingNotes/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数 09</title>
    <link href="https://samaelchen.github.io/linear_algebra_step9/"/>
    <id>https://samaelchen.github.io/linear_algebra_step9/</id>
    <published>2018-07-02T16:00:00.000Z</published>
    <updated>2018-08-15T11:58:12.584Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>矩阵的特征根与特征向量。这两个是线性代数里面非常重要的概念。</p><a id="more"></a><p>我们对这两个概念的定义是这样的，如果存在一个矩阵<span class="math inline">\(A\)</span>可以使得常数<span class="math inline">\(\lambda\)</span>和向量<span class="math inline">\(v\)</span>满足： <span class="math display">\[Av = \lambda v\]</span> 那么<span class="math inline">\(\lambda\)</span>就是矩阵的特征根，而<span class="math inline">\(v\)</span>就是特征向量。但是这里需要注意的是，<span class="math inline">\(A\)</span>一定是方阵。举个例子： <span class="math display">\[\begin{bmatrix}5 &amp; 2 &amp; 1 \\ -2 &amp; 1 &amp; -1 \\ 2 &amp; 2 &amp; 4\end{bmatrix} \begin{bmatrix}1 \\ -1 \\ 1\end{bmatrix} = \begin{bmatrix}4 \\ -4 \\ 4\end{bmatrix} = 4\begin{bmatrix}1 \\ -1 \\ 1\end{bmatrix}\]</span></p><p>不过要注意的是，零向量不能作为特征向量。</p><p>所以直观的感受上，特征向量就是经过线性变换以后，只是改变向量长度或者是变成相反的方向。</p><p>那么在特征根特征向量在图像中有什么意义呢？下面举四个例子。</p><p>首先是扭曲，如下图：</p><p><img src="https://i.imgur.com/4z9YFVw.png"></p><p>那么这样的变换过程中，落在横轴上的向量是没有发生任何变换的，因此图片中蓝色的向量就是特征向量，特征根是1。</p><p>第二个变换是映射，如下：</p><p><img src="https://i.imgur.com/KFeF5NV.png"></p><p>在这个变换中，<span class="math inline">\(b_1\)</span>是没有变化的，所以是一个特征根为1的特征向量。而<span class="math inline">\(b_2\)</span>则是刚好反了一个方向，因此是一个特征根为-1的特征向量。</p><p>第三种变换是缩放，如下：</p><p><img src="https://i.imgur.com/hSWgUwf.png"></p><p>在这种情况下，图片的所有向量都是特征向量，特征根就是缩放倍数。</p><p>第四种是旋转，如下：</p><p><img src="https://i.imgur.com/03ETOyW.png"></p><p>因为上面的旋转过程中，没有一个向量保持了原来的方向，或者转到完全相反的方向，因此这样的变换过程中，没有特征向量。</p><p>从上面的四个例子我们还可以发现一个很重要的事情，就是一个特征向量只有唯一对应一个特征根，但是一个特征根可以有多个特征向量。然后，我们就可以顺势定义一个新的概念，eigenspace。也就是<span class="math inline">\(\lambda\)</span>对应的所有特征向量加上零向量构成的subspace。</p><p>那么我们要怎么去找到特征向量和特征根呢？</p><p>首先我们回顾一下之前的公式： <span class="math display">\[Av = \lambda v = \lambda I v\]</span> 所以<span class="math inline">\((A-\lambda I) v = 0\)</span>。这样一来，我们就知道，当我们知道<span class="math inline">\(\lambda\)</span>的时候，只要找到上面那个等式的非零解，就是我们的特征根。</p><p>那么如果现在要判断一个常数是不是特征根，我们依照上面那个等式一步步向下推理，因为<span class="math inline">\((A-\lambda I)v = 0\)</span>有多个解，因此我们可以知道<span class="math inline">\(\text{Rank} (A - \lambda I) &lt; n\)</span>，所以<span class="math inline">\(A - \lambda I\)</span>不可逆，也就是说它的行列式为0。</p><p>比如说矩阵<span class="math inline">\(A = \begin{bmatrix}-4 &amp; 3 \\ 3 &amp; 6 \end{bmatrix}\)</span>，我们计算行列式<span class="math inline">\(\begin{bmatrix}-4-\lambda &amp; 3 \\ 3 &amp; 6-\lambda \end{bmatrix} = 0\)</span>。也就是说<span class="math inline">\((-4-\lambda)(6-\lambda) - 9 = 0\)</span>。所以我们就可以求出来，<span class="math inline">\(\lambda = -3\)</span>或<span class="math inline">\(\lambda = 5\)</span>。</p><p>那么特征根有一些特性。首先，一般来说，一个矩阵跟它的RREF的特征根是不一样的。如果是两个矩阵的因式分解一样，那么就有一样的特征根。</p><p>假设现在有个矩阵<span class="math inline">\(A\)</span>有<span class="math inline">\(n\)</span>个特征根（这里只考虑实数根），那么特征根的和刚好就是<span class="math inline">\(\text{trace } A\)</span>，也就是<span class="math inline">\(A\)</span>的对角线元素的和；特征根的乘积刚好就是<span class="math inline">\(\det A\)</span>。</p><p>实际上，理解一下特征根和特征向量在图像中的意义，然后知道特征根的解法是<span class="math inline">\(\det(A - \lambda I)\)</span>，特征向量的解法是<span class="math inline">\((A - \lambda I)v = 0\)</span>就好了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;矩阵的特征根与特征向量。这两个是线性代数里面非常重要的概念。&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>经济思想史读书笔记——导言</title>
    <link href="https://samaelchen.github.io/economics_thoughts_notes1/"/>
    <id>https://samaelchen.github.io/economics_thoughts_notes1/</id>
    <published>2018-07-02T16:00:00.000Z</published>
    <updated>2018-08-15T11:58:12.584Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>人类一切事业的开始和终结都是凌乱的——John Galsworthy</p><a id="more"></a><h1 id="现代经济学核心思想">现代经济学核心思想</h1><p>经济学是一门研究稀缺性问题的学科。历史上人类曾经有过四种机制来解决稀缺性问题。</p><blockquote><p>第一种，强制方式。<br>第二种，传统方式。也就是依据以往的惯例方式分配。<br>第三种，权威方式。也就是政府，教会的方式。<br>第四种，市场方式。这也是现在在不断发展的分配方式。</p></blockquote><p>一般而言，这些方式之间并不相互排斥。市场方式是现在最主流的分配方式。</p><p>现代经济学分为微观经济学和宏观经济学。微观经济学研究配置问题（生产什么和如何生产）与分配问题（实际收入如何在社会成员间分配）。因此微观经济学更多关注于供给和需求的理论，希望解释决定相对价格的力量。</p><p>而宏观经济学关注的社会的总体分析，自上而下到个体。主要聚焦于经济体的总体分析，主要聚焦于经济体的稳定和增长，关注整体经济的总变量：收入和就业水平、价格总水平、经济增长率等。</p><h1 id="研究经济思想史的方法">研究经济思想史的方法</h1><p>研究经济思想史的方法有两种，一种是相对论史学家，一种是绝对论史学家。</p><p>相对论史学家关心的是：（1）引起人们考察某些经济问题的历史、经济、社会和政治力量是什么。（2）这些力量塑造新兴理论内容的方式。他们主张在每个经济理论的发展过程中，历史都扮演一定的角色。</p><p>绝对论史学家又被称作辉格党，强调内在力量对经济理论发展的解释。绝对论者认为，理论的进步不仅仅反映了历史环境，而且取决于训练有素的专业人员对未决问题或似是而非论点的发现与解释。</p><p>相对论者和绝对论者在经济学历史上交互站上历史舞台，但是经济思想史一定是学科的外在力量和内在力量相互作用的动态过程。</p><p>研究经济思想史可以帮助我们更好理解正统经济理论和非正统经济理论。正统经济理论家主要集中在资源配置、分配、稳定和增长这四个问题上面；而非正统经济理论家则研究社会与经济中产生变换的力量。正统经济学家认为具体的社会制度、政治制度与经济制度是既定的，并在这些制度背景下研究经济行为；非正统经济学家则试图去进行解释。</p><p>正统经济学家和非正统经济学家的差别主要在于他们关注的问题不一致，而不是理论本身的直接对立。</p><h1 id="非正统经济学家的地位">非正统经济学家的地位</h1><p>所谓的正统非正统，或者说主流非主流，只是在竞争中最受欢迎的一组成为主流，不太成功的一组成为非主流。主流研究院更倾向采纳主流思想中较为狭窄的观点，而相比之下，非正统经济学家可能会更重视思想的多样性。</p><h1 id="方法论问题">方法论问题</h1><p>经济思想中需要区分经济学艺术（the art of economics），实证经济学（positive economics）和规范经济学（normative economics）。</p><p>实证经济学关心的是支配经济活动的力量。而经济学艺术关心的是政策问题。规范经济学则明确地关注应当是什么的问题。</p><p>实证经济学的方法是形式化的、抽象的，试图将经济力量与政治和社会力量分开。而经济学艺术则需要致力于政治社会力量与经济力量相互关系的研究。</p><p>以史为鉴，才能看懂现在莫名其妙的经济形式。</p><h1 id="经济学方法论进化路线">经济学方法论进化路线</h1><h2 id="逻辑实证主义logical-positivism的兴起">逻辑实证主义（logical positivism）的兴起</h2><p>逻辑实证主义家的典型代表是维也纳派，他们试图通过描述科学家实际遵循的方法，来使科学家的方法形式化。他们认为，只有当一种演绎理论在经验上被检验与核实之后，它才能被认同为正确的。逻辑实证主义将“科学的目的是确立‘真理’”的观点推到了极致。</p><h2 id="证伪主义">证伪主义</h2><p>证伪主义在波普尔的著作中得到很好的表达。她提出，经验检验不能确定一种理论的真相，只能确定假象。波普尔声称，科学的目的应当是运用经验上可检验的假设来发展理论，然后对理论进行证伪，放弃那些被证明是错误的理论。</p><p>但是证伪主义存在三个问题。第一个问题是，一些理论的经验预言并不能被检验，因为尚不存在对它们进行检验的方法。第二个问题是，难以决定是否理论被证伪或者没有被证伪。第三个问题是研究者的心态，他们未能检验已确立理论的含义，便假定理论的含义是正确的（让我想起李祥林的高斯相依函数，号称摧毁华尔街的公式）。</p><h2 id="范式">范式</h2><p>托马斯·库恩将范式引入方法论的争论中，将方法论远离了证伪主义。范式是一种既定的方法以及构成研究者分析组成部分的知识题，它遵循着任何既定时期所公认的对主流科学思想教科书陈述。</p><p>范式隐藏着这样的观点，现有理论可能并不包含真理。</p><h2 id="研究纲领">研究纲领</h2><p>Imre Lakatos发现，科学家们从事发展竞争性研究纲领时，每个研究纲领不仅包括一系列的数据进行分析和证伪，而且包括无可非议地接受一系列的硬核逻辑假设。每项研究都从硬核中得出一系列周边假设，并试图对它们进行证伪。只有当“足够多”的周边假设被证伪，硬核假设才会被重新考虑。Lakatos认为，如果对周边假设进行证伪的过程在继续，那就是进步（progressive）的，否则就是退化（degenerative）的。</p><p>他的研究有两个特性：（1）它承认了理论证伪过程的复杂性；（2）早起的分析要求某一种理论成为主流，Lakatos则提出多种可利用理论同时存在，这些理论的优点不太容易辨别。</p><h2 id="社会学方法与修辞方法">社会学方法与修辞方法</h2><p>社会学方法与修辞方法拒绝了假设存在终极的、神圣的真理。修辞方法强调语言的说服力。该方法主张，一种理论被接受，不是因为它本身是正确的，而是因为理论的提倡者借助他们出众的修辞，成功地使其他人相信理论的价值。</p><p>社会学方法考察社会与制度约束，这些约束影响着对一种理论的认可。</p><p>这两种方法都对人们发现真理的能力表示怀疑，甚至怀疑真理是否存在。在这些方法中，理论的发展并不一定因为离真理近，理论的发展有多种理由，而真理——如果存在——仅仅只是其中之一。</p><p>这两种方法最有代表性的观点就是费耶阿本德的“一切尽随其便”。</p><h2 id="后修辞学方法">后修辞学方法</h2><p>后修辞经济学家会以怀疑论来看待与研究者自身利益或预想观点相符的研究结果。他们非常有可能遵循贝叶斯统计而不是古典统计。</p><p>贝叶斯主义者认为，人们能够发现语句中更高级或更低级的真理，但不是终极真理。</p><p>事实上，在大多数教科书中，经济学的主流方式依旧是逻辑实证主义，而它在学术杂志上已经死了很久了。形式主义更有可能运用逻辑实证主义或者证伪主义，并且相信绝对论方法。而非形式主义更可能运用社会的或修辞的方法，并相信相对主义方法。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;人类一切事业的开始和终结都是凌乱的——John Galsworthy&lt;/p&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="https://samaelchen.github.io/categories/ReadingNotes/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数 08</title>
    <link href="https://samaelchen.github.io/linear_algebra_step8/"/>
    <id>https://samaelchen.github.io/linear_algebra_step8/</id>
    <published>2018-07-01T16:00:00.000Z</published>
    <updated>2018-08-15T11:58:12.584Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Coordinate system，就是坐标系。其实就是一组vector。</p><a id="more"></a><p>能够拿来做坐标系的vector set，很显然，按照上一篇博客里的内容，我们可以很自然想到，一个vector set必须符合两个条件：</p><ol type="1"><li>这个vector set是<span class="math inline">\(\mathbb{R}^n\)</span>的span</li><li>这个vector set里面的vector是independent的</li></ol><p>那其实这个vector set就是basis。</p><p>如果现在我们的basis刚刚好每个vector都是相互垂直的单位向量，那么我们就会把这个坐标系叫做直角坐标系。</p><p>那么从这个角度来看，其实，我们就可以将矩阵乘法看作是坐标系转换。而且坐标系转换，矩阵一定是可逆的。</p><p>所以如果我们要做任意的坐标系和直角坐标系之间的转换，我们遵守如下的公式：</p><p>从<span class="math inline">\(v_{B}\)</span>到<span class="math inline">\(v\)</span>就是<span class="math inline">\(v = B v_{B}\)</span>，反过来就是<span class="math inline">\(v_{B} = B^{-1} v\)</span>。</p><p>事实上，坐标系转换，或者说线性变换是机器学习里面非常常见的一种情况。比如说PCA就是这样的一种变换。PCA有一点像是在找basis。另外如果了解NMF的话，NMF看上去更像是将一组数据的basis找出来。不过要注意的是，仅仅是看上去很像而已。</p><p>线性变换具有很显著的意义，将一个在原来坐标系下面很复杂的函数，通过线性变换以后就可能得到一个非常简单的函数。</p><p>如下图：</p><p><img src="https://i.imgur.com/HJTQBeg.png"></p><p>我们要做一个关于直线<span class="math inline">\(y = \frac{1}{2} x\)</span>的映射关系。如果这个映射在直角坐标系下面，那么我们的变换矩阵是<span class="math inline">\(\begin{bmatrix} 0.6 &amp;0.8 \\ 0.8 &amp;-0.6 \end{bmatrix}\)</span>。但是如果我们用这条直线作为横轴，垂直于这条直线的向量为纵轴，就会发现，其实在这个坐标系<span class="math inline">\(\begin{bmatrix} 2 &amp;-1 \\ 1 &amp;2 \end{bmatrix}\)</span>内，变换只是<span class="math inline">\(\begin{bmatrix} 1 &amp;0 \\ 0 &amp;-1 \end{bmatrix}\)</span>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Coordinate system，就是坐标系。其实就是一组vector。&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数 07</title>
    <link href="https://samaelchen.github.io/linear_algebra_step7/"/>
    <id>https://samaelchen.github.io/linear_algebra_step7/</id>
    <published>2018-06-26T16:00:00.000Z</published>
    <updated>2018-08-15T11:58:12.584Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>subspace &amp; basis</p><a id="more"></a><p>subspace不严格的讲法可以当做是一个向量空间的子集。严格一点，我们的定义是这样的。一个向量子空间是一组向量，满足三个特征：</p><ol type="1"><li>零向量属于这个向量组</li><li>该向量组内的两个向量<span class="math inline">\(u\)</span>和<span class="math inline">\(v\)</span>的线性组合也属于改向量组。</li><li>该向量组内的向量<span class="math inline">\(u\)</span>乘以一个常数<span class="math inline">\(c\)</span>，<span class="math inline">\(cu\)</span>也属于该向量组。</li></ol><p>举个例子，比如我们现在有一个向量组<span class="math inline">\(W\)</span>属于<span class="math inline">\(\mathbb{R}^3\)</span>，且满足<span class="math inline">\(6w_1 - 5w_2 + 4w_3 = 0\)</span>。那么<span class="math inline">\(W\)</span>是不是<span class="math inline">\(\mathbb{R}^3\)</span>的一个向量子空间呢？我们按照三个特征一一过一下。</p><p>首先，零向量是不是属于这个向量组？很明显是的，让<span class="math inline">\(w_1 = w_2 = w_3 = 0\)</span>等式成立。</p><p>然后我们看，如果现在有<span class="math inline">\(u\)</span>，<span class="math inline">\(v\)</span>属于<span class="math inline">\(W\)</span>，那么<span class="math inline">\(u+v = \begin{bmatrix} u_1 + v_1 &amp;u_2 + v_2 &amp;u_3 + v_3 \end{bmatrix} ^\top\)</span>。那么实际上<span class="math inline">\(6(u_1 + v_1) - 5(u_2+v_2) + 4(u_3 + v_3) = 0\)</span>，因此第二条也满足。</p><p>最后一条，其实也非常显而易见，把<span class="math inline">\(c\)</span>提出来就可以了。</p><p>那subspace里面有一个比较特殊的类型，叫做null space。null space其实就是矩阵<span class="math inline">\(A\)</span>关于<span class="math inline">\(Ax = 0\)</span>的解。对于一个<span class="math inline">\(m \times n\)</span>的矩阵<span class="math inline">\(A\)</span>，一般我们会记做：<span class="math inline">\(\text{Null} \; A = \{v \in \mathbb{R}^n ： Av=0\}\)</span>。那null space一定会是<span class="math inline">\(\mathbb{R}^n\)</span>的向量子空间。</p><p>而basis，也就是基也是特殊的一种subspace。基有三个特性：</p><ol type="1"><li>基是最小的生成集</li><li>基是最大的线性无关向量集合</li><li>向量空间中的向量都按唯一的表达为基的线性组合。也就是说，通过基的线性组合，可以表达向量空间中所有的向量，且都是唯一的表达。</li></ol><p>第一个特性是很直觉的一件事情，假设我们现在有一个向量子空间<span class="math inline">\(V\)</span>，而<span class="math inline">\(S = \{u_1, u_2, \cdots, u_k \}\)</span>是<span class="math inline">\(V\)</span>的一个generation set。现在假设<span class="math inline">\(A = [u_1, u_2, \cdots, u_k]\)</span>，而<span class="math inline">\(\text{Col }A = \text{Span} \{u_1, u_2, \cdots, u_k \}\)</span>，因为<span class="math inline">\(A\)</span>的pivot column会是<span class="math inline">\(\text{Col }A\)</span>的basis，而且很显然会是<span class="math inline">\(V\)</span>的basis，也会是<span class="math inline">\(S\)</span>的subset。</p><p>第二个特性也是一个很直觉的事情，假设有个subspace <span class="math inline">\(V\)</span>，现在给定一个线性无关的集合<span class="math inline">\(S\)</span>，那么就存在两种情况，第一种，<span class="math inline">\(\text{Span }S\)</span>就是<span class="math inline">\(V\)</span>，那么<span class="math inline">\(S\)</span>就是<span class="math inline">\(V\)</span>的basis。第二种，存在一个向量<span class="math inline">\(v_1\)</span>不在<span class="math inline">\(\text{Span }S\)</span>里面，那么<span class="math inline">\(S \cup \{v_1\}\)</span>就是一个新的线性无关集合，然后又回到前面的两种情况。如此不断循环下去，最后就会发现，basis刚好就是最大的线性无关集合。</p><p>第三个特性就更加直觉了，basis我们可以认为是dimension，那就非常直觉了，再一个坐标系内，每一个向量都可以用坐标来唯一表示。</p><p>回顾一下，向量空间（vector space），列空间（column space），零空间（null space），行空间（row space），子空间（subspace）等等这些概念之间是什么关系？</p><p>首先是向量空间，顾名思义，就是向量所在的空间。向量的线性组合，数乘都在这个空间内。</p><p>而向量子空间还要包含零向量，其余和向量空间一样。也就是说，向量子空间是属于向量空间的。</p><p>然后是列空间，矩阵中所有列组成的向量空间就叫做列空间。相应的，行空间就是所有行组成的空间。</p><p>零空间是<span class="math inline">\(Ax=0\)</span>的所有解的集合。也叫作核。</p><p>现在我们看这些概念之间的关系。其实都是一些非常直觉的关系。</p><ol type="1"><li>矩阵<span class="math inline">\(A\)</span>的列空间<span class="math inline">\(\text{Col } A\)</span>的维度就是<span class="math inline">\(A\)</span>的pivot column，也就是<span class="math inline">\(\text{rank }A\)</span>。</li><li><span class="math inline">\(\text{Dim}(\text{Null } A) = n - \text{rank }A\)</span></li><li><span class="math inline">\(\text{Dim}(\text{Row } A) = \text{rank }A\)</span>，也就是<span class="math inline">\(A\)</span>的RREF下，非零的行</li></ol><p>那上面的这些特性，又会让我们知道<span class="math inline">\(\text{rank }A = \text{rank } A^{\top}\)</span>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;subspace &amp;amp; basis&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>Ubuntu 装机后必搞的一些软件</title>
    <link href="https://samaelchen.github.io/linux_software/"/>
    <id>https://samaelchen.github.io/linux_software/</id>
    <published>2018-06-21T16:00:00.000Z</published>
    <updated>2018-08-02T14:50:55.845Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>记录一下装机后必搞的一些软件，以后装机简单一点。</p><a id="more"></a><p>每次装机后都要装好多软件，这里记录一下，以后就简单多了。</p><p>添加PPA：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository ppa:hzwhuang/ss-qt5</span><br><span class="line">sudo add-apt-repository ppa:webupd8team/java</span><br><span class="line">sudo add-apt-repository ppa:jfi/ppa</span><br><span class="line">sudo add-apt-repository ppa:indicator-multiload/stable-daily</span><br><span class="line">sudo add-apt-repository ppa:noobslab/themes</span><br><span class="line">sudo add-apt-repository ppa:noobslab/icons</span><br><span class="line"></span><br><span class="line">wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add</span><br><span class="line">sudo sh -c <span class="string">'echo "deb http://dl.google.com/linux/chrome/deb/ stable main" &gt;&gt; /etc/apt/sources.list.d/google-chrome.list'</span></span><br><span class="line"></span><br><span class="line">curl -sL https://packagecloud.io/AtomEditor/atom/gpgkey | sudo apt-key add -</span><br><span class="line">sudo sh -c <span class="string">'echo "deb [arch=amd64] https://packagecloud.io/AtomEditor/atom/any/ any main" &gt; /etc/apt/sources.list.d/atom.list'</span></span><br><span class="line"></span><br><span class="line">wget -q -O - http://archive.getdeb.net/getdeb-archive.key | sudo apt-key add -</span><br><span class="line">sudo sh -c <span class="string">'echo "deb http://archive.getdeb.net/ubuntu xenial-getdeb apps" &gt;&gt; /etc/apt/sources.list.d/getdeb.list'</span></span><br></pre></td></tr></table></figure><p>然后更新安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line"></span><br><span class="line">sudo apt install git terminator guake shadowsocks-qt5 nethogs dpkg oracle-java8-installer atom screenfetch sensord lm-sensors hddtemp psensor indicator-multiload shutter kazam vlc okular ubuntu-tweak flatabulous-theme ultra-flat-icons tsocks vim google-chrome</span><br></pre></td></tr></table></figure><p>装完小飞机后修改tsocks的配置文件 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/tsocks.conf</span><br></pre></td></tr></table></figure></p><p>找到 server 改成 server = 127.0.0.1。</p><p>然后是百度搜狗输入法，网易云音乐安装包。用dpkg安装就好了。</p><p>然后是搜cuda，按照官方的步骤一步步安装。另外就是安装anaconda。这个也不赘述。</p><p>接着是一些骚兮兮的美化工作。用Ubuntu-tweak把系统主题改成flatabulous，图标改成ultra-flat-icons。不过可惜啊，flatabulous的作者不再继续支持这个主题了，感觉以后要用macbuntu之类的了（这两天逛gnome-look.org发现另一个挺好看的主题，ant themes，都是黑色的主题，简直本命）。</p><p>接着是设置一下psensor，在toolbar显示CPU和GPU的温度。</p><p>这些弄完，配置一下atom作为Python的ide： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">apm install linter hydrogen markdown-preview-enhanced atom-beautify language-markdown language-latex atom-language-r project-manager</span><br><span class="line">pip install flake8 flake8-docstrings</span><br><span class="line">apm install linter-flake8</span><br><span class="line">pip install autopep8</span><br></pre></td></tr></table></figure></p><p>然后是在preference里面把tab length改成4个spaces。另外为了避免enter自动补全，而只是换行，修改keymap，添加： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Disable Enter key for confirming an autocomplete suggestion</span><br><span class="line">&apos;atom-text-editor:not(mini).autocomplete-active&apos;:</span><br><span class="line">  &apos;enter&apos;: &apos;editor:newline&apos;</span><br></pre></td></tr></table></figure></p><p>另外atom-beautify的快捷键是ctrl+alt+B，跟fcitx的软键盘快捷键冲突了，吧fcitx开启软键盘的快捷键改掉就好了。</p><p>然后是安装zsh，配置一个骚气的终端。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install zsh</span><br><span class="line">chsh -s /bin/zsh</span><br></pre></td></tr></table></figure><p>接着安装oh-my-zsh</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -c <span class="string">"<span class="variable">$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)</span>"</span></span><br></pre></td></tr></table></figure><p>配置自动跳转 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install autojump</span><br><span class="line">vim .zshrc</span><br><span class="line"><span class="comment">#在最后一行加入，注意点后面是一个空格</span></span><br><span class="line">. /usr/share/autojump/autojump.sh</span><br></pre></td></tr></table></figure></p><p>配置语法高亮 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/zsh-users/zsh-syntax-highlighting.git</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"source <span class="variable">$&#123;(q-)PWD&#125;</span>/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh"</span> &gt;&gt; <span class="variable">$&#123;ZDOTDIR:-$HOME&#125;</span>/.zshrc</span><br></pre></td></tr></table></figure></p><p>配置语法历史记录 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git://github.com/zsh-users/zsh-autosuggestions <span class="variable">$ZSH_CUSTOM</span>/plugins/zsh-autosuggestions</span><br></pre></td></tr></table></figure></p><p>然后修改.zshrc，改成 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plugins=(</span><br><span class="line">  git</span><br><span class="line">  zsh-autosuggestions</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><p>然后在最后一行加入 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source $ZSH_CUSTOM/plugins/zsh-autosuggestions/zsh-autosuggestions.zsh</span><br></pre></td></tr></table></figure></p><p>最后是配置一个骚气的主题，可以看<a href="https://github.com/robbyrussell/oh-my-zsh/wiki/External-themes" class="uri" target="_blank" rel="noopener">https://github.com/robbyrussell/oh-my-zsh/wiki/External-themes</a>里面的主题，这里我用的是agnosterzak。</p><p>先安装powerline： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install fonts-powerline</span><br></pre></td></tr></table></figure></p><p>然后将主题放到~/.oh-my-zsh/themes下面： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://raw.github.com/zakaziko99/agnosterzak-ohmyzsh-theme/master/agnosterzak.zsh-theme -P ~/.oh-my-zsh/themes</span><br></pre></td></tr></table></figure></p><p>然后修改 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ZSH_THEME=&quot;agnosterzak&quot;</span><br></pre></td></tr></table></figure></p><p>最后将.bashrc底下一些新加入的export啊，alias啊什么的复制到.zshrc底下就好了。</p><p>退出终端重新进入，骚气的zsh就配置好了。</p><p>然后是电池电量不显示的问题，安装acpi就可以显示。</p><p>如果想卸载的话，执行： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo sh -c <span class="string">"<span class="variable">$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/uninstall.sh)</span>"</span></span><br></pre></td></tr></table></figure></p><p>然后将/etc/passwd里面找到自己用户名那一行，把/usr/bin/zsh改成/bin/bash/就可以了。</p><p>还有就是配置一个自我发泄的自动对命令行纠错的插件，thefuck，前面配好anaconda以后，只要pip install thefuck就行。然后在.zshrc最后加上eval $(thefuck –alias)。后面如果敲错代码，只要输入fuck，就会自动纠错。</p><p>另外，作为Unity的死忠粉，如果装的是Ubuntu 18.04，那么更新完软件以后，要做的事情就是： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install ubuntu-unity-desktop</span><br></pre></td></tr></table></figure></p><p>记着将display manager改成lightdm就好了。</p><p>目前大概就是这样吧，以后想到有什么更新再说。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录一下装机后必搞的一些软件，以后装机简单一点。&lt;/p&gt;
    
    </summary>
    
      <category term="搞着玩" scheme="https://samaelchen.github.io/categories/fun/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux 日常使用软件的日经吐槽贴</title>
    <link href="https://samaelchen.github.io/linux_usage_daily/"/>
    <id>https://samaelchen.github.io/linux_usage_daily/</id>
    <published>2018-06-21T16:00:00.000Z</published>
    <updated>2018-08-15T12:43:55.863Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>常年把Linux作为desktop的日经吐槽贴，买MacBook的小钱钱攒够就关了。每个日经贴都是买MacBook的一个理由啊。别问为什么不用Windows，用Python写machine learning的应该知道Windows下面有多折腾。</p><p>等买MacBook的小钱钱攒够了就关掉此贴（大概此生无望了/(ㄒoㄒ)/~~）。</p><a id="more"></a><p>说一下用过的发行版，Ubuntu启蒙，目前用的是Ubuntu，另外就是国产的Deepin。不过说起来，Ubuntu放弃了Unity，Deepin创始人离开，感觉Linux没什么可以留恋的了。哪天Unity彻底死掉，就弃坑了。</p><p>在国内能够愉快用Linux不得不感谢Deepin的付出。首先是国内绕不开的QQ和微信，Deepin wine都有很好的支持。此外，Deepin共同开发了搜狗输入法（其实是Deepin最早开始的，后来优麒麟不要脸抢走了的样子），网易云音乐和有道词典。</p><h1 id="日经">2018.06.22 日经</h1><p>这次日经贴就是吐槽词典的，Deepin毕竟人手有限，有道词典有个可怕的bug，就是内存泄露，这个程序已经3年没更新了，所以这个bug就一直在。</p><p>那为了愉快使用词典，我寻求了古老但坚挺的GoldenDict。在Ubuntu下面，老将还是非常稳定的。但是在最新的Deepin 15.6下面，GoldenDict会莫名其妙一直占满一个线程（WTF）。</p><p>于是只能转寻命令行查词的工具了。一开始用的是dictd，非常好用，但是不支持扩展星际译王的词典，所以放弃了。</p><p>然后用的是sdcv，星际译王的终端版本，支持扩展词典，效果棒棒哒。目前在用，效果如图：</p><p><img src="https://i.imgur.com/EQg93QK.jpg"></p><p>安装方法很简单： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install sdcv</span><br></pre></td></tr></table></figure></p><p>然后将自己下载好的辞书放到～/.stardict/dic路径下就可以了。嗯，不折腾买MacBook理由+1。</p><h1 id="日经-1">2018.07.09 日经</h1><p>这个感觉也不是Linux的问题，主要是markdown底下碰到了要打中文间隔号的时候，发现诶，好像不是很好用啊。如果跟我一样用的是搜狗输入法，那么可以输入yd，然后就有这个符号的候选了。感谢搜狗。</p><h1 id="日经-2">2018.08.15 日经</h1><p>写hexo用next主题的时候有个问题，就是有时候网页上的icon会不显示，比如home上面那个小房子。其实就是push以后，next文件夹底下的source/lib里少了font-awesome和ua-parser-js两个文件夹。到github的next源码那里复制一份拷下来就好了。这俩文件夹被github屏蔽了难道？导致每次都push不上去？！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;常年把Linux作为desktop的日经吐槽贴，买MacBook的小钱钱攒够就关了。每个日经贴都是买MacBook的一个理由啊。别问为什么不用Windows，用Python写machine learning的应该知道Windows下面有多折腾。&lt;/p&gt;
&lt;p&gt;等买MacBook的小钱钱攒够了就关掉此贴（大概此生无望了/(ㄒoㄒ)/~~）。&lt;/p&gt;
    
    </summary>
    
      <category term="搞着玩" scheme="https://samaelchen.github.io/categories/fun/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数 06</title>
    <link href="https://samaelchen.github.io/linear_algebra_step6/"/>
    <id>https://samaelchen.github.io/linear_algebra_step6/</id>
    <published>2018-06-20T16:00:00.000Z</published>
    <updated>2018-08-15T11:58:12.584Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>矩阵的行列式</p><a id="more"></a><p>矩阵的行列式是非常重要的一个概念，包括到后面的矩阵求导什么的，都有用到。如果上过吴恩达的machine learning课程就会发现这一点。一般来说我们用<span class="math inline">\(\det(A)\)</span>表示行列式。</p><p>二阶方阵还有三阶方阵的行列式计算方法都是高中的内容，不多说，回顾一下行列式的一些特性：</p><blockquote><ol type="a"><li>单位矩阵的行列式一定为1</li><li>交换矩阵两个行的位置一次，行列式乘一次<span class="math inline">\(-1\)</span></li><li>如果矩阵线性相关，行列式为0</li><li><span class="math inline">\(\det(\begin{bmatrix} ta &amp;tb \\ c &amp;d \end{bmatrix}) = t \det(\begin{bmatrix} a &amp;b \\ c &amp;d \end{bmatrix})\)</span></li><li><span class="math inline">\(\det(\begin{bmatrix} a + a&#39; &amp;b + b&#39; \\ c &amp;d \end{bmatrix}) = \det(\begin{bmatrix} a &amp;b \\ c &amp;d \end{bmatrix}) + \det(\begin{bmatrix} a&#39; &amp;b&#39; \\ c &amp;d \end{bmatrix})\)</span></li><li><span class="math inline">\(\det(AB) = \det(A) \det(B)\)</span></li><li><span class="math inline">\(\det(A^{\top}) = \det(A)\)</span></li></ol></blockquote><p>那行列式和前面的矩阵可逆之间有什么关系呢？很直觉的，如果矩阵的行列式为0，那么这个矩阵就不可逆，如果行列式不为0，那么这个矩阵就可逆。</p><p>大部分人应该都忘了大学线代教的高阶矩阵如何计算行列式，所以这里复习一下。我们引入一个定义<span class="math inline">\(A_{ij}\)</span>表示的是原来的矩阵<span class="math inline">\(A\)</span>不包含<span class="math inline">\(i\)</span>行<span class="math inline">\(j\)</span>列的所有元素，也可以记做<span class="math inline">\(c_{ij}\)</span>。这个英文是cofactor，就是代数余子式，所以取了首字母。然后我们可以这样定义行列式： <span class="math display">\[\det(A) = a_{11} c_{11} + a_{12} c_{12} + \cdots + a_{1n} c_{1n}\]</span></p><p>其中<span class="math inline">\(c_{ij} = (-1)^{i+j} \det A_{ij}\)</span>。</p><p>当然，除了用一行的元素，也可以用一列的元素。</p><p>那这样一来，不论是多么高阶的矩阵，我们都可以不断拆解，直到一直拆到二阶矩阵求行列式。</p><p>行列式的计算大概就这么一回事，计算量比较大，原理不难。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;矩阵的行列式&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数 05</title>
    <link href="https://samaelchen.github.io/linear_algebra_step5/"/>
    <id>https://samaelchen.github.io/linear_algebra_step5/</id>
    <published>2018-06-19T16:00:00.000Z</published>
    <updated>2018-08-15T11:58:12.584Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>逆矩阵</p><a id="more"></a><p>假设，现在有一个向量<span class="math inline">\(v\)</span>，经过矩阵<span class="math inline">\(A\)</span>的变换，再经过矩阵<span class="math inline">\(B\)</span>的变换，向量不变。此外，先经过<span class="math inline">\(B\)</span>再经过<span class="math inline">\(A\)</span>，仍然得到<span class="math inline">\(v\)</span>。在这种情况下，我们就可以说<span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>互逆。</p><p>那么严格一点，如果<span class="math inline">\(n \times n\)</span>的矩阵<span class="math inline">\(A\)</span>是可逆的，那么会存在一个<span class="math inline">\(n \times n\)</span>的矩阵<span class="math inline">\(B\)</span>使得<span class="math inline">\(AB = BA = I\)</span>。在这种情况下，我们称<span class="math inline">\(B\)</span>是<span class="math inline">\(A\)</span>的一个逆矩阵，可以记为<span class="math inline">\(A^{-1}\)</span>。</p><p>所以有一个很明显的点，当一个矩阵不是方阵的时候，必定是没有逆矩阵的。不过实际上在矩阵分析里面，这样的矩阵也可以求逆，叫做伪逆。</p><p>此外，一个矩阵如果有逆矩阵，那么一定只有唯一的一个逆矩阵。这个其实很好证明： <span class="math display">\[AB = BA = I，AC = CA = I \\B = BI = B(AC) = (BA)C = IC = C\]</span></p><p>那逆矩阵有什么用呢，用逆矩阵可以解之前的线性方程组，直接得到经过高斯消元法之后的结果。那实际上，逆矩阵求解对机器来说是没效率的，因为机器求逆矩阵就用了RREF。</p><p>现在考虑一下，如果我们对矩阵的乘做逆运算会怎么样。也就是<span class="math inline">\((AB)^{-1}\)</span>。那么我们可以得到的是<span class="math inline">\((AB)^{-1} = B^{-1}A^{-1}\)</span>，这里要求两个矩阵都是可逆的。由此我们可以推广，<span class="math inline">\((A_1 A_2 \cdots A_k)^{-1} = A_k^{-1} \cdots A_2^{-1} A_1^{-1}\)</span>。</p><p>那如果<span class="math inline">\(A\)</span>是可逆的，<span class="math inline">\(A^{\top}\)</span>是不是也是可逆呢？答案很明显是可逆的，可以求一下，<span class="math inline">\(AA^{-1} = (AA^{-1})^{\top} = (A^{-1})^{\top} A^{\top} = I\)</span>，然后反过来<span class="math inline">\(A^{-1}A = (A^{-1}A)^{\top} = A^{\top} (A^{-1})^{\top} = I\)</span>。所以我们就知道，<span class="math inline">\((A^{\top})^{-1} = (A^{-1})^{\top}\)</span>。</p><p>那么如何判断一个矩阵是不是可逆的？在Elementary Linear Algebra里面，提供了十几种判断依据：</p><blockquote><ol type="a"><li><span class="math inline">\(A\)</span> is invertible.</li><li>The reduced row echelon form of <span class="math inline">\(A\)</span> is <span class="math inline">\(I_n\)</span>.</li><li>The rank of <span class="math inline">\(A\)</span> equals <span class="math inline">\(n\)</span>.</li><li>The span of the columns of <span class="math inline">\(A\)</span> is <span class="math inline">\(R_n\)</span>.</li><li>The equation <span class="math inline">\(Ax = b\)</span> is consistent for every <span class="math inline">\(b\)</span> in <span class="math inline">\(R_n\)</span>.</li><li>The nullity of <span class="math inline">\(A\)</span> equals zero.</li><li>The columns of <span class="math inline">\(A\)</span> are linearly independent.</li><li>The only solution of <span class="math inline">\(Ax = 0\)</span> is <span class="math inline">\(\mathbf{0}\)</span>.</li><li>There exists an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(B\)</span> such that <span class="math inline">\(BA = I_n\)</span>.</li><li>There exists an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(C\)</span> such that <span class="math inline">\(AC = I_n\)</span>.</li><li><span class="math inline">\(A\)</span> is a product of elementary matrices.</li></ol></blockquote><p>也就是说，当一个矩阵<span class="math inline">\(A\)</span>是方阵时，如果<span class="math inline">\(A\)</span>可逆，上述的条件都是等价的。</p><p>判断完是否可逆之后，如何计算逆矩阵呢？</p><p>这里引入一个概念，叫做elementary matrix，比如说<span class="math inline">\(E = \begin{bmatrix} 1 &amp;0 &amp;0 \\ 0 &amp;0 &amp;1 \\ 0 &amp;1 &amp;0 \end{bmatrix}\)</span>。那这个变换矩阵的作用就是交换第二行和第三行。我们回顾一下，之前说的RREF其实做的事情就是多次的elementary matrix乘原来的矩阵。那么如果我们的RREF是单位矩阵，那么其实<span class="math inline">\(A\)</span>的逆矩阵就是这么多elementary matrix的乘积，不过这里要注意的是elementary matrix <span class="math inline">\(E\)</span>的顺序。</p><p>矩阵的逆大概就是这么多内容，简单一点判断，如果矩阵是方阵，且满秩，就可逆。计算方法就用高斯消元法的那一套来。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;逆矩阵&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数 04</title>
    <link href="https://samaelchen.github.io/linear_algebra_step4/"/>
    <id>https://samaelchen.github.io/linear_algebra_step4/</id>
    <published>2018-06-18T16:00:00.000Z</published>
    <updated>2018-08-15T11:58:12.584Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>矩阵乘法</p><a id="more"></a><p>矩阵的叉乘运算是高中内容，比较简单： <span class="math display">\[C = AB \\c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \cdots + a_{in}b_{nj}\]</span> 所以这里我们不去花太多时间讨论计算的事情，这里回顾一下矩阵乘法的意义。另外一般而言，叉乘省略乘号。如果是element-wise multiplication一般会用<span class="math inline">\(\odot\)</span>。</p><p>矩阵可以看作是一个线性系统，因此，一种看法，我们可以把矩阵乘法看作是一组向量通过一个线性系统变换，得到另一组向量。</p><p>另外一种视角是，我们将一个矩阵看作是一个线性变换的函数，那么两个矩阵相乘就可以看做是一个线性变换的组合，或者说是函数的组合。但是这里要注意一点，矩阵相乘前后顺序不一致，得到的结果不一样。</p><p>矩阵乘法有一些性质：</p><ol type="1"><li><span class="math inline">\(s(AC) = (sA)C = A(sC)\)</span></li><li><span class="math inline">\((A + B)C = AC + BC\)</span></li><li><span class="math inline">\(C(P + Q) = CP + CQ\)</span></li><li><span class="math inline">\(IA = A = AI\)</span></li><li><span class="math inline">\(A^k = AAA \cdots A(\text{k times})\)</span></li><li><span class="math inline">\((AC)^{\top} = C^{\top}A^{\top}\)</span></li></ol><p>另外矩阵可以做增广，也可以做分块。增广很好理解，跟之前线性方程组做增广矩阵非常像，只要两个矩阵的row相等，就可以拼在一起<span class="math inline">\([ A \ B ]\)</span></p><p>矩阵分块也很好理解，就是将一个很大的矩阵分割成好几个小矩阵。实际上，做partition这个事情的好处是，我们可以在一定程度上减少运算量。如下图：</p><p><img src="https://i.imgur.com/X508XpX.png"></p><p>矩阵的乘法其实并不难，而且现在都可以用机器来计算，一般来说GPU比CPU更擅长算这个，这也是为什么深度学习需要用GPU来加速的原因。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;矩阵乘法&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>LSTM的PyTorch实现</title>
    <link href="https://samaelchen.github.io/LSTM-pytorch/"/>
    <id>https://samaelchen.github.io/LSTM-pytorch/</id>
    <published>2018-06-18T16:00:00.000Z</published>
    <updated>2018-08-15T11:58:12.580Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>基于PyTorch的LSTM实现。</p><a id="more"></a><p>PyTorch封装了很多常用的神经网络，要实现LSTM非常的容易。这里用官网的实例修改实现练习里面的character level LSTM。</p><p>首先还是老样子，import需要的module： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><p>然后为了将数据放到网络里面，我们需要做一个编码单词的函数： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_char_sequence</span><span class="params">(word, to_ix)</span>:</span></span><br><span class="line">    idxs = [to_ix[char] <span class="keyword">for</span> char <span class="keyword">in</span> word]</span><br><span class="line">    <span class="keyword">return</span>(torch.tensor(idxs, dtype=torch.long))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_sequence</span><span class="params">(seq, to_ix)</span>:</span></span><br><span class="line">    idxs = [to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> seq]</span><br><span class="line">    char_idxs = [prepare_char_sequence(w, char_to_ix) <span class="keyword">for</span> w <span class="keyword">in</span> seq]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(idxs, dtype=torch.long), char_idxs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">training_data = [</span><br><span class="line">    (<span class="string">"The dog ate the apple"</span>.split(), [<span class="string">"DET"</span>, <span class="string">"NN"</span>, <span class="string">"V"</span>, <span class="string">"DET"</span>, <span class="string">"NN"</span>]),</span><br><span class="line">    (<span class="string">"Everybody read that book"</span>.split(), [<span class="string">"NN"</span>, <span class="string">"V"</span>, <span class="string">"DET"</span>, <span class="string">"NN"</span>])</span><br><span class="line">]</span><br><span class="line">word_to_ix = &#123;&#125;</span><br><span class="line">char_to_ix = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> sent, tags <span class="keyword">in</span> training_data:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sent:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word_to_ix:</span><br><span class="line">            word_to_ix[word] = len(word_to_ix)</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> word:</span><br><span class="line">            <span class="keyword">if</span> char <span class="keyword">not</span> <span class="keyword">in</span> char_to_ix:</span><br><span class="line">                char_to_ix[char] = len(char_to_ix)</span><br><span class="line">print(word_to_ix)</span><br><span class="line">print(char_to_ix)</span><br><span class="line"></span><br><span class="line">tag_to_ix = &#123;<span class="string">"DET"</span>: <span class="number">0</span>, <span class="string">"NN"</span>: <span class="number">1</span>, <span class="string">"V"</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># These will usually be more like 32 or 64 dimensional.</span></span><br><span class="line"><span class="comment"># We will keep them small, so we can see how the weights change as we train.</span></span><br><span class="line">WORD_EMBEDDING_DIM = <span class="number">5</span></span><br><span class="line">CHAR_EMBEDDING_DIM = <span class="number">5</span></span><br><span class="line">CHAR_HIDDEN_DIM = <span class="number">3</span></span><br><span class="line">WORD_HIDDEN_DIM = <span class="number">6</span></span><br></pre></td></tr></table></figure></p><p>其实这里想想，如果是全文本，我们的character level编码也就26个字母表那么多。</p><p>然后我们定义一个character level的网络： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMTagger</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, char_embedding_dim, word_embedding_dim, char_hidden_dim, word_hidden_dim, char_size, vocab_size, tagset_size)</span>:</span></span><br><span class="line">        super(LSTMTagger, self).__init__()</span><br><span class="line">        self.char_embedding_dim = char_embedding_dim</span><br><span class="line">        self.word_embedding_dim = word_embedding_dim</span><br><span class="line">        self.char_hidden_dim = char_hidden_dim</span><br><span class="line">        self.word_hidden_dim = word_hidden_dim</span><br><span class="line">        self.char_embeddings = nn.Embedding(char_size, char_embedding_dim)</span><br><span class="line">        self.word_embeddings = nn.Embedding(vocab_size, word_embedding_dim)</span><br><span class="line"></span><br><span class="line">        self.char_lstm = nn.LSTM(char_embedding_dim, char_hidden_dim)</span><br><span class="line">        self.word_lstm = nn.LSTM((word_embedding_dim + char_hidden_dim), word_hidden_dim)</span><br><span class="line"></span><br><span class="line">        self.hidden2tag = nn.Linear(word_hidden_dim, tagset_size)</span><br><span class="line">        self.char_hidden = self.init_hidden(self.char_hidden_dim)</span><br><span class="line">        self.word_hidden = self.init_hidden(self.word_hidden_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, hidden_dim)</span>:</span></span><br><span class="line">        <span class="keyword">return</span>(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, hidden_dim),</span><br><span class="line">               torch.zeros(<span class="number">1</span>, <span class="number">1</span>, hidden_dim))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, sentence)</span>:</span></span><br><span class="line">        char_lstm_result = []</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sentence[<span class="number">1</span>]:</span><br><span class="line">            self.char_hidden = self.init_hidden(self.char_hidden_dim)</span><br><span class="line">            char_embeds = self.char_embeddings(word)</span><br><span class="line">            lstm_char_out, self.char_hidden = self.char_lstm(char_embeds.view(len(word), <span class="number">1</span>, <span class="number">-1</span>), self.char_hidden)</span><br><span class="line">            char_lstm_result.append(lstm_char_out[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">        word_embeds = self.word_embeddings(sentence[<span class="number">0</span>])</span><br><span class="line">        char_lstm_result = torch.stack(char_lstm_result)</span><br><span class="line">        lstm_in = torch.cat((word_embeds.view(len(sentence[<span class="number">0</span>]), <span class="number">1</span>, <span class="number">-1</span>), char_lstm_result), <span class="number">2</span>)</span><br><span class="line">        lstm_out, self.hidden = self.word_lstm(lstm_in, self.word_hidden)</span><br><span class="line">        tag_space = self.hidden2tag(lstm_out.view(len(sentence[<span class="number">0</span>]), <span class="number">-1</span>))</span><br><span class="line">        tag_scores = F.log_softmax(tag_space, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> tag_scores</span><br></pre></td></tr></table></figure></p><p>在forward部分可以看到，这里有两个LSTM。第一个LSTM做的事情是将character拼成word，相当于是返回了一个character level的word embedding。然后用这个embedding和直接embedding的word vector拼到一起，放到第二个LSTM里面训练词性标注。另外要注意的是，这里虽然有两个LSTM模型，但是我们并没有定义第一个LSTM的loss function。因为我们要让这个网络按照最后词性标注的效果来训练，因此我们不需要定义这个网络的loss function。</p><p>定义一下相关的参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = LSTMTagger(CHAR_EMBEDDING_DIM, WORD_EMBEDDING_DIM, CHAR_HIDDEN_DIM, WORD_HIDDEN_DIM, len(char_to_ix), len(word_to_ix), len(tag_to_ix))</span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>看一下训练前的输出结果是什么： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    inputs = prepare_sequence(training_data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix)</span><br><span class="line">    tag_scores = model(inputs)</span><br><span class="line">    print(tag_scores)</span><br></pre></td></tr></table></figure></p><p>再看一下训练300轮之后的结果： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">300</span>):</span><br><span class="line">    <span class="keyword">for</span> sentence, tags <span class="keyword">in</span> training_data:</span><br><span class="line">        model.zero_grad()</span><br><span class="line">        model.hidden = model.init_hidden(WORD_EMBEDDING_DIM)</span><br><span class="line">        sentence_in = prepare_sequence(sentence, word_to_ix)</span><br><span class="line">        targets = prepare_char_sequence(tags, tag_to_ix)</span><br><span class="line">        tag_scores = model(sentence_in)</span><br><span class="line"></span><br><span class="line">        loss = loss_function(tag_scores, targets)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    inputs = prepare_sequence(training_data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix)</span><br><span class="line">    tag_scores = model(inputs)</span><br><span class="line">    print(tag_scores)</span><br></pre></td></tr></table></figure></p><p>一般而言，character level的LSTM会比word level的更有效果。这里因为是一个toy级别的，看不出太显著的差别来。如果是海量数据，一般而言会有比较明显的效果。</p><p>另外，原来的example是单向的LSTM，这里顺便做一个双向的。其实双向的LSTM就是正向一个，反向再一个，所以hidden的部分是两倍。所以要修改的地方就是网络的定义，将单向LSTM的hidden乘以2就好了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMTagger</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedding_dim, hidden_dim, vocab_size, tagset_size)</span>:</span></span><br><span class="line">        super(LSTMTagger, self).__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line"></span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        self.hidden2tag = nn.Linear(hidden_dim * <span class="number">2</span>, tagset_size)</span><br><span class="line">        self.hidden = self.init_hidden()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span>(torch.zeros(<span class="number">1</span> * <span class="number">2</span>, <span class="number">1</span>, self.hidden_dim),</span><br><span class="line">               torch.zeros(<span class="number">1</span> * <span class="number">2</span>, <span class="number">1</span>, self.hidden_dim))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, sentence)</span>:</span></span><br><span class="line">        embeds = self.word_embeddings(sentence)</span><br><span class="line">        lstm_out, self.hidden = self.lstm(</span><br><span class="line">            embeds.view(len(sentence), <span class="number">1</span>, <span class="number">-1</span>), self.hidden)</span><br><span class="line">        tag_space = self.hidden2tag(lstm_out.view(len(sentence), <span class="number">-1</span>))</span><br><span class="line">        tag_scores = F.log_softmax(tag_space, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> tag_scores</span><br></pre></td></tr></table></figure><p>这样就简单实现了一个toy级别的双向LSTM和character level的单向LSTM。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;基于PyTorch的LSTM实现。&lt;/p&gt;
    
    </summary>
    
      <category term="PyTorch" scheme="https://samaelchen.github.io/categories/PyTorch/"/>
    
    
  </entry>
  
</feed>
